{"cells":[{"cell_type":"markdown","metadata":{"id":"C_jdZ5vHJ4A9"},"source":["# Task description\n","- Classify the speakers of given features.\n","- Main goal: Learn how to use transformer.\n","- Baselines:\n","  - Easy: Run sample code and know how to use transformer.\n","  - Medium: Know how to adjust parameters of transformer.\n","  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n","  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n","\n","- Other links\n","  - Competiton: [link](https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b)\n","  - Slide: [link](https://docs.google.com/presentation/d/1LDAW0GGrC9B6D7dlNdYzQL6D60-iKgFr/edit?usp=sharing&ouid=104280564485377739218&rtpof=true&sd=true)\n","  - Data: [link](https://github.com/googly-mingto/ML2023HW4/releases)\n","\n","# Download dataset\n","- Data is [here](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gtKxUzSgXKj3","outputId":"b5c1719f-4e86-44ca-83d4-76cc14d96062"},"outputs":[],"source":["!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n","!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n","!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n","!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n","\n","!cat Dataset.tar.gz.part* > Dataset.tar.gz\n","!rm Dataset.tar.gz.partaa\n","!rm Dataset.tar.gz.partab\n","!rm Dataset.tar.gz.partac\n","!rm Dataset.tar.gz.partad\n","# unzip the file\n","!tar zxf Dataset.tar.gz\n","!rm Dataset.tar.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6Y1cfpDfpON","outputId":"4f562818-1b49-4fd3-8a98-ba6d3cd3d4f2"},"outputs":[],"source":["!tar zxf Dataset.tar.gz"]},{"cell_type":"markdown","metadata":{},"source":["# Set seed"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"E6burzCXIyuA"},"outputs":[],"source":["import numpy as np\n","import torch\n","import random\n","\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","set_seed(87)"]},{"cell_type":"markdown","metadata":{"id":"k7dVbxW2LASN"},"source":["# Data\n","\n","## Dataset\n","- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n","- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n","- We randomly select 600 speakers from Voxceleb2.\n","- Then preprocess the raw waveforms into mel-spectrograms.\n","\n","- Args:\n","  - data_dir: The path to the data directory.\n","  - metadata_path: The path to the metadata.\n","  - segment_len: The length of audio segment for training. \n","- The architecture of data directory \\\\\n","  - data directory \\\\\n","  |---- metadata.json \\\\\n","  |---- testdata.json \\\\\n","  |---- mapping.json \\\\\n","  |---- uttr-{random string}.pt \\\\\n","\n","- The information in metadata\n","  - \"n_mels\": The dimention of mel-spectrogram.\n","  - \"speakers\": A dictionary. \n","    - Key: speaker ids.\n","    - value: \"feature_path\" and \"mel_len\"\n","\n","\n","For efficiency, we segment the mel-spectrograms into segments in the traing step."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"KpuGxl4CI2pr"},"outputs":[],"source":["import os\n","import json\n","import torch\n","import random\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n"," \n"," \n","class myDataset(Dataset):\n","\tdef __init__(self, data_dir, segment_len=128):\n","\t\tself.data_dir = data_dir\n","\t\tself.segment_len = segment_len\n","\t\n","\t\t# Load the mapping from speaker neme to their corresponding id. \n","\t\tmapping_path = Path(data_dir) / \"mapping.json\"\n","\t\tmapping = json.load(mapping_path.open())\n","\t\tself.speaker2id = mapping[\"speaker2id\"]\n","\t\n","\t\t# Load metadata of training data.\n","\t\tmetadata_path = Path(data_dir) / \"metadata.json\"\n","\t\tmetadata = json.load(open(metadata_path))[\"speakers\"]\n","\t\n","\t\t# Get the total number of speaker.\n","\t\tself.speaker_num = len(metadata.keys())\n","\t\tself.data = []\n","\t\tfor speaker in metadata.keys():\n","\t\t\tfor utterances in metadata[speaker]:\n","\t\t\t\tself.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n"," \n","\tdef __len__(self):\n","\t\t\treturn len(self.data)\n"," \n","\tdef __getitem__(self, index):\n","\t\tfeat_path, speaker = self.data[index]\n","\t\t# Load preprocessed mel-spectrogram.\n","\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","\t\t# Segmemt mel-spectrogram into \"segment_len\" frames.\n","\t\tif len(mel) > self.segment_len:\n","\t\t\t# Randomly get the starting point of the segment.\n","\t\t\tstart = random.randint(0, len(mel) - self.segment_len)\n","\t\t\t# Get a segment with \"segment_len\" frames.\n","\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])\n","\t\telse:\n","\t\t\tmel = torch.FloatTensor(mel)\n","\t\t# Turn the speaker id into long for computing loss later.\n","\t\tspeaker = torch.FloatTensor([speaker]).long()\n","\t\treturn mel, speaker\n"," \n","\tdef get_speaker_number(self):\n","\t\treturn self.speaker_num"]},{"cell_type":"markdown","metadata":{"id":"668hverTMlGN"},"source":["## Dataloader\n","- Split dataset into training dataset(90%) and validation dataset(10%).\n","- Create dataloader to iterate the data."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"B7c2gZYoJDRS"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","def collate_batch(batch):\n","\t# Process features within a batch.\n","\t\"\"\"Collate a batch of data.\"\"\"\n","\tmel, speaker = zip(*batch)\n","\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n","\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n","\t# mel: (batch size, length, 40)\n","\treturn mel, torch.FloatTensor(speaker).long()\n","\n","\n","def get_dataloader(data_dir, batch_size, n_workers):\n","\t\"\"\"Generate dataloader\"\"\"\n","\tdataset = myDataset(data_dir)\n","\tspeaker_num = dataset.get_speaker_number()\n","\t# Split dataset into training dataset and validation dataset\n","\ttrainlen = int(0.9 * len(dataset))\n","\tlengths = [trainlen, len(dataset) - trainlen]\n","\ttrainset, validset = random_split(dataset, lengths)\n","\n","\ttrain_loader = DataLoader(\n","\t\ttrainset,\n","\t\tbatch_size=batch_size,\n","\t\tshuffle=True,\n","\t\tdrop_last=True,\n","\t\tnum_workers=n_workers,\n","\t\tpin_memory=True,\n","\t\tcollate_fn=collate_batch,\n","\t)\n","\tvalid_loader = DataLoader(\n","\t\tvalidset,\n","\t\tbatch_size=batch_size,\n","\t\tnum_workers=n_workers,\n","\t\tdrop_last=True,\n","\t\tpin_memory=True,\n","\t\tcollate_fn=collate_batch,\n","\t)\n","\n","\treturn train_loader, valid_loader, speaker_num"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["56666"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset = myDataset('data/Dataset')\n","len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"5FOSZYxrMqhc"},"source":["# Model\n","- TransformerEncoderLayer:\n","  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","  - Parameters:\n","    - d_model: the number of expected features of the input (required).\n","\n","    - nhead: the number of heads of the multiheadattention models (required).\n","\n","    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n","\n","    - dropout: the dropout value (default=0.1).\n","\n","    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n","\n","- TransformerEncoder:\n","  - TransformerEncoder is a stack of N transformer encoder layers\n","  - Parameters:\n","    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n","\n","    - num_layers: the number of sub-encoder-layers in the encoder (required).\n","\n","    - norm: the layer normalization component (optional)."]},{"cell_type":"markdown","metadata":{},"source":["### Simple\n","\n","Acc=0.66025"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iXZ5B0EKJGs8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Classifier(nn.Module):\n","\tdef __init__(self, d_model=80, n_spks=600, dropout=0.1):\n","\t\tsuper().__init__()\n","\t\t# Project the dimension of features from that of input into d_model.\n","\t\tself.prenet = nn.Linear(40, d_model)\n","\t\tself.encoder_layer = nn.TransformerEncoderLayer(\n","\t\t\td_model=d_model, dim_feedforward=256, nhead=2\n","\t\t)\n","\t\t# self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n","\n","\t\t# Project the the dimension of features from d_model into speaker nums.\n","\t\tself.pred_layer = nn.Sequential(\n","\t\t\tnn.Linear(d_model, d_model),\n","\t\t\tnn.Sigmoid(),\n","\t\t\tnn.Linear(d_model, n_spks),\n","\t\t)\n","\n","\tdef forward(self, mels):\n","\t\t\"\"\"\n","\t\targs:\n","\t\t\tmels: (batch size, length, 40)\n","\t\treturn:\n","\t\t\tout: (batch size, n_spks)\n","\t\t\"\"\"\n","\t\t# input: [B, seq_len, 40]\n","\t\tout = self.prenet(mels)  # [B, seq_len, d_model]\n","\t\tout = out.permute(1, 0, 2)  # [seq_len, B, d_model]\n","\t\tout = self.encoder_layer(out)  # [seq_len, B, d_model]\n","\t\tout = out.transpose(0, 1)  # [B, seq_len, d_model]\n","\t\t# mean pooling\n","\t\tstats = out.mean(dim=1)  # [B, d_model]\n","\t\tout = self.pred_layer(stats)  # [B, n_spks]\n","\t\treturn out"]},{"cell_type":"markdown","metadata":{},"source":["### Medium\n","\n","Acc=0.81750"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","ClassifierMeidum                              [1, 600]                  2,102,784\n","├─Linear: 1-1                                 [1, 100, 512]             20,992\n","├─TransformerEncoder: 1-2                     [100, 1, 512]             --\n","│    └─ModuleList: 2-1                        --                        --\n","│    │    └─TransformerEncoderLayer: 3-1      [100, 1, 512]             2,102,784\n","│    │    └─TransformerEncoderLayer: 3-2      [100, 1, 512]             2,102,784\n","│    │    └─TransformerEncoderLayer: 3-3      [100, 1, 512]             2,102,784\n","│    │    └─TransformerEncoderLayer: 3-4      [100, 1, 512]             2,102,784\n","│    │    └─TransformerEncoderLayer: 3-5      [100, 1, 512]             2,102,784\n","│    │    └─TransformerEncoderLayer: 3-6      [100, 1, 512]             2,102,784\n","├─Sequential: 1-3                             [1, 600]                  --\n","│    └─Linear: 2-2                            [1, 512]                  262,656\n","│    └─Sigmoid: 2-3                           [1, 512]                  --\n","│    └─Linear: 2-4                            [1, 600]                  307,800\n","===============================================================================================\n","Total params: 15,310,936\n","Trainable params: 15,310,936\n","Non-trainable params: 0\n","Total mult-adds (M): 631.89\n","===============================================================================================\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 12.71\n","Params size (MB): 27.62\n","Estimated Total Size (MB): 40.34\n","==============================================================================================="]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchinfo import summary\n","\n","# TODO: Modify the parameters of the transformer modules in the sample code\n","class ClassifierMeidum(nn.Module):\n","\tdef __init__(self, d_model=512, n_spks=600, dropout=0.1):\n","\t\tsuper().__init__()\n","\t\t# Project the dimension of features from that of input into d_model.\n","\t\tself.prenet = nn.Linear(40, d_model)\n","\t\tself.encoder_layer = nn.TransformerEncoderLayer(\n","\t\t\td_model=d_model, dim_feedforward=1024, nhead=4\n","\t\t)\n","\t\tself.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n","\n","\t\t# Project the the dimension of features from d_model into speaker nums.\n","\t\tself.pred_layer = nn.Sequential(\n","\t\t\tnn.Linear(d_model, d_model),\n","\t\t\tnn.Sigmoid(),\n","\t\t\tnn.Linear(d_model, n_spks),\n","\t\t)\n","\n","\tdef forward(self, mels):\n","\t\t\"\"\"\n","\t\targs:\n","\t\t\tmels: (batch size, length, 40)\n","\t\treturn:\n","\t\t\tout: (batch size, n_spks)\n","\t\t\"\"\"\n","\t\t# input: [B, seq_len, 40]\n","\t\tout = self.prenet(mels)  # [B, seq_len, d_model]\n","\t\tout = out.permute(1, 0, 2)  # [seq_len, B, d_model]\n","\t\t# out = self.encoder_layer(out)  # [seq_len, B, d_model]\n","\t\tout = self.encoder(out)\n","\t\tout = out.transpose(0, 1)  # [B, seq_len, d_model]\n","\t\t# mean pooling\n","\t\tstats = out.mean(dim=1)  # [B, d_model]\n","\t\tout = self.pred_layer(stats)  # [B, n_spks]\n","\t\treturn out\n","\n","summary(ClassifierMeidum(), (1, 100, 40))"]},{"cell_type":"markdown","metadata":{},"source":["### Strong\n","\n","Acc=0.88500"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","ClassifierStrong                              [1, 600]                  --\n","├─Linear: 1-1                                 [1, 100, 512]             20,992\n","├─ConformerBlock: 1-2                         [100, 1, 512]             --\n","│    └─Scale: 2-1                             [100, 1, 512]             --\n","│    │    └─PreNorm: 3-1                      [100, 1, 512]             2,100,736\n","│    └─PreNorm: 2-2                           [100, 1, 512]             --\n","│    │    └─LayerNorm: 3-2                    [100, 1, 512]             1,024\n","│    │    └─Attention: 3-3                    [100, 1, 512]             1,114,688\n","│    └─ConformerConvModule: 2-3               [100, 1, 512]             --\n","│    │    └─Sequential: 3-4                   [100, 1, 512]             1,611,264\n","│    └─Scale: 2-4                             [100, 1, 512]             --\n","│    │    └─PreNorm: 3-5                      [100, 1, 512]             2,100,736\n","│    └─LayerNorm: 2-5                         [100, 1, 512]             1,024\n","├─Sequential: 1-3                             [1, 600]                  --\n","│    └─Linear: 2-6                            [1, 512]                  262,656\n","│    └─Sigmoid: 2-7                           [1, 512]                  --\n","│    └─Linear: 2-8                            [1, 600]                  307,800\n","===============================================================================================\n","Total params: 7,520,920\n","Trainable params: 7,520,920\n","Non-trainable params: 0\n","Total mult-adds (M): 687.04\n","===============================================================================================\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 11.89\n","Params size (MB): 30.08\n","Estimated Total Size (MB): 41.99\n","==============================================================================================="]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchinfo import summary\n","from conformer import ConformerBlock\n","\n","# TODO: Construct Conformer replace Transformer\n","class ClassifierStrong(nn.Module):\n","\tdef __init__(self, d_model=512, n_spks=600, dropout=0.1):\n","\t\tsuper().__init__()\n","\t\t# Project the dimension of features from that of input into d_model.\n","\t\tself.prenet = nn.Linear(40, d_model)\n","\t\tself.conformer = ConformerBlock(dim=d_model,\n","                                            heads=8,\n","                                            ff_mult=4,\n","                                            attn_dropout=dropout,\n","                                            ff_dropout=dropout)\n","\t\t# Project the the dimension of features from d_model into speaker nums.\n","\t\tself.pred_layer = nn.Sequential(\n","\t\t\tnn.Linear(d_model, d_model),\n","\t\t\tnn.Sigmoid(),\n","\t\t\tnn.Linear(d_model, n_spks),\n","\t\t)\n","\n","\tdef forward(self, mels):\n","\t\t\"\"\"\n","\t\targs:\n","\t\t\tmels: (batch size, length, 40)\n","\t\treturn:\n","\t\t\tout: (batch size, n_spks)\n","\t\t\"\"\"\n","\t\t# input: [B, seq_len, 40]\n","\t\tout = self.prenet(mels)  # [B, seq_len, d_model]\n","\t\tout = out.permute(1, 0, 2)  # [seq_len, B, d_model]\n","\t\t# out = self.encoder_layer(out)  # [seq_len, B, d_model]\n","\t\tout = self.conformer(out)\n","\t\tout = out.transpose(0, 1)  # [B, seq_len, d_model]\n","\t\t# mean pooling\n","\t\tstats = out.mean(dim=1)  # [B, d_model]\n","\t\tout = self.pred_layer(stats)  # [B, n_spks]\n","\t\treturn out\n","\n","summary(ClassifierStrong(), (1, 100, 40))"]},{"cell_type":"markdown","metadata":{"id":"W7yX8JinM5Ly"},"source":["# Learning rate schedule\n","- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n","- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n","- The warmup schedule\n","  - Set learning rate to 0 in the beginning.\n","  - The learning rate increases linearly from 0 to initial learning rate during warmup period."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ykt0N1nVJJi2"},"outputs":[],"source":["import math\n","\n","import torch\n","from torch.optim import Optimizer\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","\n","def get_cosine_schedule_with_warmup(\n","\toptimizer: Optimizer,\n","\tnum_warmup_steps: int,\n","\tnum_training_steps: int,\n","\tnum_cycles: float = 0.5,\n","\tlast_epoch: int = -1,\n","):\n","\t\"\"\"\n","\tCreate a schedule with a learning rate that decreases following the values of the cosine function between the\n","\tinitial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n","\tinitial lr set in the optimizer.\n","\n","\tArgs:\n","\t\toptimizer (:class:`~torch.optim.Optimizer`):\n","\t\tThe optimizer for which to schedule the learning rate.\n","\t\tnum_warmup_steps (:obj:`int`):\n","\t\tThe number of steps for the warmup phase.\n","\t\tnum_training_steps (:obj:`int`):\n","\t\tThe total number of training steps.\n","\t\tnum_cycles (:obj:`float`, `optional`, defaults to 0.5):\n","\t\tThe number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n","\t\tfollowing a half-cosine).\n","\t\tlast_epoch (:obj:`int`, `optional`, defaults to -1):\n","\t\tThe index of the last epoch when resuming training.\n","\n","\tReturn:\n","\t\t:obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n","\t\"\"\"\n","\tdef lr_lambda(current_step):\n","\t\t# Warmup\n","\t\tif current_step < num_warmup_steps:\n","\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n","\t\t# decadence\n","\t\tprogress = float(current_step - num_warmup_steps) / float(\n","\t\t\tmax(1, num_training_steps - num_warmup_steps)\n","\t\t)\n","\t\treturn max(\n","\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n","\t\t)\n","\n","\treturn LambdaLR(optimizer, lr_lambda, last_epoch)"]},{"cell_type":"markdown","metadata":{"id":"-LN2XkteM_uH"},"source":["# Model Function\n","- Model forward function."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"N-rr8529JMz0"},"outputs":[],"source":["import torch\n","\n","\n","def model_fn(batch, model, criterion, device):\n","\t\"\"\"Forward a batch through the model.\"\"\"\n","\n","\tmels, labels = batch\n","\tmels = mels.to(device)\n","\tlabels = labels.to(device)\n","\n","\touts = model(mels)\n","\n","\tloss = criterion(outs, labels)\n","\n","\t# Get the speaker id with highest probability.\n","\tpreds = outs.argmax(1)\n","\t# Compute accuracy.\n","\taccuracy = torch.mean((preds == labels).float())\n","\n","\treturn loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"cwM_xyOtNCI2"},"source":["# Validate\n","- Calculate accuracy of the validation set."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"YAiv6kpdJRTJ"},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","\n","\n","def valid(dataloader, model, criterion, device): \n","\t\"\"\"Validate on validation set.\"\"\"\n","\n","\tmodel.eval()\n","\trunning_loss = 0.0\n","\trunning_accuracy = 0.0\n","\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n","\n","\tfor i, batch in enumerate(dataloader):\n","\t\twith torch.no_grad():\n","\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n","\t\t\trunning_loss += loss.item()\n","\t\t\trunning_accuracy += accuracy.item()\n","\n","\t\tpbar.update(dataloader.batch_size)\n","\t\tpbar.set_postfix(\n","\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n","\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n","\t\t)\n","\n","\tpbar.close()\n","\tmodel.train()\n","\n","\treturn running_accuracy / len(dataloader)"]},{"cell_type":"markdown","metadata":{"id":"g6ne9G-eNEdG"},"source":["# Main function"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Usv9s-CuJSG7","outputId":"ae9e1044-1a74-4e50-f387-81c6fd247a0d"},"outputs":[],"source":["from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, random_split\n","\n","\n","def parse_args():\n","\t\"\"\"arguments\"\"\"\n","\tconfig = {\n","\t\t\"data_dir\": \"data/Dataset\",\n","\t\t\"save_path\": \"model.ckpt\",\n","\t\t\"batch_size\": 256,\n","\t\t\"n_workers\": 8,\n","\t\t\"valid_steps\": 2000,\n","\t\t\"warmup_steps\": 1000,\n","\t\t\"save_steps\": 10000,\n","\t\t\"total_steps\": 100000,\n","  \t\t\"lr\": 1e-4,\n","\t\t\"weight_decay\": 1e-4,\n","\t}\n","\n","\treturn config\n","\n","\n","def main(\n","\tdata_dir,\n","\tsave_path,\n","\tbatch_size,\n","\tn_workers,\n","\tvalid_steps,\n","\twarmup_steps,\n","\ttotal_steps,\n","\tsave_steps,\n","\tlr,\n"," \tweight_decay,\n"," \tmodel\n","):\n","\t\"\"\"Main function.\"\"\"\n","\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\tprint(f\"[Info]: Use {device} now!\")\n","\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n","\ttrain_iterator = iter(train_loader)\n","\tprint(f\"[Info]: Finish loading data!\",flush = True)\n","\n","\t# model = Classifier(n_spks=speaker_num).to(device)\n","\t# model = ClassifierMeidum(n_spks=speaker_num).to(device)\n","\tcriterion = nn.CrossEntropyLoss()\n","\toptimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","\tprint(f\"[Info]: Finish creating model!\",flush = True)\n","\n","\tbest_accuracy = -1.0\n","\tbest_state_dict = None\n","\n","\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","\tfor step in range(total_steps):\n","\t\t# Get data\n","\t\ttry:\n","\t\t\tbatch = next(train_iterator)\n","\t\texcept StopIteration:\n","\t\t\ttrain_iterator = iter(train_loader)\n","\t\t\tbatch = next(train_iterator)\n","\n","\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n","\t\tbatch_loss = loss.item()\n","\t\tbatch_accuracy = accuracy.item()\n","\n","\t\t# Updata model\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\tscheduler.step()\n","\t\toptimizer.zero_grad()\n","\n","\t\t# Log\n","\t\tpbar.update()\n","\t\tpbar.set_postfix(\n","\t\t\tloss=f\"{batch_loss:.2f}\",\n","\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n","\t\t\tstep=step + 1,\n","\t\t)\n","\n","\t\t# Do validation\n","\t\tif (step + 1) % valid_steps == 0:\n","\t\t\tpbar.close()\n","\n","\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n","\n","\t\t\t# keep the best model\n","\t\t\tif valid_accuracy > best_accuracy:\n","\t\t\t\tbest_accuracy = valid_accuracy\n","\t\t\t\tbest_state_dict = model.state_dict()\n","\n","\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","\t\t# Save the best model so far.\n","\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n","\t\t\ttorch.save(best_state_dict, save_path)\n","\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n","\n","\tpbar.close()\n","\n","\n","# if __name__ == \"__main__\":\n","# \tmain(**parse_args())"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:27<00:00, 73.65 step/s, accuracy=0.00, loss=5.86, step=2000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10025.89 uttr/s, accuracy=0.01, loss=5.87]\n","Train: 100% 2000/2000 [00:26<00:00, 74.98 step/s, accuracy=0.07, loss=5.35, step=4000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10693.26 uttr/s, accuracy=0.04, loss=5.39]\n","Train: 100% 2000/2000 [00:27<00:00, 73.42 step/s, accuracy=0.09, loss=5.06, step=6000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10698.28 uttr/s, accuracy=0.07, loss=5.03]\n","Train: 100% 2000/2000 [00:27<00:00, 73.22 step/s, accuracy=0.09, loss=4.76, step=8000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9899.35 uttr/s, accuracy=0.09, loss=4.76]\n","Train: 100% 2000/2000 [00:26<00:00, 74.46 step/s, accuracy=0.17, loss=4.45, step=1e+4] \n","Valid:  99% 5632/5667 [00:00<00:00, 9853.57 uttr/s, accuracy=0.11, loss=4.57] \n","Train:   1% 16/2000 [00:00<00:23, 83.99 step/s, accuracy=0.11, loss=4.59, step=1e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 10000, best model saved. (accuracy=0.1135)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:26<00:00, 74.87 step/s, accuracy=0.11, loss=4.38, step=12000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10577.45 uttr/s, accuracy=0.13, loss=4.40]\n","Train: 100% 2000/2000 [00:26<00:00, 74.80 step/s, accuracy=0.18, loss=4.15, step=14000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9242.96 uttr/s, accuracy=0.15, loss=4.26]\n","Train: 100% 2000/2000 [00:27<00:00, 73.64 step/s, accuracy=0.16, loss=4.13, step=16000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10031.83 uttr/s, accuracy=0.17, loss=4.14]\n","Train: 100% 2000/2000 [00:27<00:00, 73.91 step/s, accuracy=0.20, loss=4.00, step=18000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9639.94 uttr/s, accuracy=0.19, loss=4.02] \n","Train: 100% 2000/2000 [00:26<00:00, 74.58 step/s, accuracy=0.25, loss=3.84, step=2e+4]  \n","Valid:  99% 5632/5667 [00:00<00:00, 9815.25 uttr/s, accuracy=0.20, loss=3.91] \n","Train:   1% 23/2000 [00:00<00:18, 104.59 step/s, accuracy=0.26, loss=3.73, step=2e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 20000, best model saved. (accuracy=0.2040)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:26<00:00, 74.67 step/s, accuracy=0.21, loss=3.82, step=22000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9459.80 uttr/s, accuracy=0.22, loss=3.83]\n","Train: 100% 2000/2000 [00:27<00:00, 72.75 step/s, accuracy=0.26, loss=3.69, step=24000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9947.50 uttr/s, accuracy=0.23, loss=3.73] \n","Train: 100% 2000/2000 [00:27<00:00, 71.76 step/s, accuracy=0.30, loss=3.59, step=26000] \n","Valid:  99% 5632/5667 [00:00<00:00, 7324.08 uttr/s, accuracy=0.25, loss=3.65]\n","Train: 100% 2000/2000 [00:27<00:00, 71.90 step/s, accuracy=0.26, loss=3.54, step=28000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10434.53 uttr/s, accuracy=0.26, loss=3.60]\n","Train: 100% 2000/2000 [00:27<00:00, 73.95 step/s, accuracy=0.29, loss=3.52, step=3e+4]  \n","Valid:  99% 5632/5667 [00:00<00:00, 9570.69 uttr/s, accuracy=0.26, loss=3.52]\n","Train:   1% 17/2000 [00:00<00:22, 89.04 step/s, accuracy=0.33, loss=3.36, step=3e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 30000, best model saved. (accuracy=0.2631)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:26<00:00, 74.16 step/s, accuracy=0.32, loss=3.30, step=32000] \n","Valid:  99% 5632/5667 [00:00<00:00, 8986.95 uttr/s, accuracy=0.28, loss=3.44]\n","Train: 100% 2000/2000 [00:27<00:00, 73.51 step/s, accuracy=0.30, loss=3.34, step=34000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9536.60 uttr/s, accuracy=0.29, loss=3.41]\n","Train: 100% 2000/2000 [00:26<00:00, 74.33 step/s, accuracy=0.31, loss=3.20, step=36000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9272.72 uttr/s, accuracy=0.30, loss=3.35] \n","Train: 100% 2000/2000 [00:27<00:00, 73.76 step/s, accuracy=0.38, loss=3.07, step=38000] \n","Valid:  99% 5632/5667 [00:00<00:00, 10224.81 uttr/s, accuracy=0.31, loss=3.29]\n","Train: 100% 2000/2000 [00:27<00:00, 73.39 step/s, accuracy=0.32, loss=3.12, step=4e+4]  \n","Valid:  99% 5632/5667 [00:00<00:00, 9461.23 uttr/s, accuracy=0.31, loss=3.26]\n","Train:   1% 16/2000 [00:00<00:24, 80.90 step/s, accuracy=0.31, loss=3.13, step=4e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 40000, best model saved. (accuracy=0.3113)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:26<00:00, 74.22 step/s, accuracy=0.36, loss=2.95, step=42000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9531.26 uttr/s, accuracy=0.32, loss=3.22] \n","Train: 100% 2000/2000 [00:26<00:00, 74.24 step/s, accuracy=0.33, loss=3.14, step=44000] \n","Valid:  99% 5632/5667 [00:00<00:00, 11055.45 uttr/s, accuracy=0.32, loss=3.20]\n","Train: 100% 2000/2000 [00:27<00:00, 73.39 step/s, accuracy=0.33, loss=3.16, step=46000] \n","Valid:  99% 5632/5667 [00:00<00:00, 11490.14 uttr/s, accuracy=0.33, loss=3.13]\n","Train: 100% 2000/2000 [00:27<00:00, 73.74 step/s, accuracy=0.34, loss=3.13, step=48000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9906.90 uttr/s, accuracy=0.34, loss=3.12]\n","Train: 100% 2000/2000 [00:27<00:00, 73.44 step/s, accuracy=0.38, loss=3.02, step=5e+4]  \n","Valid:  99% 5632/5667 [00:00<00:00, 10051.16 uttr/s, accuracy=0.35, loss=3.11]\n","Train:   1% 25/2000 [00:00<00:15, 124.04 step/s, accuracy=0.38, loss=2.97, step=5e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 50000, best model saved. (accuracy=0.3459)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:26<00:00, 74.32 step/s, accuracy=0.38, loss=2.94, step=52000] \n","Valid:  99% 5632/5667 [00:00<00:00, 9249.46 uttr/s, accuracy=0.34, loss=3.07]\n","Train: 100% 2000/2000 [00:27<00:00, 71.53 step/s, accuracy=0.36, loss=3.08, step=54000] \n","Valid:  99% 5632/5667 [00:00<00:00, 12280.83 uttr/s, accuracy=0.35, loss=3.05]\n","Train: 100% 2000/2000 [00:32<00:00, 61.28 step/s, accuracy=0.42, loss=2.79, step=56000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12062.84 uttr/s, accuracy=0.36, loss=3.02]\n","Train: 100% 2000/2000 [00:32<00:00, 61.59 step/s, accuracy=0.38, loss=2.85, step=58000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12352.86 uttr/s, accuracy=0.37, loss=2.99]\n","Train: 100% 2000/2000 [00:32<00:00, 60.95 step/s, accuracy=0.36, loss=2.94, step=6e+4] \n","Valid:  99% 5632/5667 [00:00<00:00, 9688.19 uttr/s, accuracy=0.36, loss=2.97] \n","Train:   1% 13/2000 [00:00<00:29, 66.75 step/s, accuracy=0.43, loss=2.83, step=6e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 60000, best model saved. (accuracy=0.3651)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:32<00:00, 60.99 step/s, accuracy=0.41, loss=2.85, step=62000] \n","Valid:  99% 5632/5667 [00:00<00:00, 12413.91 uttr/s, accuracy=0.36, loss=2.97]\n","Train: 100% 2000/2000 [00:32<00:00, 61.23 step/s, accuracy=0.36, loss=2.94, step=64000]\n","Valid:  99% 5632/5667 [00:00<00:00, 11817.77 uttr/s, accuracy=0.36, loss=2.97]\n","Train: 100% 2000/2000 [00:32<00:00, 61.51 step/s, accuracy=0.39, loss=2.80, step=66000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12090.36 uttr/s, accuracy=0.37, loss=2.95]\n","Train: 100% 2000/2000 [00:32<00:00, 61.19 step/s, accuracy=0.42, loss=2.65, step=68000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12297.87 uttr/s, accuracy=0.36, loss=2.93]\n","Train: 100% 2000/2000 [00:32<00:00, 61.37 step/s, accuracy=0.38, loss=2.85, step=7e+4] \n","Valid:  99% 5632/5667 [00:00<00:00, 12539.82 uttr/s, accuracy=0.37, loss=2.93]\n","Train:   1% 13/2000 [00:00<00:29, 66.98 step/s, accuracy=0.41, loss=2.79, step=7e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 70000, best model saved. (accuracy=0.3690)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:32<00:00, 61.23 step/s, accuracy=0.41, loss=2.73, step=72000]\n","Valid:  99% 5632/5667 [00:00<00:00, 11763.35 uttr/s, accuracy=0.38, loss=2.89]\n","Train: 100% 2000/2000 [00:32<00:00, 61.76 step/s, accuracy=0.43, loss=2.75, step=74000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12101.90 uttr/s, accuracy=0.37, loss=2.90]\n","Train: 100% 2000/2000 [00:32<00:00, 61.09 step/s, accuracy=0.39, loss=2.86, step=76000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12515.46 uttr/s, accuracy=0.37, loss=2.90]\n","Train: 100% 2000/2000 [00:32<00:00, 61.71 step/s, accuracy=0.41, loss=2.89, step=78000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12706.38 uttr/s, accuracy=0.38, loss=2.89]\n","Train: 100% 2000/2000 [00:33<00:00, 60.58 step/s, accuracy=0.39, loss=2.79, step=8e+4] \n","Valid:  99% 5632/5667 [00:00<00:00, 12223.80 uttr/s, accuracy=0.38, loss=2.88]\n","Train:   1% 13/2000 [00:00<00:29, 66.86 step/s, accuracy=0.43, loss=2.74, step=8e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 80000, best model saved. (accuracy=0.3816)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:32<00:00, 61.68 step/s, accuracy=0.41, loss=2.82, step=82000]\n","Valid:  99% 5632/5667 [00:00<00:00, 11595.46 uttr/s, accuracy=0.38, loss=2.89]\n","Train: 100% 2000/2000 [00:32<00:00, 61.25 step/s, accuracy=0.45, loss=2.61, step=84000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12471.21 uttr/s, accuracy=0.39, loss=2.88]\n","Train: 100% 2000/2000 [00:32<00:00, 61.41 step/s, accuracy=0.44, loss=2.47, step=86000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12256.37 uttr/s, accuracy=0.37, loss=2.88]\n","Train: 100% 2000/2000 [00:32<00:00, 61.20 step/s, accuracy=0.36, loss=2.69, step=88000]\n","Valid:  99% 5632/5667 [00:00<00:00, 10667.71 uttr/s, accuracy=0.38, loss=2.88]\n","Train: 100% 2000/2000 [00:32<00:00, 61.01 step/s, accuracy=0.42, loss=2.65, step=9e+4] \n","Valid:  99% 5632/5667 [00:00<00:00, 11860.35 uttr/s, accuracy=0.38, loss=2.88]\n","Train:   1% 13/2000 [00:00<00:28, 68.55 step/s, accuracy=0.43, loss=2.54, step=9e+4]"]},{"name":"stdout","output_type":"stream","text":["Step 90000, best model saved. (accuracy=0.3851)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [00:32<00:00, 61.59 step/s, accuracy=0.41, loss=2.75, step=92000] \n","Valid:  99% 5632/5667 [00:00<00:00, 12069.01 uttr/s, accuracy=0.39, loss=2.86]\n","Train: 100% 2000/2000 [00:33<00:00, 60.32 step/s, accuracy=0.44, loss=2.62, step=94000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12359.42 uttr/s, accuracy=0.38, loss=2.86]\n","Train: 100% 2000/2000 [00:32<00:00, 61.32 step/s, accuracy=0.45, loss=2.59, step=96000]\n","Valid:  99% 5632/5667 [00:00<00:00, 11856.84 uttr/s, accuracy=0.38, loss=2.86]\n","Train: 100% 2000/2000 [00:32<00:00, 61.09 step/s, accuracy=0.41, loss=2.67, step=98000]\n","Valid:  99% 5632/5667 [00:00<00:00, 12027.98 uttr/s, accuracy=0.39, loss=2.86]\n","Train: 100% 2000/2000 [00:32<00:00, 61.54 step/s, accuracy=0.41, loss=2.71, step=1e+5] \n","Valid:  99% 5632/5667 [00:00<00:00, 12039.17 uttr/s, accuracy=0.37, loss=2.89]\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n"]},{"name":"stdout","output_type":"stream","text":["Step 100000, best model saved. (accuracy=0.3888)\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Classifier(n_spks=600).to(device)\n","config = parse_args()\n","config['save_path'] = 'model_simple.ckpt'\n","config['lr'] = 1e-3\n","config['total_steps'] = 70000\n","main(**config, model=model)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [13:41<00:00,  2.44 step/s, accuracy=0.31, loss=3.66, step=2000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1633.75 uttr/s, accuracy=0.27, loss=3.76]\n","Train: 100% 2000/2000 [15:30<00:00,  2.15 step/s, accuracy=0.62, loss=1.92, step=4000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1627.75 uttr/s, accuracy=0.53, loss=2.29]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.78, loss=1.07, step=6000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1633.76 uttr/s, accuracy=0.66, loss=1.60]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.83, loss=0.79, step=8000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1638.21 uttr/s, accuracy=0.72, loss=1.28]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.91, loss=0.47, step=1e+4]\n","Valid:  99% 5632/5667 [00:03<00:00, 1640.79 uttr/s, accuracy=0.75, loss=1.12]\n","Train:   0% 0/2000 [00:00<?, ? step/s]"]},{"name":"stdout","output_type":"stream","text":["Step 10000, best model saved. (accuracy=0.7495)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.93, loss=0.37, step=12000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1640.97 uttr/s, accuracy=0.77, loss=1.02]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.95, loss=0.24, step=14000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1643.02 uttr/s, accuracy=0.79, loss=0.95]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.95, loss=0.21, step=16000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1629.35 uttr/s, accuracy=0.80, loss=0.90]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.98, loss=0.13, step=18000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1641.45 uttr/s, accuracy=0.81, loss=0.91]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.97, loss=0.14, step=2e+4] \n","Valid:  99% 5632/5667 [00:03<00:00, 1644.28 uttr/s, accuracy=0.81, loss=0.89]\n","Train:   0% 0/2000 [00:00<?, ? step/s]"]},{"name":"stdout","output_type":"stream","text":["Step 20000, best model saved. (accuracy=0.8077)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.96, loss=0.11, step=22000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1635.75 uttr/s, accuracy=0.82, loss=0.86]\n","Train: 100% 2000/2000 [13:40<00:00,  2.44 step/s, accuracy=0.97, loss=0.15, step=24000]\n","Valid:  99% 5632/5667 [00:03<00:00, 1632.18 uttr/s, accuracy=0.83, loss=0.84]\n","Train: 100% 2000/2000 [07:41<00:00,  4.33 step/s, accuracy=0.98, loss=0.08, step=26000]\n","Valid:  99% 5632/5667 [00:01<00:00, 3130.88 uttr/s, accuracy=0.82, loss=0.87]\n","Train:  41% 825/2000 [02:39<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_medium.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ClassifierMeidum(n_spks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 67\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(data_dir, save_path, batch_size, n_workers, valid_steps, warmup_steps, total_steps, save_steps, lr, weight_decay, model)\u001b[0m\n\u001b[1;32m     64\u001b[0m \ttrain_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[1;32m     65\u001b[0m \tbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iterator)\n\u001b[0;32m---> 67\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     69\u001b[0m batch_accuracy \u001b[38;5;241m=\u001b[39m accuracy\u001b[38;5;241m.\u001b[39mitem()\n","Cell \u001b[0;32mIn[9], line 8\u001b[0m, in \u001b[0;36mmodel_fn\u001b[0;34m(batch, model, criterion, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward a batch through the model.\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m mels, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m----> 8\u001b[0m mels \u001b[38;5;241m=\u001b[39m \u001b[43mmels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m outs \u001b[38;5;241m=\u001b[39m model(mels)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["config = parse_args()\n","config['save_path'] = 'model_medium.ckpt'\n","config['lr'] = 1e-4\n","config['total_steps'] = 25000\n","model = ClassifierMeidum(n_spks=600).to(device)\n","main(**config, model=model)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=0.12, loss=4.72, step=2000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4419.23 uttr/s, accuracy=0.11, loss=4.76]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=0.22, loss=3.84, step=4000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4448.73 uttr/s, accuracy=0.23, loss=3.82]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=0.33, loss=3.20, step=6000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4460.21 uttr/s, accuracy=0.33, loss=3.22]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.41, loss=2.78, step=8000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4400.18 uttr/s, accuracy=0.40, loss=2.81]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=0.57, loss=2.21, step=1e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4459.88 uttr/s, accuracy=0.45, loss=2.50]\n","                                                                                      \n","Train:  41% 825/2000 [22:46<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 10000, best model saved. (accuracy=0.4538)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.58, loss=1.95, step=12000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4440.21 uttr/s, accuracy=0.50, loss=2.26]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.64, loss=1.57, step=14000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4443.42 uttr/s, accuracy=0.56, loss=2.03]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.67, loss=1.48, step=16000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4439.27 uttr/s, accuracy=0.58, loss=1.89]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.70, loss=1.36, step=18000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4459.22 uttr/s, accuracy=0.62, loss=1.72]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=0.75, loss=1.23, step=2e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4472.96 uttr/s, accuracy=0.66, loss=1.56]\n","                                                                                      \n","Train:  41% 825/2000 [42:43<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 20000, best model saved. (accuracy=0.6555)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=0.78, loss=0.95, step=22000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4442.53 uttr/s, accuracy=0.66, loss=1.51]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.75, loss=0.97, step=24000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4493.70 uttr/s, accuracy=0.68, loss=1.42]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.82, loss=0.80, step=26000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4443.65 uttr/s, accuracy=0.70, loss=1.33]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.84, loss=0.70, step=28000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4419.98 uttr/s, accuracy=0.70, loss=1.30]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.88, loss=0.52, step=3e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4135.32 uttr/s, accuracy=0.72, loss=1.25]\n","                                                                                      \n","Train:  41% 825/2000 [1:02:40<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 30000, best model saved. (accuracy=0.7200)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:58<00:00,  8.39 step/s, accuracy=0.90, loss=0.42, step=32000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4456.67 uttr/s, accuracy=0.72, loss=1.24]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.89, loss=0.46, step=34000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4465.51 uttr/s, accuracy=0.73, loss=1.16]\n","Train: 100% 2000/2000 [03:57<00:00,  8.40 step/s, accuracy=0.91, loss=0.38, step=36000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4486.19 uttr/s, accuracy=0.74, loss=1.16]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.93, loss=0.33, step=38000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4483.06 uttr/s, accuracy=0.74, loss=1.13]\n","Train: 100% 2000/2000 [03:58<00:00,  8.39 step/s, accuracy=0.95, loss=0.29, step=4e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4496.13 uttr/s, accuracy=0.75, loss=1.13]\n","                                                                                        \n","Train:  41% 825/2000 [1:22:37<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 40000, best model saved. (accuracy=0.7504)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.97, loss=0.25, step=42000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4469.53 uttr/s, accuracy=0.76, loss=1.09]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.98, loss=0.16, step=44000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4492.58 uttr/s, accuracy=0.77, loss=1.11]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.95, loss=0.20, step=46000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4503.83 uttr/s, accuracy=0.76, loss=1.08]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.13, step=48000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4461.54 uttr/s, accuracy=0.76, loss=1.09]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.98, loss=0.12, step=5e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4460.48 uttr/s, accuracy=0.76, loss=1.09]\n","                                                                                        \n","Train:  41% 825/2000 [1:42:34<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 50000, best model saved. (accuracy=0.7653)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.13, step=52000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4484.49 uttr/s, accuracy=0.77, loss=1.05]\n","Train: 100% 2000/2000 [03:57<00:00,  8.40 step/s, accuracy=0.98, loss=0.13, step=54000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4513.82 uttr/s, accuracy=0.77, loss=1.06]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.97, loss=0.14, step=56000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4483.92 uttr/s, accuracy=0.77, loss=1.06]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.09, step=58000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4434.99 uttr/s, accuracy=0.77, loss=1.04]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.98, loss=0.12, step=6e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4542.63 uttr/s, accuracy=0.78, loss=1.08]\n","                                                                                        \n","Train:  41% 825/2000 [2:02:30<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 60000, best model saved. (accuracy=0.7757)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.98, loss=0.08, step=62000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4467.73 uttr/s, accuracy=0.78, loss=1.04]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=1.00, loss=0.07, step=64000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4488.56 uttr/s, accuracy=0.78, loss=1.07]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.07, step=66000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4424.63 uttr/s, accuracy=0.77, loss=1.05]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.05, step=68000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4454.69 uttr/s, accuracy=0.78, loss=1.04]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.05, step=7e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4500.81 uttr/s, accuracy=0.78, loss=1.05]\n","                                                                                        \n","Train:  41% 825/2000 [2:22:27<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 70000, best model saved. (accuracy=0.7802)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=1.00, loss=0.04, step=72000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4041.20 uttr/s, accuracy=0.79, loss=1.02]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=0.99, loss=0.04, step=74000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4454.85 uttr/s, accuracy=0.79, loss=1.00]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.04, step=76000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4452.50 uttr/s, accuracy=0.78, loss=1.04]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.03, step=78000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4490.91 uttr/s, accuracy=0.78, loss=1.03]\n","Train: 100% 2000/2000 [04:00<00:00,  8.33 step/s, accuracy=1.00, loss=0.04, step=8e+4]\n","Valid:  99% 5632/5667 [00:02<00:00, 2216.82 uttr/s, accuracy=0.78, loss=1.06]\n","                                                                                        \n","Train:  41% 825/2000 [2:42:27<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 80000, best model saved. (accuracy=0.7903)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.06, step=82000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4529.33 uttr/s, accuracy=0.78, loss=1.03]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.03, step=84000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4475.71 uttr/s, accuracy=0.79, loss=1.05]\n","Train: 100% 2000/2000 [03:58<00:00,  8.40 step/s, accuracy=1.00, loss=0.04, step=86000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4500.24 uttr/s, accuracy=0.79, loss=1.03]\n","Train: 100% 2000/2000 [03:57<00:00,  8.40 step/s, accuracy=1.00, loss=0.04, step=88000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4497.13 uttr/s, accuracy=0.78, loss=1.04]\n","Train: 100% 2000/2000 [03:57<00:00,  8.42 step/s, accuracy=1.00, loss=0.03, step=9e+4]\n","Valid:  99% 5632/5667 [00:01<00:00, 4500.47 uttr/s, accuracy=0.79, loss=1.01]\n","                                                                                        \n","Train:  41% 825/2000 [3:02:23<03:45,  5.20 step/s, accuracy=0.99, loss=0.06, step=26825]"]},{"name":"stdout","output_type":"stream","text":["Step 90000, best model saved. (accuracy=0.7903)\n"]},{"name":"stderr","output_type":"stream","text":["Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.03, step=92000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4530.40 uttr/s, accuracy=0.78, loss=1.04]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.03, step=94000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4470.35 uttr/s, accuracy=0.78, loss=1.02]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.02, step=96000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4501.41 uttr/s, accuracy=0.79, loss=1.04]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.03, step=98000]\n","Valid:  99% 5632/5667 [00:01<00:00, 4499.72 uttr/s, accuracy=0.78, loss=1.05]\n","Train: 100% 2000/2000 [03:57<00:00,  8.41 step/s, accuracy=1.00, loss=0.03, step=1e+5]\n","Valid:  99% 5632/5667 [00:01<00:00, 4526.14 uttr/s, accuracy=0.79, loss=1.02]\n","                                                                                        \n","Train:   0% 0/2000 [00:00<?, ? step/s]5.20 step/s, accuracy=0.99, loss=0.06, step=26825]\n"]},{"name":"stdout","output_type":"stream","text":["Step 100000, best model saved. (accuracy=0.7905)\n"]}],"source":["config = parse_args()\n","config['save_path'] = 'model_strong.ckpt'\n","config['lr'] = 1e-4\n","config['total_steps'] = 100000\n","model = ClassifierStrong(n_spks=600).to(device)\n","main(**config, model=model)"]},{"cell_type":"markdown","metadata":{"id":"NLatBYAhNNMx"},"source":["# Inference\n","\n","## Dataset of inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"efS4pCmAJXJH"},"outputs":[],"source":["import os\n","import json\n","import torch\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","\n","\n","class InferenceDataset(Dataset):\n","\tdef __init__(self, data_dir):\n","\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n","\t\tmetadata = json.load(testdata_path.open())\n","\t\tself.data_dir = data_dir\n","\t\tself.data = metadata[\"utterances\"]\n","\n","\tdef __len__(self):\n","\t\treturn len(self.data)\n","\n","\tdef __getitem__(self, index):\n","\t\tutterance = self.data[index]\n","\t\tfeat_path = utterance[\"feature_path\"]\n","\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","\t\treturn feat_path, mel\n","\n","\n","def inference_collate_batch(batch):\n","\t\"\"\"Collate a batch of data.\"\"\"\n","\tfeat_paths, mels = zip(*batch)\n","\n","\treturn feat_paths, torch.stack(mels)"]},{"cell_type":"markdown","metadata":{"id":"tl0WnYwxNK_S"},"source":["## Main funcrion of Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103,"referenced_widgets":["86ec61177f98486c9fb2e64bb6df2c49","182f99951f374b4baccc8c3ab97a9bad","df57c5567db445f082b46cc4d1eebe93","b5c19144f75d46ee9947696707c268e3","db336c75a03e4c44b90dcb9a31d37ea2","d048cedf2e7947c5b5019d84df464751","49ae3b44a8644087b2b0a218a22874fc","2533ff88e88944b8b53b527f14ae8e8a","ac5fd2b03cbf4254b2fd721f868a0eaf","cf1d709e33cc4dac949b9c395c5bcb84","515f5394b8ff42ffa3343d96aabf0041"]},"id":"i8SAbuXEJb2A","outputId":"5dbe7c78-734e-48f7-bf8a-7f05923a4d72"},"outputs":[],"source":["import json\n","import csv\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","def parse_args():\n","\t\"\"\"arguments\"\"\"\n","\tconfig = {\n","\t\t\"data_dir\": \"./data/Dataset\",\n","\t\t\"model_path\": \"./model.ckpt\",\n","\t\t\"output_path\": \"./output.csv\",\n","\t}\n","\n","\treturn config\n","\n","\n","def main(\n","\tdata_dir,\n","\tmodel_path,\n","\toutput_path,\n","):\n","\t\"\"\"Main function.\"\"\"\n","\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\tprint(f\"[Info]: Use {device} now!\")\n","\n","\tmapping_path = Path(data_dir) / \"mapping.json\"\n","\tmapping = json.load(mapping_path.open())\n","\n","\tdataset = InferenceDataset(data_dir)\n","\tdataloader = DataLoader(\n","\t\tdataset,\n","\t\tbatch_size=1,\n","\t\tshuffle=False,\n","\t\tdrop_last=False,\n","\t\tnum_workers=8,\n","\t\tcollate_fn=inference_collate_batch,\n","\t)\n","\tprint(f\"[Info]: Finish loading data!\",flush = True)\n","\n","\tspeaker_num = len(mapping[\"id2speaker\"])\n","\tmodel = Classifier(n_spks=speaker_num).to(device)\n","\tmodel.load_state_dict(torch.load(model_path))\n","\tmodel.eval()\n","\tprint(f\"[Info]: Finish creating model!\",flush = True)\n","\n","\tresults = [[\"Id\", \"Category\"]]\n","\tfor feat_paths, mels in tqdm(dataloader):\n","\t\twith torch.no_grad():\n","\t\t\tmels = mels.to(device)\n","\t\t\touts = model(mels)\n","\t\t\tpreds = outs.argmax(1).cpu().numpy()\n","\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n","\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n","\n","\twith open(output_path, 'w', newline='') as csvfile:\n","\t\twriter = csv.writer(csvfile)\n","\t\twriter.writerows(results)\n","\n","\n","if __name__ == \"__main__\":\n","\tmain(**parse_args())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!rm -r Dataset \n","#for the usage of \"Save Version\", only save the output needed to reduce the waiting time"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"d2l","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"},"widgets":{"application/vnd.jupyter.widget-state+json":{"182f99951f374b4baccc8c3ab97a9bad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d048cedf2e7947c5b5019d84df464751","placeholder":"​","style":"IPY_MODEL_49ae3b44a8644087b2b0a218a22874fc","value":"100%"}},"2533ff88e88944b8b53b527f14ae8e8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49ae3b44a8644087b2b0a218a22874fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"515f5394b8ff42ffa3343d96aabf0041":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86ec61177f98486c9fb2e64bb6df2c49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_182f99951f374b4baccc8c3ab97a9bad","IPY_MODEL_df57c5567db445f082b46cc4d1eebe93","IPY_MODEL_b5c19144f75d46ee9947696707c268e3"],"layout":"IPY_MODEL_db336c75a03e4c44b90dcb9a31d37ea2"}},"ac5fd2b03cbf4254b2fd721f868a0eaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5c19144f75d46ee9947696707c268e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf1d709e33cc4dac949b9c395c5bcb84","placeholder":"​","style":"IPY_MODEL_515f5394b8ff42ffa3343d96aabf0041","value":" 8000/8000 [00:25&lt;00:00, 350.10it/s]"}},"cf1d709e33cc4dac949b9c395c5bcb84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d048cedf2e7947c5b5019d84df464751":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db336c75a03e4c44b90dcb9a31d37ea2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df57c5567db445f082b46cc4d1eebe93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2533ff88e88944b8b53b527f14ae8e8a","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac5fd2b03cbf4254b2fd721f868a0eaf","value":8000}}}}},"nbformat":4,"nbformat_minor":4}
