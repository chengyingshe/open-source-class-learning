{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT (Generative Pre-trained Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # use mirror\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "\n",
    "# remove all columns except 'text'\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n",
    "dataset = concatenate_datasets([bookcorpus, wiki])\n",
    "# split the dataset: 90% for training, 10% for testing\n",
    "d = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_txt(dataset, output_filename='data.txt'):\n",
    "    \"\"\"Save dataset to local txt file\"\"\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for t in dataset['text']:\n",
    "            print(t, file=f)\n",
    "        \n",
    "dataset_to_txt(d['train'], 'train.txt')\n",
    "dataset_to_txt(d['test'], 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "import json\n",
    "import os\n",
    "\n",
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "\n",
    "# 仅根据训练集合训练词元分析器\n",
    "files = [\"train.txt\"]\n",
    "# BERT中采用的默认词表大小为30522，可以随意修改\n",
    "vocab_size = 30_522\n",
    "# 最大序列长度，该值越小，训练速度越快\n",
    "max_length = 512\n",
    "# 是否将长样本截断\n",
    "truncate_longer_samples = False\n",
    "\n",
    "# 初始化WordPiece词元分析器\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# 训练词元分析器\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# 允许截断达到最大512个词元\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "model_path = \"pretrained-bert\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "# 保存词元分析器模型\n",
    "tokenizer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一些词元分析器中的配置保存到配置文件，包括特殊词元、转换为小写、最大序列长度等\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "  tokenizer_cfg = {\n",
    "      \"do_lower_case\": True,\n",
    "      \"unk_token\": \"[UNK]\",\n",
    "      \"sep_token\": \"[SEP]\",\n",
    "      \"pad_token\": \"[PAD]\",\n",
    "      \"cls_token\": \"[CLS]\",\n",
    "      \"mask_token\": \"[MASK]\",\n",
    "      \"model_max_length\": max_length,\n",
    "      \"max_len\": max_length,\n",
    "  }\n",
    "  json.dump(tokenizer_cfg, f)\n",
    "\n",
    "# 当词元分析器进行训练和配置时，将其装载到BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"使用词元分析对句子进行处理并截断的映射函数（Mapping function）\"\"\"\n",
    "    return tokenizer(examples[\"text\"], \n",
    "                     truncation=True,  # 长度超过时截断\n",
    "                     padding=\"max_length\",  # 当长度不够时padding至max_length\n",
    "                     max_length=max_length, \n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"使用词元分析对句子进行处理且不截断的映射函数（Mapping function）\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# 编码函数将依赖于 truncate_longer_samples 变量\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "# 对训练数据集进行分词处理\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# 对测试数据集进行分词处理\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:  # 当截断时使用Torch张量计算（cuda加速）\n",
    "    # 移除其他列，将 input_ids 和 attention_mask 设置为 PyTorch 张量\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # 移除其他列，将它们保留为 Python 列表\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# 主要数据处理函数，拼接数据集中的所有文本并生成最大序列长度的块\n",
    "def group_texts(examples):\n",
    "    # 拼接所有文本\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 舍弃了剩余部分，如果模型支持填充而不是舍弃，则可以根据需要自定义这部分\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # 按照最大长度分割成块\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# 请注意，使用 batched=True，此映射一次处理 1000 个文本\n",
    "# 为了加速这一部分，使用了多进程处理\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                      desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # 将它们从列表转换为 PyTorch 张量\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# 使用配置文件初始化模型\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# 初始化数据整理器，随机屏蔽 20%（默认为 15%）的标记\n",
    "# 用于掩盖语言建模（MLM）任务\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # 输出目录，用于保存模型检查点\n",
    "    evaluation_strategy=\"steps\",    # 每隔 `logging_steps` 步进行一次评估\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,            # 训练时的轮数，可以根据需要进行调整\n",
    "    per_device_train_batch_size=64, # 训练批量大小，可以根据 GPU 内存容量将其设置得尽可能大\n",
    "    gradient_accumulation_steps=8,  # 在更新权重之前累积梯度\n",
    "    per_device_eval_batch_size=64,  # 评估批量大小\n",
    "    logging_steps=1000,             # 每隔 1000 步进行一次评估，记录并保存模型检查点\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # 是否在训练结束时加载最佳模型（根据损失）\n",
    "    # save_total_limit=3,           # 如果磁盘空间有限，则可以限制只保存 3 个模型权重\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 加载模型检查点\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-10000\"))\n",
    "# 加载词元分析器\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 进行预测\n",
    "examples = [\n",
    "  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "  for prediction in fill_mask(example):\n",
    "    print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$RMS(\\alpha) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\alpha_i)^2}$$\n",
    "$$\\bar{\\alpha_i} = \\frac{\\alpha_i}{RMS(\\alpha)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps  # 防止分母为0\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        dtype = hidden_states.dtype\n",
    "        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n",
    "        \n",
    "        return (self.weight * hidden_states).to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
