{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "download_shell = (\n",
    "    \"huggingface-cli download --local-dir-use-symlinks False --resume-download  --repo-type dataset %s --local-dir %s\"\n",
    "    % ('togethercomputer/RedPajama-Data-1T', 'data/RedPajama')\n",
    ")\n",
    "os.system(download_shell)\n",
    "\n",
    "# 将以.jsonl结尾的文件都放入以_分隔的第一个元素的目录下\n",
    "import shutil\n",
    "\n",
    "data_dir = 'data/RedPajama'\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.jsonl'):\n",
    "        new_dir_name = file.split('_')[0]\n",
    "        new_dir_path = os.path.join(data_dir, new_dir_name)\n",
    "        if not os.path.exists(new_dir_path):\n",
    "            os.makedirs(new_dir_path)\n",
    "\n",
    "        old_file_path = os.path.join(data_dir, file)\n",
    "        new_file_path = os.path.join(new_dir_path, file)\n",
    "\n",
    "        shutil.move(old_file_path, new_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: NFC Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting to 1 for number of processes\n",
      "Parsed 0 input files. Files written : 0it [00:00, ?it/s]  | 0/1 [00:00<?, ?it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "! python preprocessing/normalize_text.py --data_dir data/RedPajama/arxiv/ --target_dir data/RedPajama_norm/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter Short Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing, writing to disk!\n"
     ]
    }
   ],
   "source": [
    "! python preprocessing/filter.py data/RedPajama_norm/arxiv/ data/RedPajama_filtered.pickle 64 arxiv 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: MinHash Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "1it [00:00, 51.63it/s]\n",
      "100%|███████████████████████████████████████████| 64/64 [00:18<00:00,  3.48it/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.43s/it]\n"
     ]
    }
   ],
   "source": [
    "! python dedup/to_hash.py arxiv data/RedPajama_norm/arxiv/ data/RedPajama_minhash/arxiv/ 64 0 0 0 -w 13 -k 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Duplicate Pairs Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Processed 0.0%. 0.0003256797790527344\n",
      "1: Processed 0.0%. 0.0003390312194824219\n",
      "2: Processed 0.0%. 0.0002963542938232422\n",
      "3: Processed 0.0%. 0.00045037269592285156\n",
      "4: Processed 0.0%. 0.00028967857360839844\n",
      "5: Processed 0.0%. 0.0002856254577636719\n",
      "6: Processed 0.0%. 0.00029730796813964844\n",
      "8: Processed 0.0%. 0.00031065940856933594\n",
      "7: Processed 0.0%. 0.0004742145538330078\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n",
      "Total number of documents: 64\n"
     ]
    }
   ],
   "source": [
    "! python dedup/generate_duplicate_pairs.py --input_dir data/RedPajama_minhash/ --out_file data/redpj_duplicates/duplicate_pairs.txt --range 13 --bands 9 --processes 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Duplicate Graph Construction & Search for Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started graph building\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "length of the set of duplicates: 0 0.003435373306274414\n",
      "0it [00:00, ?it/s]\n",
      "number of connected components: 1 0.0035572052001953125\n",
      "Graph generated duplicates list!!! 0.0036547183990478516\n"
     ]
    }
   ],
   "source": [
    "! python dedup/generate_connected_components.py --input_dir data/redpj_duplicates --out_file data/redpj_duplicates/connected_components.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4: Generate Final List of Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing duplicates!!!\n",
      "100%|██████████████████████████████████████████| 1/1 [00:00<00:00, 41120.63it/s]\n",
      "number of duplicate documents that will be removed: 0\n"
     ]
    }
   ],
   "source": [
    "! python dedup/generate_duplicates_dict.py --input_file data/redpj_duplicates/connected_components.pickle --out_file data/redpj_duplicates/duplicates.pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Interleave & Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling chunk of documents 0\n",
      "Sampling chunk of documents 0\n",
      "Finished sampling documents.\n",
      "Finished sampling documents.\n",
      "Total number of processed documents: 10  Total time: 0.03184103965759277\n",
      "Total number of processed documents: 20  Total time: 0.04440808296203613\n",
      "Total number of processed documents: 30  Total time: 0.0540311336517334\n",
      "Total number of processed documents: 40  Total time: 0.0596623420715332\n",
      "Total number of processed documents: 50  Total time: 0.06805253028869629\n",
      "Total number of processed documents: 60  Total time: 0.07453274726867676\n",
      "Finished writing documents.\n",
      "Pass 1 finished...\n"
     ]
    }
   ],
   "source": [
    "! python preprocessing/shuffle_holdout.py pass1 --input_dir data/RedPajama_norm/ --duplicates data/redpj_duplicates/duplicates.pickle --short_docs data/RedPajama_filtered.pickle --out_dir data/SlimPajama/pass1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Split Dataset into Train and Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for j in {1..20}\n",
    "do\n",
    "    python preprocessing/shuffle_holdout.py pass2 \"$((j-1))\" \"$j\" \"$j\" --input_dir data/SlimPajama/pass1 --train_dir data/SlimPajama/train --holdout_dir data/SlimPajama/holdout > $j.log 2>&1 &\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Deduplicate Train against Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting hashes for eval 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 175.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total written: 50\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python dedup/dedup_train.py 1 --src_dir data/SlimPajama/train --tgt_dir data/SlimPajama/holdout --out_dir data/SlimPajama/train_deduped\n",
    "for j in {2..20}\n",
    "do\n",
    "    python dedup/dedup_train.py \"$j\" --src_dir data/SlimPajama/train --tgt_dir data/SlimPajama/holdout --out_dir data/SlimPajama/train_deduped > $j.log 2>&1 &\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch3_dataprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
