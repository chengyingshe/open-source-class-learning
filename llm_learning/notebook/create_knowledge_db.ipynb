{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding API - ZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "响应示例：\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"model\": \"embedding-3\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"embedding\": [\n",
    "                -0.02675454691052437,\n",
    "                0.019060475751757622,\n",
    "                ...... \n",
    "                -0.005519774276763201,\n",
    "                0.014949671924114227\n",
    "            ],\n",
    "            \"index\": 0,\n",
    "            \"object\": \"embedding\"\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "            \"embedding\": [\n",
    "                -0.02675454691052437,\n",
    "                0.019060475751757622,\n",
    "                ...... \n",
    "                -0.005519774276763201,\n",
    "                0.014949671924114227\n",
    "            ],\n",
    "            \"index\": 2,\n",
    "            \"object\": \"embedding\"\n",
    "        }\n",
    "    ],\n",
    "    \"object\": \"list\",\n",
    "    \"usage\": {\n",
    "        \"completion_tokens\": 0,\n",
    "        \"prompt_tokens\": 100,\n",
    "        \"total_tokens\": 100\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhipuai import ZhipuAI\n",
    "import os\n",
    "API_KEY = 'fc4c27cb51ee4722a642b76909d01464.czerMWREZH1xkavS'\n",
    "\n",
    "def zhipu_embedding(text: str):\n",
    "    client = ZhipuAI(api_key=API_KEY)\n",
    "    response = client.embeddings.create(\n",
    "        model=\"embedding-2\",\n",
    "        input=text,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "text = '要生成 embedding 的输入文本，字符串形式。'\n",
    "response = zhipu_embedding(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.017893229,\n",
       " 0.064432174,\n",
       " -0.009351327,\n",
       " 0.027082685,\n",
       " 0.0040648775,\n",
       " -0.05599671,\n",
       " -0.042226028,\n",
       " -0.030019397,\n",
       " -0.01632937,\n",
       " 0.067769825]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.data[0].embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 源文档提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "\n",
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径\n",
    "loader = PyMuPDFLoader(\"knowledge_db/LLM-TAP-v2.pdf\")\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pdf_pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 PDF 一共包含 533 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(pdf_pages)}，\",  f\"该 PDF 一共包含 {len(pdf_pages)} 页\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-05T03:53:41+00:00', 'source': 'knowledge_db/LLM-TAP-v2.pdf', 'file_path': 'knowledge_db/LLM-TAP-v2.pdf', 'total_pages': 533, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-03-16T16:49:12+08:00', 'trapped': '', 'modDate': \"D:20250316164912+08'00'\", 'creationDate': 'D:20250305035341Z', 'page': 10}\n",
      "------\n",
      "查看该文档的内容:\n",
      "1. 绪论\n",
      "大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n",
      "监督学习方法通过大量无标注文本进行训练。2018 年以来，Google、OpenAI、Meta、百度、华为\n",
      "等公司和研究机构相继发布了BERT[1]、GPT[2] 等多种模型，这些模型在几乎所有自然语言处理任\n",
      "务中都表现出色。2019 年，大语言模型呈现爆发式的增长，特别是2022 年11 月ChatGPT（Chat\n",
      "Generative Pre-trained Transformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n",
      "系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n",
      "了强大的对世界知识的掌握和对语言的理解能力。\n",
      "本章主要介绍大语言模型的基本概念、发展历程和构建流程。\n",
      "1.1 大语言模型的基本概念\n",
      "使用语言是人类与其他动物最重要的区别之一，而人类的多种智能也与此密切相关，逻辑思维\n",
      "以语言的形式表达，大量的知识也以文字的形式记录和传播。如今，互联网上已经拥有数万亿个网页\n",
      "的资源，其中大部分信息都是用自然语言描述的。因此，如果人工智能算法想要获取知识，就必须懂\n",
      "得如何理解人类所使用的不太精确、可能有歧义甚至有些混乱的语言。语言模型（Language Model，\n",
      "LM）的目标就是对自然语言的概率分布建模。词汇表V 上的语言模型，由函数P(w1w2 · · · wm) 表\n",
      "示，可以形式化地构建为词序列w1w2 · · · wm 的概率分布，表示词序列w1w2 · · · wm 作为一个句子\n",
      "出现的可能性的大小。由于联合概率P(w1w2 · · · wm) 的参数量巨大，因此直接计算P(w1w2 · · · wm)\n",
      "非常困难[3]。《现代汉语词典》（第7 版）包含约7 万词，句子长度按照20 个词计算，语言模型的\n",
      "参数量达到7.9792 × 1096 的天文数字。在中文的书面语中，超过100 个词的句子并不罕见，如果\n",
      "要将所有可能性都纳入考虑，则语言模型的复杂度会进一步增加，以目前的计算手段无法进行存\n",
      "储和运算。\n",
      "为了减小P(w1w2 · · · wm) 模型的参数空间，可以利用句子序列（通常是从左至右）的生成过\n",
      "程将其进行分解，使用链式法则可以得到\n"
     ]
    }
   ],
   "source": [
    "pdf_page = pdf_pages[10]\n",
    "print(f\"每一个元素的类型：{type(pdf_page)}.\", \n",
    "    f\"该文档的描述性数据：{pdf_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{pdf_page.page_content}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "file_path = \"knowledge_db/transformers_README.md\"\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(file_path)\n",
    "md_pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入后的变量类型为：<class 'list'>， 该 Markdown 一共包含 1 页\n"
     ]
    }
   ],
   "source": [
    "print(f\"载入后的变量类型为：{type(md_pages)}，\",  f\"该 Markdown 一共包含 {len(md_pages)} 页\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'source': 'knowledge_db/transformers_README.md'}\n",
      "------\n",
      "查看该文档的内容:\n",
      "Generating the documentation\n",
      "\n",
      "To generate the documentation, you first have to build it. Several packages are necessary to build the doc, \n",
      "you can install them with the following command, at the root \n"
     ]
    }
   ],
   "source": [
    "md_page = md_pages[0]\n",
    "print(f\"每一个元素的类型：{type(md_page)}.\", \n",
    "    f\"该文档的描述性数据：{md_page.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{md_page.page_content[0:][:200]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.绪论\n",
      "大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\n",
      "监督学习方法通过大量无标注文本进行训练。2018年以来，Google、OpenAI、Meta、百度、华为\n",
      "等公司和研究机构相继发布了BERT[1]、GPT[2]等多种模型，这些模型在几乎所有自然语言处理任\n",
      "务中都表现出色。2019年，大语言模型呈现爆发式的增长，特别是2022年11月ChatGPT（ChatGenerativePre-trainedTransformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\n",
      "系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\n",
      "了强大的对世界知识的掌握和对语言的理解能力。\n",
      "本章主要介绍大语言模型的基本概念、发展历程和构建流程。1.1大语言模型的基本概念\n",
      "使用语言是人类与其他动物最重要的区别之一，而人类的多种智能也与此密切相关，逻辑思维\n",
      "以语言的形式表达，大量的知识也以文字的形式记录和传播。如今，互联网上已经拥有数万亿个网页\n",
      "的资源，其中大部分信息都是用自然语言描述的。因此，如果人工智能算法想要获取知识，就必须懂\n",
      "得如何理解人类所使用的不太精确、可能有歧义甚至有些混乱的语言。语言模型（LanguageModel，LM）的目标就是对自然语言的概率分布建模。词汇表V上的语言模型，由函数P(w1w2···wm)表\n",
      "示，可以形式化地构建为词序列w1w2···wm的概率分布，表示词序列w1w2···wm作为一个句子\n",
      "出现的可能性的大小。由于联合概率P(w1w2···wm)的参数量巨大，因此直接计算P(w1w2···wm)\n",
      "非常困难[3]。《现代汉语词典》（第7版）包含约7万词，句子长度按照20个词计算，语言模型的\n",
      "参数量达到7.9792×1096的天文数字。在中文的书面语中，超过100个词的句子并不罕见，如果\n",
      "要将所有可能性都纳入考虑，则语言模型的复杂度会进一步增加，以目前的计算手段无法进行存\n",
      "储和运算。\n",
      "为了减小P(w1w2···wm)模型的参数空间，可以利用句子序列（通常是从左至右）的生成过\n",
      "程将其进行分解，使用链式法则可以得到\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[^\\u4e00-\\u9fff](\\n)[^\\u4e00-\\u9fff]', re.DOTALL)\n",
    "pdf_page.page_content = re.sub(pattern, lambda match: match.group(0).replace('\\n', ''), pdf_page.page_content)\n",
    "pdf_page.page_content = pdf_page.page_content.replace('•', '')\n",
    "pdf_page.page_content = pdf_page.page_content.replace(' ', '')\n",
    "\n",
    "print(pdf_page.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the documentation\n",
      "To generate the documentation, you first have to build it. Several packages are necessary to build the doc, \n",
      "you can install them with the following command, at the root of the code repository:\n",
      "bash\n",
      "pip install -e \".[docs]\"\n",
      "Then you need to install our special tool that builds the documentation:\n",
      "bash\n",
      "pip install git+https://github.com/huggingface/doc-builder\n",
      "NOTE\n",
      "You only need to generate the documentation to inspect it locally (if you're planning changes and want to\n",
      "check how they look before committing for instance). You don't have to commit the built documentation.\n",
      "Building the documentation\n",
      "Once you have setup the doc-builder and additional packages, you can generate the documentation by \n",
      "typing the following command:\n",
      "bash\n",
      "doc-builder build transformers docs/source/en/ --build_dir ~/tmp/test-build\n",
      "You can adapt the --build_dir to set any temporary folder that you prefer. This command will create it and generate\n",
      "the MDX files that will be rendered as the documentation on the main website. You can inspect them in your favorite\n",
      "Markdown editor.\n",
      "Previewing the documentation\n",
      "To preview the docs, first install the watchdog module with:\n",
      "bash\n",
      "pip install watchdog\n",
      "Then run the following command:\n",
      "bash\n",
      "doc-builder preview {package_name} {path_to_docs}\n",
      "For example:\n",
      "bash\n",
      "doc-builder preview transformers docs/source/en/\n",
      "The docs will be viewable at http://localhost:3000. You can also preview the docs once you have opened a PR. You will see a bot add a comment to a link where the documentation with your changes lives.\n",
      "NOTE\n",
      "The preview command only works with existing doc files. When you add a completely new file, you need to update _toctree.yml & restart preview command (ctrl-c to stop it & call doc-builder preview ... again).\n",
      "Adding a new element to the navigation bar\n",
      "Accepted files are Markdown (.md).\n",
      "Create a file with its extension and put it in the source directory. You can then link it to the toc-tree by putting\n",
      "the filename without the extension in the _toctree.yml file.\n",
      "Renaming section headers and moving sections\n",
      "It helps to keep the old links working when renaming the section header and/or moving sections from one document to another. This is because the old links are likely to be used in Issues, Forums, and Social media and it'd make for a much more superior user experience if users reading those months later could still easily navigate to the originally intended information.\n",
      "Therefore, we simply keep a little map of moved sections at the end of the document where the original section was. The key is to preserve the original anchor.\n",
      "So if you renamed a section from: \"Section A\" to \"Section B\", then you can add at the end of the file:\n",
      "```\n",
      "Sections that were moved:\n",
      "[ Section A ]\n",
      "```\n",
      "and of course, if you moved it to another file, then:\n",
      "```\n",
      "Sections that were moved:\n",
      "[ Section A ]\n",
      "```\n",
      "Use the relative style to link to the new file so that the versioned docs continue to work.\n",
      "For an example of a rich moved section set please see the very end of the Trainer doc.\n",
      "Writing Documentation - Specification\n",
      "The huggingface/transformers documentation follows the\n",
      "Google documentation style for docstrings,\n",
      "although we can write them directly in Markdown.\n",
      "Adding a new tutorial\n",
      "Adding a new tutorial or section is done in two steps:\n",
      "Add a new file under ./source. This file can either be ReStructuredText (.rst) or Markdown (.md).\n",
      "Link that file in ./source/_toctree.yml on the correct toc-tree.\n",
      "Make sure to put your new file under the proper section. It's unlikely to go in the first section (Get Started), so\n",
      "depending on the intended targets (beginners, more advanced users, or researchers) it should go in sections two, three, or\n",
      "four.\n",
      "Translating\n",
      "When translating, refer to the guide at ./TRANSLATING.md.\n",
      "Adding a new model\n",
      "When adding a new model:\n",
      "Create a file xxx.md or under ./source/model_doc (don't hesitate to copy an existing file as template).\n",
      "Link that file in ./source/_toctree.yml.\n",
      "Write a short overview of the model:\n",
      "Overview with paper & authors\n",
      "Paper abstract\n",
      "Tips and tricks and how to use it best\n",
      "Add the classes that should be linked in the model. This generally includes the configuration, the tokenizer, and\n",
      "  every model of that class (the base model, alongside models with additional heads), both in PyTorch and TensorFlow.\n",
      "  The order is generally:\n",
      "Configuration\n",
      "Tokenizer\n",
      "PyTorch base model\n",
      "PyTorch head models\n",
      "TensorFlow base model\n",
      "TensorFlow head models\n",
      "Flax base model\n",
      "Flax head models\n",
      "These classes should be added using our Markdown syntax. Usually as follows:\n",
      "```\n",
      "XXXConfig\n",
      "[[autodoc]] XXXConfig\n",
      "```\n",
      "This will include every public method of the configuration that is documented. If for some reason you wish for a method\n",
      "not to be displayed in the documentation, you can do so by specifying which methods should be in the docs:\n",
      "```\n",
      "XXXTokenizer\n",
      "[[autodoc]] XXXTokenizer\n",
      "    - build_inputs_with_special_tokens\n",
      "    - get_special_tokens_mask\n",
      "    - create_token_type_ids_from_sequences\n",
      "    - save_vocabulary\n",
      "```\n",
      "If you just want to add a method that is not documented (for instance magic methods like __call__ are not documented\n",
      "by default) you can put the list of methods to add in a list that contains all:\n",
      "```\n",
      "XXXTokenizer\n",
      "[[autodoc]] XXXTokenizer\n",
      "    - all\n",
      "    - call\n",
      "```\n",
      "Writing source documentation\n",
      "Values that should be put in code should either be surrounded by backticks: `like so`. Note that argument names\n",
      "and objects like True, None, or any strings should usually be put in code.\n",
      "When mentioning a class, function, or method, it is recommended to use our syntax for internal links so that our tool\n",
      "adds a link to its documentation with this syntax: [`XXXClass`] or [`function`]. This requires the class or \n",
      "function to be in the main package.\n",
      "If you want to create a link to some internal class or function, you need to\n",
      "provide its path. For instance: [`utils.ModelOutput`]. This will be converted into a link with\n",
      "utils.ModelOutput in the description. To get rid of the path and only keep the name of the object you are\n",
      "linking to in the description, add a ~: [`~utils.ModelOutput`] will generate a link with ModelOutput in the description.\n",
      "The same works for methods so you can either use [`XXXClass.method`] or [`~XXXClass.method`].\n",
      "Defining arguments in a method\n",
      "Arguments should be defined with the Args: (or Arguments: or Parameters:) prefix, followed by a line return and\n",
      "an indentation. The argument should be followed by its type, with its shape if it is a tensor, a colon, and its\n",
      "description:\n",
      "Args:\n",
      "        n_layers (`int`): The number of layers of the model.\n",
      "If the description is too long to fit in one line, another indentation is necessary before writing the description\n",
      "after the argument.\n",
      "Here's an example showcasing everything so far:\n",
      "``\n",
      "    Args:\n",
      "        input_ids (torch.LongTensorof shape(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary.\n",
      "```\n",
      "For optional arguments or arguments with defaults we follow the following syntax: imagine we have a function with the\n",
      "following signature:\n",
      "def my_function(x: str = None, a: float = 1):\n",
      "then its documentation should look like this:\n",
      "Args:\n",
      "        x (`str`, *optional*):\n",
      "            This argument controls ...\n",
      "        a (`float`, *optional*, defaults to 1):\n",
      "            This argument is used to ...\n",
      "Note that we always omit the \"defaults to `None`\" when None is the default for any argument. Also note that even\n",
      "if the first line describing your argument type and its default gets long, you can't break it on several lines. You can\n",
      "however, write as many lines as you want in the indented description (see the example above with input_ids).\n",
      "Writing a multi-line code block\n",
      "Multi-line code blocks can be useful for displaying examples. They are done between two lines of three backticks as usual in Markdown:\n",
      "first line of code\n",
      "second line\n",
      "etc\n",
      "```\n",
      "````\n",
      "We follow the doctest syntax for the examples to automatically test\n",
      "the results to stay consistent with the library.\n",
      "Writing a return block\n",
      "The return block should be introduced with the Returns: prefix, followed by a line return and an indentation.\n",
      "The first line should be the type of the return, followed by a line return. No need to indent further for the elements\n",
      "building the return.\n",
      "Here's an example of a single value return:\n",
      "python\n",
      "    Returns:\n",
      "        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.\n",
      "Here's an example of a tuple return, comprising several objects:\n",
      "python\n",
      "    Returns:\n",
      "        `tuple(torch.FloatTensor)` comprising various elements depending on the configuration ([`BertConfig`]) and inputs:\n",
      "        - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.FloatTensor` of shape `(1,)` --\n",
      "          Total loss is the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n",
      "        - **prediction_scores** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --\n",
      "          Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      "Adding an image\n",
      "Due to the rapidly growing repository, it is important to make sure that no files that would significantly weigh down the repository are added. This includes images, videos, and other non-text files. We prefer to leverage a hf.co hosted dataset like\n",
      "the ones hosted on hf-internal-testing in which to place these files and reference\n",
      "them by URL. We recommend putting them in the following dataset: huggingface/documentation-images.\n",
      "If an external contribution, feel free to add the images to your PR and ask a Hugging Face member to migrate your images\n",
      "to this dataset.\n",
      "Styling the docstring\n",
      "We have an automatic script running with the make style comment that will make sure that:\n",
      "- the docstrings fully take advantage of the line width\n",
      "- all code examples are formatted using black, like the code of the Transformers library\n",
      "This script may have some weird failures if you made a syntax mistake or if you uncover a bug. Therefore, it's\n",
      "recommended to commit your changes before running make style, so you can revert the changes done by that script\n",
      "easily.\n",
      "Testing documentation examples\n",
      "Good documentation often comes with an example of how a specific function or class should be used. \n",
      "Each model class should contain at least one example showcasing\n",
      "how to use this model class in inference. E.g. the class Wav2Vec2ForCTC \n",
      "includes an example of how to transcribe speech to text in the \n",
      "docstring of its forward function.\n",
      "Writing documentation examples\n",
      "The syntax for Example docstrings can look as follows:\n",
      "```python\n",
      "    Example:\n",
      "```\n",
      "The docstring should give a minimal, clear example of how the respective model \n",
      "is to be used in inference and also include the expected (ideally sensible)\n",
      "output.\n",
      "Often, readers will try out the example before even going through the function \n",
      "or class definitions. Therefore, it is of utmost importance that the example \n",
      "works as expected.\n",
      "Docstring testing\n",
      "To do so each example should be included in the doctests. \n",
      "We use pytests' doctest integration to verify that all of our examples run correctly. \n",
      "For Transformers, the doctests are run on a daily basis via GitHub Actions as can be \n",
      "seen here.\n",
      "For Python files\n",
      "Run all the tests in the docstrings of a given file with the following command, here is how we test the modeling file of Wav2Vec2 for instance:\n",
      "bash\n",
      "pytest --doctest-modules src/transformers/models/wav2vec2/modeling_wav2vec2.py -sv --doctest-continue-on-failure\n",
      "If you want to isolate a specific docstring, just add :: after the file name then type the whole path of the function/class/method whose docstring you want to test. For instance, here is how to just test the forward method of Wav2Vec2ForCTC:\n",
      "bash\n",
      "pytest --doctest-modules src/transformers/models/wav2vec2/modeling_wav2vec2.py::transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.forward -sv --doctest-continue-on-failure\n",
      "For Markdown files\n",
      "You can test locally a given file with this command (here testing the quicktour):\n",
      "bash\n",
      "pytest --doctest-modules docs/source/quicktour.md -sv --doctest-continue-on-failure --doctest-glob=\"*.md\"\n",
      "Writing doctests\n",
      "Here are a few tips to help you debug the doctests and make them pass:\n",
      "The outputs of the code need to match the expected output exactly, so make sure you have the same outputs. In particular doctest will see a difference between single quotes and double quotes, or a missing parenthesis. The only exceptions to that rule are:\n",
      "whitespace: one give whitespace (space, tabulation, new line) is equivalent to any number of whitespace, so you can add new lines where there are spaces to make your output more readable.\n",
      "numerical values: you should never put more than 4 or 5 digits to expected results as different setups or library versions might get you slightly different results. doctest is configured to ignore any difference lower than the precision to which you wrote (so 1e-4 if you write 4 digits).\n",
      "Don't leave a block of code that is very long to execute. If you can't make it fast, you can either not use the doctest syntax on it (so that it's ignored), or if you want to use the doctest syntax to show the results, you can add a comment # doctest: +SKIP at the end of the lines of code too long to execute\n",
      "Each line of code that produces a result needs to have that result written below. You can ignore an output if you don't want to show it in your code example by adding a comment # doctest: +IGNORE_RESULT at the end of the line of code producing it.\n"
     ]
    }
   ],
   "source": [
    "md_page.page_content = md_page.page_content.replace('\\n\\n', '\\n')\n",
    "print(md_page.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 文档分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.绪论\\n大语言模型是一种由包含数百亿个及以上参数的深度神经网络构建的语言模型，通常使用自\\n监督学习方法通过大量无标注文本进行训练。2018年以来，Google、OpenAI、Meta、百度、华为\\n等公司和研究机构相继发布了BERT[1]、GPT[2]等多种模型，这些模型在几乎所有自然语言处理任\\n务中都表现出色。2019年，大语言模型呈现爆发式的增长，特别是2022年11月ChatGPT（ChatGenerativePre-trainedTransformer）的发布，引起了全世界的广泛关注。用户可以使用自然语言与\\n系统交互，实现问答、分类、摘要、翻译、聊天等从理解到生成的各种任务。大语言模型展现出\\n了强大的对世界知识的掌握和对语言的理解能力。\\n本章主要介绍大语言模型的基本概念、发展历程和构建流程。1.1大语言模型的基本概念\\n使用语言是人类与其他动物最重要的区别之一，而人类的多种智能也与此密切相关，逻辑思维\\n以语言的形式表达，大量的知识也以文字的形式记录和传播。如今，互联网上已经拥有数万亿个网页\\n的资源，其中大部分信息都是用自然语言描述的。因此，如果人工智能算法想要获取知识，就必须懂',\n",
       " '的资源，其中大部分信息都是用自然语言描述的。因此，如果人工智能算法想要获取知识，就必须懂\\n得如何理解人类所使用的不太精确、可能有歧义甚至有些混乱的语言。语言模型（LanguageModel，LM）的目标就是对自然语言的概率分布建模。词汇表V上的语言模型，由函数P(w1w2···wm)表\\n示，可以形式化地构建为词序列w1w2···wm的概率分布，表示词序列w1w2···wm作为一个句子\\n出现的可能性的大小。由于联合概率P(w1w2···wm)的参数量巨大，因此直接计算P(w1w2···wm)\\n非常困难[3]。《现代汉语词典》（第7版）包含约7万词，句子长度按照20个词计算，语言模型的\\n参数量达到7.9792×1096的天文数字。在中文的书面语中，超过100个词的句子并不罕见，如果\\n要将所有可能性都纳入考虑，则语言模型的复杂度会进一步增加，以目前的计算手段无法进行存\\n储和运算。\\n为了减小P(w1w2···wm)模型的参数空间，可以利用句子序列（通常是从左至右）的生成过\\n程将其进行分解，使用链式法则可以得到']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "* RecursiveCharacterTextSplitter 递归字符文本分割\n",
    "RecursiveCharacterTextSplitter 将按不同的字符递归地分割(按照这个优先级[\"\\n\\n\", \"\\n\", \" \", \"\"])，\n",
    "    这样就能尽量把所有和语义相关的内容尽可能长时间地保留在同一位置\n",
    "RecursiveCharacterTextSplitter需要关注的是4个参数：\n",
    "\n",
    "* separators - 分隔符字符串数组\n",
    "* chunk_size - 每个文档的字符数量限制\n",
    "* chunk_overlap - 两份文档重叠区域的长度\n",
    "* length_function - 长度计算函数\n",
    "'''\n",
    "#导入文本分割器\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 知识库中单段文本长度\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "# 知识库中相邻文本重合长度\n",
    "OVERLAP_SIZE = 50\n",
    "# 使用递归字符文本分割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=OVERLAP_SIZE\n",
    ")\n",
    "text_splitter.split_text(pdf_page.page_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分后的文件数量：1605\n",
      "切分后的字符数（可以用来大致评估 token 数）：647216\n"
     ]
    }
   ],
   "source": [
    "split_docs = text_splitter.split_documents(pdf_pages)\n",
    "print(f\"切分后的文件数量：{len(split_docs)}\")\n",
    "print(f\"切分后的字符数（可以用来大致评估 token 数）：{sum([len(doc.page_content) for doc in split_docs])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up knowledge database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knowledge_db/transformers_README.md', 'knowledge_db/LLM-TAP-v2.pdf', 'knowledge_db/dir1/dir12/file2.md', 'knowledge_db/dir1/dir11/dir111/file1.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_paths = []\n",
    "folder_path = 'knowledge_db'\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_paths.append(file_path)\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "\n",
    "# 遍历文件路径并把实例化的loader存放在loaders里\n",
    "loaders = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "\n",
    "    file_type = file_path.split('.')[-1]\n",
    "    if file_type == 'pdf':\n",
    "        loaders.append(PyMuPDFLoader(file_path))\n",
    "    elif file_type == 'md':\n",
    "        loaders.append(UnstructuredMarkdownLoader(file_path))\n",
    "        \n",
    "# 下载文件并存储到text\n",
    "texts = []\n",
    "\n",
    "for loader in loaders: texts.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一个元素的类型：<class 'langchain_core.documents.base.Document'>.\n",
      "------\n",
      "该文档的描述性数据：{'producer': 'xdvipdfmx (20240305)', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-05T03:53:41+00:00', 'source': 'knowledge_db/LLM-TAP-v2.pdf', 'file_path': 'knowledge_db/LLM-TAP-v2.pdf', 'total_pages': 533, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-03-16T16:49:12+08:00', 'trapped': '', 'modDate': \"D:20250316164912+08'00'\", 'creationDate': 'D:20250305035341Z', 'page': 0}\n",
      "------\n",
      "查看该文档的内容:\n",
      "大语言模型\n",
      "从理论到实践\n",
      "（第二版）\n",
      "张奇桂韬郑锐⻩萱菁著\n",
      "预览版\n",
      "2025 年3 月5 日\n",
      "·\n"
     ]
    }
   ],
   "source": [
    "text = texts[1]\n",
    "print(f\"每一个元素的类型：{type(text)}.\", \n",
    "    f\"该文档的描述性数据：{text.metadata}\", \n",
    "    f\"查看该文档的内容:\\n{text.page_content[0:]}\", \n",
    "    sep=\"\\n------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
    "\n",
    "embedding = ZhipuAIEmbeddings(api_key=API_KEY)\n",
    "# 定义持久化路径\n",
    "persist_directory = 'database/vector_db/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1355385/2600787329.py:8: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs[:20], # 为了速度，只选择前 20 个切分的 doc 进行生成；使用千帆时因QPS限制，建议选择前 5 个doc\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：40\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相似度检索\n",
    "\n",
    "$$similarity=cos(A,B)=\\frac{A\\cdot B}{\\parallel A\\parallel\\parallel B\\parallel}=\\frac{\\sum_1^na_ib_i}{\\sqrt{\\sum_1^na_i^2}\\sqrt{\\sum_1^nb_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n",
      "检索到的第0个内容: \n",
      "大语言模型\n",
      "从理论到实践\n",
      "（第二版）\n",
      "张奇桂韬郑锐⻩萱菁著\n",
      "预览版\n",
      "2025 年3 月5 日\n",
      "·\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      "大语言模型\n",
      "从理论到实践\n",
      "（第二版）\n",
      "张奇桂韬郑锐⻩萱菁著\n",
      "预览版\n",
      "2025 年3 月5 日\n",
      "·\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      "1.2 大语言模型的发展历程. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\n",
      "1.3 大语言模型的构建流程. . . . . . . . . . . .\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "question=\"什么是大语言模型\"\n",
    "sim_docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(sim_docs)}\")\n",
    "\n",
    "for i, sim_doc in enumerate(sim_docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMR检索(Maximum marginal relevance)\n",
    "\n",
    "核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 检索到的第0个内容: \n",
      "大语言模型\n",
      "从理论到实践\n",
      "（第二版）\n",
      "张奇桂韬郑锐⻩萱菁著\n",
      "预览版\n",
      "2025 年3 月5 日\n",
      "·\n",
      "--------------\n",
      "MMR 检索到的第1个内容: \n",
      "2.2.3 预训练语言模型实践. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n",
      "2.3 大语言模型的结构. . . . . . . . . . . . . . . \n",
      "--------------\n",
      "MMR 检索到的第2个内容: \n",
      "2.1.1 嵌入表示层. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n",
      "2.1.2 注意力层. . . . . . . . . \n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "mmr_docs = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "for i, sim_doc in enumerate(mmr_docs):\n",
    "    print(f\"MMR 检索到的第{i}个内容: \\n{sim_doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
