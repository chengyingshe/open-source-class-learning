{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MntmiANZCk2L"
      },
      "source": [
        "# GenAI HW9: Quick Summary of Lecture Video (演講影片快速摘要)\n",
        "## Objectives\n",
        "- ### Learn to quickly build applications related to speech recognition using existing APIs. (學習以現成的API快速搭建語音辨識相關的應用。)\n",
        "\n",
        "\n",
        "#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voEioD2DCoeq"
      },
      "source": [
        "# Part1 - Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSccLtt234Pm"
      },
      "source": [
        "## The lecture video provided for this assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgHVz9WF4Vfp"
      },
      "source": [
        "(1) For ease of processing, it has already been converted to a MP3 file.\n",
        "\n",
        "(2) If you would like to view the original video, the link is here:\n",
        "\n",
        "- 李琳山教授 信號與人生 (2023)\n",
        "\n",
        "  - https://www.youtube.com/watch?v=MxoQV4M0jY8\n",
        "\n",
        "\n",
        "(3) Since the original lecture video is quite long, we have edited the segment from 1:43:24 to 2:00:49 to use for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdoLJZE33oCD"
      },
      "source": [
        "## Install all necessary packages and import them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HREsIZV33yDy"
      },
      "source": [
        "The following code block takes about **150** seconds to run, but it may vary slightly depending on the condition of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "119AJB_AAhJl",
        "outputId": "c2a197ec-d8b5-4ee0-d547-db0018394def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: srt==3.5.3 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (3.5.3)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: datasets==2.20.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (1.22.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (0.3.6)\n",
            "Requirement already satisfied: pandas in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (1.2.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (0.70.14)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2023.9.2)\n",
            "Requirement already satisfied: aiohttp in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (0.29.1)\n",
            "Requirement already satisfied: packaging in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from datasets==2.20.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (5.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from aiohttp->datasets==2.20.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.32.2->datasets==2.20.0) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from pandas->datasets==2.20.0) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets==2.20.0) (1.17.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: DateTime==5.5 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from DateTime==5.5) (7.2)\n",
            "Requirement already satisfied: pytz in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from DateTime==5.5) (2024.2)\n",
            "Requirement already satisfied: setuptools in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from zope.interface->DateTime==5.5) (75.8.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: opencv-contrib-python==4.8.0.76 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from opencv-contrib-python==4.8.0.76) (1.22.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: opencv-python==4.8.0.76 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from opencv-python==4.8.0.76) (1.22.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: opencv-python-headless==4.10.0.84 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from opencv-python-headless==4.10.0.84) (1.22.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openpyxl==3.1.4 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (3.1.4)\n",
            "Requirement already satisfied: et-xmlfile in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openpyxl==3.1.4) (2.0.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openai==1.35.3 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (1.35.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from openai==1.35.3) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai==1.35.3) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai==1.35.3) (2.10)\n",
            "Requirement already satisfied: certifi in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.35.3) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai==1.35.3) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.35.3) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai==1.35.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /home/scy/anaconda3/envs/llm/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai==1.35.3) (2.27.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Cloning https://github.com/openai/whisper.git (to revision ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab) to /tmp/pip-req-build-4r8los_j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-4r8los_j\n"
          ]
        }
      ],
      "source": [
        "# Install packages.\n",
        "!pip install srt==3.5.3\n",
        "!pip install datasets==2.20.0\n",
        "!pip install DateTime==5.5\n",
        "!pip install OpenCC==1.1.7\n",
        "!pip install opencv-contrib-python==4.8.0.76\n",
        "!pip install opencv-python==4.8.0.76\n",
        "!pip install opencv-python-headless==4.10.0.84\n",
        "!pip install openpyxl==3.1.4\n",
        "!pip install openai==1.35.3\n",
        "!pip install git+https://github.com/openai/whisper.git@ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
        "!pip install numpy==1.25.2\n",
        "!pip install soundfile==0.12.1\n",
        "!pip install -q -U google-generativeai==0.7.0\n",
        "!pip install anthropic==0.29.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWqXz6C6omR9"
      },
      "source": [
        "The following code block takes about **5** seconds to run, but it may vary slightly depending on the condition of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JFwSa_x6C53S"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages/numba/__init__.py:48: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.22.0)\n",
            "  import scipy\n"
          ]
        }
      ],
      "source": [
        "# Import packages.\n",
        "# import os\n",
        "# os.environ['HF_HUB_OFFLINE'] = '1'  # Set offline mode\n",
        "\n",
        "import whisper\n",
        "import srt\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import pathlib\n",
        "import textwrap\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from opencc import OpenCC\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFY6VDAyeooa"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBROu_HfgF1J"
      },
      "source": [
        "The code block below takes about **10** seconds to run, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PAieqtY8evUJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the latest cached version of the dataset since kuanhuggingface/NTU-GenAI-2024-HW9 couldn't be found on the Hugging Face Hub\n",
            "Found the latest cached dataset configuration 'default' at /home/scy/.cache/huggingface/datasets/kuanhuggingface___ntu-gen_ai-2024-hw9/default/0.0.0/d2f9d8f70117fbdace8e104f2692364470ef5392 (last modified on Thu Feb 27 14:13:06 2025).\n"
          ]
        }
      ],
      "source": [
        "# Load dataset.\n",
        "dataset_name = \"kuanhuggingface/NTU-GenAI-2024-HW9\"\n",
        "dataset = load_dataset(dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pN3dOGyrI-"
      },
      "source": [
        "The code block below takes about **15** seconds to run, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E68E8Ej2isAX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now, we are going to transcribe the audio: 李琳山教授 信號與人生 (2023) (ntu-gen-ai-2024-hw9-16k.mp3).\n"
          ]
        }
      ],
      "source": [
        "# Prepare audio.\n",
        "input_audio = dataset[\"test\"][\"audio\"][0]\n",
        "input_audio_name = input_audio[\"path\"]\n",
        "input_audio_array = input_audio[\"array\"].astype(np.float32)\n",
        "sampling_rate = input_audio[\"sampling_rate\"]\n",
        "\n",
        "print(f\"Now, we are going to transcribe the audio: 李琳山教授 信號與人生 (2023) ({input_audio_name}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxTn1CfzDCXy"
      },
      "source": [
        "# Part2 - Automatic Speech Recognition (ASR)\n",
        "The function \"speech_recognition\" aims to convert audio to subtitle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OmWjjLUGC9z3"
      },
      "outputs": [],
      "source": [
        "def speech_recognition(model_name, input_audio, output_subtitle_path, decode_options, cache_dir=\"./\"):\n",
        "    '''\n",
        "        (1) Objective:\n",
        "            - This function aims to convert audio to subtitle.\n",
        "\n",
        "        (2) Arguments:\n",
        "\n",
        "            - model_name (str):\n",
        "                The name of the model. There are five model sizes, including tiny, base, small, medium, large-v3.\n",
        "                For example, you can use 'tiny', 'base', 'small', 'medium', 'large-v3' to specify the model name.\n",
        "                You can see 'https://github.com/openai/whisper' for more details.\n",
        "\n",
        "            - input_audio (Union[str, np.ndarray, torch.Tensor]):\n",
        "                The path to the audio file to open, or the audio waveform\n",
        "                - For example, if your input audio path is 'input.wav', you can use 'input.wav' to specify the input audio path.\n",
        "                - For example, if your input audio array is 'audio_array', you can use 'audio_array' to specify the input audio array.\n",
        "\n",
        "            - output_subtitle_path (str):\n",
        "                The path of the output subtitle file.\n",
        "                For example, if you want to save the subtitle file as 'output.srt', you can use 'output.srt' to specify the output subtitle path.\n",
        "\n",
        "            - decode_options (dict):\n",
        "                The options for decoding the audio file, including 'initial_prompt', 'prompt', 'prefix', 'temperature'.\n",
        "                - initial_prompt (str):\n",
        "                    Optional text to provide as a prompt for the first window. This can be used to provide, or\n",
        "                    \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n",
        "                    to make it more likely to predict those word correctly.\n",
        "                    Default: None.\n",
        "\n",
        "                You can see \"https://github.com/openai/whisper/blob/main/whisper/decoding.py\" and \"https://github.com/openai/whisper/blob/main/whisper/transcribe.py\"\n",
        "                for more details.\n",
        "\n",
        "                - temperature (float):\n",
        "                    The temperature for sampling from the model. Higher values mean more randomness.\n",
        "                    Default: 0.0\n",
        "\n",
        "            - cache_dir (str):\n",
        "                The path of the cache directory for saving the model.\n",
        "                For example, if you want to save the cache files in 'cache' directory, you can use 'cache' to specify the cache directory.\n",
        "                Default: './'\n",
        "\n",
        "        (3) Example:\n",
        "\n",
        "            - If you want to use the 'base' model to convert 'input.wav' to 'output.srt' and save the cache files in 'cache' directory,\n",
        "            you can call this function as follows:\n",
        "\n",
        "                speech_recognition(model_name='base', input_audio_path='input.wav', output_subtitle_path='output.srt', cache_dir='cache')\n",
        "    '''\n",
        "\n",
        "    # Record the start time.\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"=============== Loading Whisper-{model_name} ===============\")\n",
        "\n",
        "    # Load the model.\n",
        "    model = whisper.load_model(name=model_name, download_root=cache_dir)\n",
        "\n",
        "    print(f\"Begin to utilize Whisper-{model_name} to transcribe the audio.\")\n",
        "\n",
        "    # Transcribe the audio.\n",
        "    transcription = model.transcribe(audio=input_audio, language=decode_options[\"language\"], verbose=False,\n",
        "                                     initial_prompt=decode_options[\"initial_prompt\"], temperature=decode_options[\"temperature\"])\n",
        "\n",
        "    # Record the end time.\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(f\"The process of speech recognition costs {end_time - start_time} seconds.\")\n",
        "\n",
        "    subtitles = []\n",
        "    # Convert the transcription to subtitle and iterate over the segments.\n",
        "    for i, segment in tqdm(enumerate(transcription[\"segments\"])):\n",
        "\n",
        "        # Convert the start time to subtitle format.\n",
        "        start_time = datetime.timedelta(seconds=segment[\"start\"])\n",
        "\n",
        "        # Convert the end time to subtitle format.\n",
        "        end_time = datetime.timedelta(seconds=segment[\"end\"])\n",
        "\n",
        "        # Get the subtitle text.\n",
        "        text = segment[\"text\"]\n",
        "\n",
        "        # Append the subtitle to the subtitle list.\n",
        "        subtitles.append(srt.Subtitle(index=i, start=start_time, end=end_time, content=text))\n",
        "\n",
        "    # Convert the subtitle list to subtitle content.\n",
        "    srt_content = srt.compose(subtitles)\n",
        "\n",
        "    print(f\"\\n=============== Saving the subtitle to {output_subtitle_path} ===============\")\n",
        "\n",
        "    # Save the subtitle content to the subtitle file.\n",
        "    with open(output_subtitle_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(srt_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ZkyefXpvmh"
      },
      "source": [
        "In the following block, you can modify your desired parameters and the path of input file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "UULEr1GpDAl6"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Whisper { run: \"auto\" }\n",
        "\n",
        "''' In this block, you can modify your desired parameters and the path of input file. '''\n",
        "\n",
        "# The name of the model you want to use.\n",
        "# For example, you can use 'tiny', 'base', 'small', 'medium', 'large-v3' to specify the model name.\n",
        "# @markdown **model_name**: The name of the model you want to use.\n",
        "model_name = \"medium\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large-v3\"]\n",
        "\n",
        "# Define the suffix of the output file.\n",
        "# @markdown **suffix**: The output file name is \"output-{suffix}.* \", where .* is the file extention (.txt or .srt)\n",
        "suffix = \"信號與人生\" # @param {type: \"string\"}\n",
        "\n",
        "# Path to the output file.\n",
        "output_subtitle_path = f\"./output-{suffix}.srt\"\n",
        "\n",
        "# Path of the output raw text file from the SRT file.\n",
        "output_raw_text_path = f\"./output-{suffix}.txt\"\n",
        "\n",
        "# Path to the directory where the model and dataset will be cached.\n",
        "cache_dir = \"./\"\n",
        "\n",
        "# The language of the lecture video.\n",
        "# @markdown **language**: The language of the lecture video.\n",
        "language = \"zh\" # @param {type:\"string\"}\n",
        "\n",
        "# Optional text to provide as a prompt for the first window.\n",
        "# @markdown **initial_prompt**: Optional text to provide as a prompt for the first window.\n",
        "initial_prompt = \"請用繁體中文\" #@param {type:\"string\"}\n",
        "\n",
        "# The temperature for sampling from the model. Higher values mean more randomness.\n",
        "# @markdown  **temperature**: The temperature for sampling from the model. Higher values mean more randomness.\n",
        "temperature = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TBhoPRKR9S4w"
      },
      "outputs": [],
      "source": [
        "# Construct DecodingOptions\n",
        "decode_options = {\n",
        "    \"language\": language,\n",
        "    \"initial_prompt\": initial_prompt,\n",
        "    \"temperature\": temperature\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4SfQ5Xn-fjya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting: (1) Model: whisper-medium (2) Language: zh (2) Initial Prompt: 請用繁體中文 (3) Temperature: 0\n",
            "Transcribe 李琳山教授 信號與人生 (2023)\n"
          ]
        }
      ],
      "source": [
        "# print message.\n",
        "message = \"Transcribe 李琳山教授 信號與人生 (2023)\"\n",
        "print(f\"Setting: (1) Model: whisper-{model_name} (2) Language: {language} (2) Initial Prompt: {initial_prompt} (3) Temperature: {temperature}\")\n",
        "print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxgZ2DNgpGlO"
      },
      "source": [
        "The code block below takes about **90 (240)** seconds to run when using the **base (medium)** model and **a T4 GPU**, although there might be some slight variation depending on the state of Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SOULGnw5RF6U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============== Loading Whisper-medium ===============\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/scy/anaconda3/envs/llm/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Begin to utilize Whisper-medium to transcribe the audio.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 104500/104500 [01:31<00:00, 1146.52frames/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The process of speech recognition costs 100.70884275436401 seconds.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "370it [00:00, 485573.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=============== Saving the subtitle to ./output-信號與人生.srt ===============\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Running ASR.\n",
        "speech_recognition(model_name=model_name, input_audio=input_audio_array, output_subtitle_path=output_subtitle_path, decode_options=decode_options, cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgmFtnti1qhU"
      },
      "source": [
        "You can check the result of automatic speech recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XeU54f5X1erZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "00:00:00,000 --> 00:00:04,000\n",
            "每次說這個學問是做出來的\n",
            "\n",
            "2\n",
            "00:00:06,000 --> 00:00:08,000\n",
            "什麼意思?\n",
            "\n",
            "3\n",
            "00:00:08,000 --> 00:00:12,000\n",
            "要做才會獲得學問\n",
            "\n",
            "4\n",
            "00:00:13,000 --> 00:00:16,000\n",
            "你如果每天光是坐在那裡聽\n",
            "\n",
            "5\n",
            "00:00:17,000 --> 00:00:20,000\n",
            "學問很可能是左耳進右耳出的\n",
            "\n",
            "6\n",
            "00:00:21,000 --> 00:00:23,000\n",
            "你光是坐在那兒讀\n",
            "\n",
            "7\n",
            "00:00:23,000 --> 00:00:26,000\n",
            "學問可能從眼睛進入腦海之後就忘掉了\n",
            "\n",
            "8\n",
            "00:00:26,000 --> 00:00:29,000\n",
            "如何能夠學問在腦海裡面\n",
            "\n",
            "9\n",
            "00:00:31,000 --> 00:00:33,000\n",
            "真的變成你自己學問\n",
            "\n",
            "10\n",
            "00:00:33,000 --> 00:00:35,000\n",
            "就是要做\n",
            "\n",
            "11\n",
            "00:00:36,000 --> 00:00:39,000\n",
            "可能有很多同學有這個經驗\n",
            "\n",
            "12\n",
            "00:00:39,000 --> 00:00:41,000\n",
            "你如果去修某一門課\n",
            "\n",
            "13\n",
            "00:00:41,000 --> 00:00:44,000\n",
            "或者做某一個實驗\n",
            "\n",
            "14\n",
            "00:00:44,000 --> 00:00:47,000\n",
            "在期末就是要教一個final project\n",
            "\n",
            "15\n",
            "00:00:48,000 --> 00:00:50,000\n",
            "那個final project就是要你把\n",
            "\n",
            "16\n",
            "00:00:51,000 --> 00:00:53,000\n",
            "學到的很多東西\n",
            "\n",
            "17\n",
            "00:00:53,000 --> 00:00:56,000\n",
            "最後整合在你的final project裡面\n",
            "\n",
            "18\n",
            "00:00:56,000 --> 00:00:58,000\n",
            "最後做出來的時候\n",
            "\n",
            "19\n",
            "00:00:58,000 --> 00:01:00,000\n",
            "就是把它們都整合了\n",
            "\n",
            "20\n",
            "00:01:00,000 --> 00:01:02,000\n",
            "當你學期結束\n",
            "\n",
            "21\n",
            "00:01:02,000 --> 00:01:04,000\n",
            "真的把final project做完的時候\n",
            "\n",
            "22\n",
            "00:01:04,000 --> 00:01:05,000\n",
            "你會忽然發現\n",
            "\n",
            "23\n",
            "00:01:05,000 --> 00:01:07,000\n",
            "我真的學到很多東西\n",
            "\n",
            "24\n",
            "00:01:07,000 --> 00:01:10,000\n",
            "那就是做出來的學問\n",
            "\n",
            "25\n",
            "00:01:10,000 --> 00:01:13,000\n",
            "也許可以舉另外一個例子\n",
            "\n",
            "26\n",
            "00:01:13,000 --> 00:01:16,000\n",
            "就是你如果學了某一些很複雜的演算法\n",
            "\n",
            "27\n",
            "00:01:16,000 --> 00:01:17,000\n",
            "或者什麼\n",
            "\n",
            "28\n",
            "00:01:17,000 --> 00:01:21,000\n",
            "好像覺得那些不見得在你的腦海裡\n",
            "\n",
            "29\n",
            "00:01:21,000 --> 00:01:24,000\n",
            "可是後來老師出了個習題\n",
            "\n",
            "30\n",
            "00:01:24,000 --> 00:01:26,000\n",
            "那個習題教你寫一個很大的程式\n",
            "\n",
            "31\n",
            "00:01:26,000 --> 00:01:28,000\n",
            "要把所有東西都包進去\n",
            "\n",
            "32\n",
            "00:01:28,000 --> 00:01:31,000\n",
            "當你把這個程式寫完的時候你會發現\n",
            "\n",
            "33\n",
            "00:01:31,000 --> 00:01:35,000\n",
            "你忽然把演算法裡所有東西都弄通了\n",
            "\n",
            "34\n",
            "00:01:35,000 --> 00:01:38,000\n",
            "那就是學問是做出來的\n",
            "\n",
            "35\n",
            "00:01:38,000 --> 00:01:40,000\n",
            "所以我們永遠要記得\n",
            "\n",
            "36\n",
            "00:01:40,000 --> 00:01:44,000\n",
            "盡量多動手多做\n",
            "\n",
            "37\n",
            "00:01:44,000 --> 00:01:46,000\n",
            "在動手跟做的過程之中\n",
            "\n",
            "38\n",
            "00:01:46,000 --> 00:01:49,000\n",
            "學問才可以變成是自己的\n",
            "\n",
            "39\n",
            "00:01:49,000 --> 00:01:51,000\n",
            "同樣的情形就是說\n",
            "\n",
            "40\n",
            "00:01:51,000 --> 00:01:57,000\n",
            "很多時候這樣動手或者做的表現或者成績\n",
            "\n",
            "41\n",
            "00:01:57,000 --> 00:02:00,000\n",
            "沒有一個成績單上的數字\n",
            "\n",
            "42\n",
            "00:02:00,000 --> 00:02:03,000\n",
            "使得很多人覺得那不重要\n",
            "\n",
            "43\n",
            "00:02:03,000 --> 00:02:07,000\n",
            "很多人甚至覺得這門課要做final project\n",
            "\n",
            "44\n",
            "00:02:07,000 --> 00:02:09,000\n",
            "我就不修了太累了\n",
            "\n",
            "45\n",
            "00:02:09,000 --> 00:02:12,000\n",
            "或者說那門課需要怎麼樣怎麼樣太累\n",
            "\n",
            "46\n",
            "00:02:12,000 --> 00:02:13,000\n",
            "我就不要做了\n",
            "\n",
            "47\n",
            "00:02:13,000 --> 00:02:17,000\n",
            "而不知道其實那個才是讓你做的機會\n",
            "\n",
            "48\n",
            "00:02:17,000 --> 00:02:19,000\n",
            "然後可以學到最多\n",
            "\n",
            "49\n",
            "00:02:19,000 --> 00:02:24,000\n",
            "也就是說雖然很可能那麼辛苦的做很多事\n",
            "\n",
            "50\n",
            "00:02:24,000 --> 00:02:27,000\n",
            "沒有讓你獲得什麼具體成績\n",
            "\n",
            "51\n",
            "00:02:27,000 --> 00:02:30,000\n",
            "對你的overfitting可能沒有幫助\n",
            "\n",
            "52\n",
            "00:02:30,000 --> 00:02:33,000\n",
            "可是對你的全面學習是很有幫助\n",
            "\n",
            "53\n",
            "00:02:33,000 --> 00:02:35,000\n",
            "是該學的\n",
            "\n",
            "54\n",
            "00:02:35,000 --> 00:02:38,000\n",
            "那不要漏掉這些事\n",
            "\n",
            "55\n",
            "00:02:38,000 --> 00:02:41,000\n",
            "那這是我所說的\n",
            "\n",
            "56\n",
            "00:02:41,000 --> 00:02:46,000\n",
            "那這個課業內可以做的這些事\n",
            "\n",
            "57\n",
            "00:02:46,000 --> 00:02:50,000\n",
            "那剛才我們講到思考的時候\n",
            "\n",
            "58\n",
            "00:02:50,000 --> 00:02:52,000\n",
            "我覺得我漏掉一點\n",
            "\n",
            "59\n",
            "00:02:52,000 --> 00:02:56,000\n",
            "你如果修我的信號課你可能會發現\n",
            "\n",
            "60\n",
            "00:02:56,000 --> 00:03:00,000\n",
            "我上課沒講到一個數學式子的時候\n",
            "\n",
            "61\n",
            "00:03:00,000 --> 00:03:02,000\n",
            "我通常都不推他的\n",
            "\n",
            "62\n",
            "00:03:02,000 --> 00:03:06,000\n",
            "我是在解釋那個數學式子在說什麼話\n",
            "\n",
            "63\n",
            "00:03:06,000 --> 00:03:08,000\n",
            "同樣的呢\n",
            "\n",
            "64\n",
            "00:03:08,000 --> 00:03:10,000\n",
            "沒講到一個什麼什麼事情的時候\n",
            "\n",
            "65\n",
            "00:03:10,000 --> 00:03:14,000\n",
            "我通常就在解釋他在說什麼話\n",
            "\n",
            "66\n",
            "00:03:14,000 --> 00:03:16,000\n",
            "也就是說\n",
            "\n",
            "67\n",
            "00:03:16,000 --> 00:03:20,000\n",
            "我在講的就是我讀到特本那裡的時候\n",
            "\n",
            "68\n",
            "00:03:20,000 --> 00:03:22,000\n",
            "我心裡怎麼想的\n",
            "\n",
            "69\n",
            "00:03:22,000 --> 00:03:28,000\n",
            "也就是我在告訴同學如何這個讀書的時候\n",
            "\n",
            "70\n",
            "00:03:28,000 --> 00:03:32,000\n",
            "如何一面讀一面練習思考\n",
            "\n",
            "71\n",
            "00:03:32,000 --> 00:03:37,000\n",
            "那這個才是最重要的一件事\n",
            "\n",
            "72\n",
            "00:03:37,000 --> 00:03:40,000\n",
            "如何培養自己思考的能力\n",
            "\n",
            "73\n",
            "00:03:40,000 --> 00:03:42,000\n",
            "跟培養思考的習慣\n",
            "\n",
            "74\n",
            "00:03:42,000 --> 00:03:46,000\n",
            "我覺得最好的辦法就是讀書的時候\n",
            "\n",
            "75\n",
            "00:03:46,000 --> 00:03:50,000\n",
            "凡是讀到一個數學式子都去想一想\n",
            "\n",
            "76\n",
            "00:03:50,000 --> 00:03:53,000\n",
            "那個數學式子到底在說什麼\n",
            "\n",
            "77\n",
            "00:03:53,000 --> 00:03:57,000\n",
            "凡是讀到特本上講什麼就去想一想\n",
            "\n",
            "78\n",
            "00:03:57,000 --> 00:03:59,000\n",
            "那個到底在說什麼\n",
            "\n",
            "79\n",
            "00:03:59,000 --> 00:04:03,000\n",
            "你要真的了解他在說什麼的時候\n",
            "\n",
            "80\n",
            "00:04:03,000 --> 00:04:06,000\n",
            "你就用了很多思考的功夫\n",
            "\n",
            "81\n",
            "00:04:06,000 --> 00:04:09,000\n",
            "你就在練習自己思考的能力了\n",
            "\n",
            "82\n",
            "00:04:09,000 --> 00:04:14,000\n",
            "好 以上說的是課業內的部分\n",
            "\n",
            "83\n",
            "00:04:14,000 --> 00:04:18,000\n",
            "那當然除了課業內之外呢\n",
            "\n",
            "84\n",
            "00:04:18,000 --> 00:04:21,000\n",
            "還有一大堆是不在課業內的\n",
            "\n",
            "85\n",
            "00:04:21,000 --> 00:04:24,000\n",
            "那就是課業外的\n",
            "\n",
            "86\n",
            "00:04:24,000 --> 00:04:27,000\n",
            "課業外也有很多式的\n",
            "\n",
            "87\n",
            "00:04:27,000 --> 00:04:33,000\n",
            "那我們可以舉例來說\n",
            "\n",
            "88\n",
            "00:04:33,000 --> 00:04:38,000\n",
            "課業外有什麼可以學習的\n",
            "\n",
            "89\n",
            "00:04:38,000 --> 00:04:42,000\n",
            "那我通常把學習定義成為\n",
            "\n",
            "90\n",
            "00:04:42,000 --> 00:04:43,000\n",
            "什麼是學習\n",
            "\n",
            "91\n",
            "00:04:43,000 --> 00:04:49,000\n",
            "學習就是一種增長\n",
            "\n",
            "92\n",
            "00:04:49,000 --> 00:04:53,000\n",
            "一種進步\n",
            "\n",
            "93\n",
            "00:04:53,000 --> 00:04:57,000\n",
            "然後獲得快樂\n",
            "\n",
            "94\n",
            "00:04:57,000 --> 00:05:00,000\n",
            "這就是學習\n",
            "\n",
            "95\n",
            "00:05:00,000 --> 00:05:03,000\n",
            "所以即使是課業外的任何事情\n",
            "\n",
            "96\n",
            "00:05:03,000 --> 00:05:05,000\n",
            "只要你覺得是有增長的\n",
            "\n",
            "97\n",
            "00:05:05,000 --> 00:05:07,000\n",
            "是有進步的\n",
            "\n",
            "98\n",
            "00:05:07,000 --> 00:05:08,000\n",
            "讓你覺得快樂的\n",
            "\n",
            "99\n",
            "00:05:08,000 --> 00:05:12,000\n",
            "那應該就是值得學習的地方\n",
            "\n",
            "100\n",
            "00:05:12,000 --> 00:05:16,000\n",
            "那我們可以舉很多例子\n",
            "\n",
            "101\n",
            "00:05:16,000 --> 00:05:20,000\n",
            "譬如說很多同學喜歡打球\n",
            "\n",
            "102\n",
            "00:05:20,000 --> 00:05:22,000\n",
            "打球是不是學習\n",
            "\n",
            "103\n",
            "00:05:22,000 --> 00:05:24,000\n",
            "當然是\n",
            "\n",
            "104\n",
            "00:05:24,000 --> 00:05:26,000\n",
            "在打球中間有沒有增長\n",
            "\n",
            "105\n",
            "00:05:26,000 --> 00:05:27,000\n",
            "當然是\n",
            "\n",
            "106\n",
            "00:05:27,000 --> 00:05:30,000\n",
            "在打球中間有沒有增長\n",
            "\n",
            "107\n",
            "00:05:30,000 --> 00:05:31,000\n",
            "當然有增長\n",
            "\n",
            "108\n",
            "00:05:31,000 --> 00:05:35,000\n",
            "打球不只是對健康有增長\n",
            "\n",
            "109\n",
            "00:05:35,000 --> 00:05:39,000\n",
            "而且可能對於譬如說手腦協調\n",
            "\n",
            "110\n",
            "00:05:39,000 --> 00:05:41,000\n",
            "譬如說團隊精神\n",
            "\n",
            "111\n",
            "00:05:41,000 --> 00:05:44,000\n",
            "譬如說個人之間的互動\n",
            "\n",
            "112\n",
            "00:05:44,000 --> 00:05:45,000\n",
            "什麼可能都有幫助\n",
            "\n",
            "113\n",
            "00:05:45,000 --> 00:05:47,000\n",
            "所以打球當然是有增長的\n",
            "\n",
            "114\n",
            "00:05:47,000 --> 00:05:50,000\n",
            "那當然是很好的學習的機會\n",
            "\n",
            "115\n",
            "00:05:50,000 --> 00:05:52,000\n",
            "有人喜歡爬山\n",
            "\n",
            "116\n",
            "00:05:52,000 --> 00:05:54,000\n",
            "爬山是不是好的學習機會\n",
            "\n",
            "117\n",
            "00:05:54,000 --> 00:05:55,000\n",
            "當然是\n",
            "\n",
            "118\n",
            "00:05:55,000 --> 00:05:57,000\n",
            "這個我以前兩年前就講過很多\n",
            "\n",
            "119\n",
            "00:05:57,000 --> 00:05:59,000\n",
            "爬山可以學到很多的\n",
            "\n",
            "120\n",
            "00:05:59,000 --> 00:06:02,000\n",
            "那爬山當然是一種學習\n",
            "\n",
            "121\n",
            "00:06:02,000 --> 00:06:03,000\n",
            "有人說我不喜歡爬山\n",
            "\n",
            "122\n",
            "00:06:03,000 --> 00:06:05,000\n",
            "我去旅行好不好\n",
            "\n",
            "123\n",
            "00:06:05,000 --> 00:06:07,000\n",
            "旅行當然好\n",
            "\n",
            "124\n",
            "00:06:07,000 --> 00:06:09,000\n",
            "旅行可以增長見識\n",
            "\n",
            "125\n",
            "00:06:09,000 --> 00:06:11,000\n",
            "可以擴增事業\n",
            "\n",
            "126\n",
            "00:06:11,000 --> 00:06:13,000\n",
            "可以增加很多很多\n",
            "\n",
            "127\n",
            "00:06:13,000 --> 00:06:14,000\n",
            "當然是有進步的\n",
            "\n",
            "128\n",
            "00:06:14,000 --> 00:06:16,000\n",
            "所以當然是很好的學習\n",
            "\n",
            "129\n",
            "00:06:16,000 --> 00:06:20,000\n",
            "你凡是獲得快樂都是很好的事\n",
            "\n",
            "130\n",
            "00:06:20,000 --> 00:06:23,000\n",
            "那這些都值得下功夫去\n",
            "\n",
            "131\n",
            "00:06:23,000 --> 00:06:25,000\n",
            "把它看成是學習\n",
            "\n",
            "132\n",
            "00:06:25,000 --> 00:06:27,000\n",
            "都值得下功夫去做的\n",
            "\n",
            "133\n",
            "00:06:27,000 --> 00:06:29,000\n",
            "我們再講另外一系列\n",
            "\n",
            "134\n",
            "00:06:29,000 --> 00:06:30,000\n",
            "譬如說\n",
            "\n",
            "135\n",
            "00:06:30,000 --> 00:06:34,000\n",
            "有人說談戀愛是不是學習\n",
            "\n",
            "136\n",
            "00:06:34,000 --> 00:06:37,000\n",
            "談戀愛除了你在談戀愛上\n",
            "\n",
            "137\n",
            "00:06:37,000 --> 00:06:39,000\n",
            "會有收穫以外\n",
            "\n",
            "138\n",
            "00:06:39,000 --> 00:06:41,000\n",
            "本身也是有收穫的\n",
            "\n",
            "139\n",
            "00:06:41,000 --> 00:06:44,000\n",
            "因為讓你體驗到人跟人之間\n",
            "\n",
            "140\n",
            "00:06:44,000 --> 00:06:45,000\n",
            "的各種感覺\n",
            "\n",
            "141\n",
            "00:06:45,000 --> 00:06:48,000\n",
            "人跟人之間的各種期待等等\n",
            "\n",
            "142\n",
            "00:06:48,000 --> 00:06:50,000\n",
            "有沒有幫助\n",
            "\n",
            "143\n",
            "00:06:50,000 --> 00:06:52,000\n",
            "當然有幫助\n",
            "\n",
            "144\n",
            "00:06:52,000 --> 00:06:54,000\n",
            "對每一個人都是很好的學習\n",
            "\n",
            "145\n",
            "00:06:54,000 --> 00:06:57,000\n",
            "所以談戀愛當然是一件很好的事\n",
            "\n",
            "146\n",
            "00:06:57,000 --> 00:07:00,000\n",
            "有人會說那要靠緣分\n",
            "\n",
            "147\n",
            "00:07:00,000 --> 00:07:02,000\n",
            "沒有緣分沒有辦法\n",
            "\n",
            "148\n",
            "00:07:02,000 --> 00:07:03,000\n",
            "對不對\n",
            "\n",
            "149\n",
            "00:07:03,000 --> 00:07:04,000\n",
            "對\n",
            "\n",
            "150\n",
            "00:07:04,000 --> 00:07:06,000\n",
            "但是你不是一定要談戀愛嗎\n",
            "\n",
            "151\n",
            "00:07:06,000 --> 00:07:07,000\n",
            "你可以交朋友\n",
            "\n",
            "152\n",
            "00:07:07,000 --> 00:07:10,000\n",
            "交朋友是不是學習\n",
            "\n",
            "153\n",
            "00:07:10,000 --> 00:07:11,000\n",
            "當然是\n",
            "\n",
            "154\n",
            "00:07:11,000 --> 00:07:13,000\n",
            "交朋友也一樣\n",
            "\n",
            "155\n",
            "00:07:13,000 --> 00:07:16,000\n",
            "讓我們學到很多人際的互動\n",
            "\n",
            "156\n",
            "00:07:16,000 --> 00:07:20,000\n",
            "學到很多人跟人之間的溝通\n",
            "\n",
            "157\n",
            "00:07:20,000 --> 00:07:22,000\n",
            "人跟人之間的期待\n",
            "\n",
            "158\n",
            "00:07:22,000 --> 00:07:24,000\n",
            "人跟人之間的感覺\n",
            "\n",
            "159\n",
            "00:07:24,000 --> 00:07:26,000\n",
            "這都是交朋友之後學到的\n",
            "\n",
            "160\n",
            "00:07:26,000 --> 00:07:28,000\n",
            "對我們電機系的同學而言\n",
            "\n",
            "161\n",
            "00:07:28,000 --> 00:07:31,000\n",
            "你四周有一大群好同學\n",
            "\n",
            "162\n",
            "00:07:31,000 --> 00:07:34,000\n",
            "都是很好的交朋友的對象\n",
            "\n",
            "163\n",
            "00:07:34,000 --> 00:07:36,000\n",
            "你下一番功夫交朋友好不好\n",
            "\n",
            "164\n",
            "00:07:36,000 --> 00:07:37,000\n",
            "好\n",
            "\n",
            "165\n",
            "00:07:37,000 --> 00:07:40,000\n",
            "當然是有幫助的\n",
            "\n",
            "166\n",
            "00:07:40,000 --> 00:07:42,000\n",
            "另外當然我們可以舉很多\n",
            "\n",
            "167\n",
            "00:07:42,000 --> 00:07:44,000\n",
            "我們最現成的例子\n",
            "\n",
            "168\n",
            "00:07:44,000 --> 00:07:47,000\n",
            "譬如說我們的戲學會辦各種活動\n",
            "\n",
            "169\n",
            "00:07:47,000 --> 00:07:49,000\n",
            "那些活動有沒有幫助\n",
            "\n",
            "170\n",
            "00:07:49,000 --> 00:07:50,000\n",
            "當然有\n",
            "\n",
            "171\n",
            "00:07:50,000 --> 00:07:54,000\n",
            "我們舉例來講電業\n",
            "\n",
            "172\n",
            "00:07:54,000 --> 00:07:57,000\n",
            "你如果去參加某一個舞跳個舞\n",
            "\n",
            "173\n",
            "00:07:57,000 --> 00:08:00,000\n",
            "或者參加某個劇演個劇\n",
            "\n",
            "174\n",
            "00:08:00,000 --> 00:08:01,000\n",
            "有沒有幫助\n",
            "\n",
            "175\n",
            "00:08:01,000 --> 00:08:02,000\n",
            "當然有幫助\n",
            "\n",
            "176\n",
            "00:08:02,000 --> 00:08:05,000\n",
            "你在這中間一定發現有所增長\n",
            "\n",
            "177\n",
            "00:08:05,000 --> 00:08:06,000\n",
            "有所進步\n",
            "\n",
            "178\n",
            "00:08:06,000 --> 00:08:09,000\n",
            "那是為什麼有那麼多同學要去參加\n",
            "\n",
            "179\n",
            "00:08:09,000 --> 00:08:13,000\n",
            "就是因為發現那個確實是有增長有進步\n",
            "\n",
            "180\n",
            "00:08:13,000 --> 00:08:15,000\n",
            "有的人說\n",
            "\n",
            "181\n",
            "00:08:15,000 --> 00:08:17,000\n",
            "我不去跳那個舞\n",
            "\n",
            "182\n",
            "00:08:17,000 --> 00:08:20,000\n",
            "或者演那個劇\n",
            "\n",
            "183\n",
            "00:08:20,000 --> 00:08:22,000\n",
            "我做幕後的\n",
            "\n",
            "184\n",
            "00:08:22,000 --> 00:08:25,000\n",
            "譬如說是幕後的什麼規劃\n",
            "\n",
            "185\n",
            "00:08:25,000 --> 00:08:29,000\n",
            "或者說是什麼光舞的什麼軟體組\n",
            "\n",
            "186\n",
            "00:08:29,000 --> 00:08:32,000\n",
            "還是什麼服裝道具組\n",
            "\n",
            "187\n",
            "00:08:32,000 --> 00:08:33,000\n",
            "一樣\n",
            "\n",
            "188\n",
            "00:08:33,000 --> 00:08:36,000\n",
            "那個都是可以有獲得很多的增長\n",
            "\n",
            "189\n",
            "00:08:36,000 --> 00:08:37,000\n",
            "很多進步的\n",
            "\n",
            "190\n",
            "00:08:37,000 --> 00:08:39,000\n",
            "當然都是很有用的\n",
            "\n",
            "191\n",
            "00:08:39,000 --> 00:08:42,000\n",
            "都是很好的學習\n",
            "\n",
            "192\n",
            "00:08:42,000 --> 00:08:47,000\n",
            "那當然也包括電業以外的戲學會\n",
            "\n",
            "193\n",
            "00:08:47,000 --> 00:08:50,000\n",
            "其他的各種活動都一樣\n",
            "\n",
            "194\n",
            "00:08:50,000 --> 00:08:55,000\n",
            "也包括電機系以外的其他的校內\n",
            "\n",
            "195\n",
            "00:08:55,000 --> 00:08:59,000\n",
            "或者校外的各種活動幾乎都一樣\n",
            "\n",
            "196\n",
            "00:08:59,000 --> 00:09:02,000\n",
            "都可以讓人有所增長有所進步\n",
            "\n",
            "197\n",
            "00:09:02,000 --> 00:09:04,000\n",
            "都是很好的學習的機會\n",
            "\n",
            "198\n",
            "00:09:04,000 --> 00:09:06,000\n",
            "都是很好的學習\n",
            "\n",
            "199\n",
            "00:09:06,000 --> 00:09:10,000\n",
            "同樣的問題是這些東西都沒有考試\n",
            "\n",
            "200\n",
            "00:09:10,000 --> 00:09:12,000\n",
            "沒有成績\n",
            "\n",
            "201\n",
            "00:09:12,000 --> 00:09:14,000\n",
            "不能顯示在成績單上\n",
            "\n",
            "202\n",
            "00:09:14,000 --> 00:09:18,000\n",
            "因此對有一些同學會認為那個浪費時間\n",
            "\n",
            "203\n",
            "00:09:18,000 --> 00:09:20,000\n",
            "我不需要花時間去做那個\n",
            "\n",
            "204\n",
            "00:09:20,000 --> 00:09:24,000\n",
            "因為不影響我的overfitting的目標\n",
            "\n",
            "205\n",
            "00:09:24,000 --> 00:09:26,000\n",
            "裡面沒有這個嘛\n",
            "\n",
            "206\n",
            "00:09:26,000 --> 00:09:28,000\n",
            "具體成績沒有這些嘛\n",
            "\n",
            "207\n",
            "00:09:28,000 --> 00:09:30,000\n",
            "那不要這樣想\n",
            "\n",
            "208\n",
            "00:09:30,000 --> 00:09:33,000\n",
            "因為那些都非常的重要\n",
            "\n",
            "209\n",
            "00:09:33,000 --> 00:09:36,000\n",
            "都對你發展非常的重要\n",
            "\n",
            "210\n",
            "00:09:36,000 --> 00:09:39,000\n",
            "那我們說電機工程\n",
            "\n",
            "211\n",
            "00:09:39,000 --> 00:09:41,000\n",
            "今天的電機工程\n",
            "\n",
            "212\n",
            "00:09:41,000 --> 00:09:44,000\n",
            "很少什麼事情自己一個人可以做成功的\n",
            "\n",
            "213\n",
            "00:09:44,000 --> 00:09:47,000\n",
            "你必須跟很多人一起\n",
            "\n",
            "214\n",
            "00:09:47,000 --> 00:09:50,000\n",
            "才可能做成功一個非常重要的\n",
            "\n",
            "215\n",
            "00:09:50,000 --> 00:09:52,000\n",
            "有意義的工作\n",
            "\n",
            "216\n",
            "00:09:52,000 --> 00:09:55,000\n",
            "那當你跟一群人在一起做的時候\n",
            "\n",
            "217\n",
            "00:09:55,000 --> 00:09:59,000\n",
            "你必須學會如何進入一個團隊\n",
            "\n",
            "218\n",
            "00:09:59,000 --> 00:10:03,000\n",
            "從邊緣開始慢慢進入核心\n",
            "\n",
            "219\n",
            "00:10:03,000 --> 00:10:06,000\n",
            "從底層開始慢慢變成leader\n",
            "\n",
            "220\n",
            "00:10:06,000 --> 00:10:09,000\n",
            "然後如何可以推動你想做的事\n",
            "\n",
            "221\n",
            "00:10:09,000 --> 00:10:13,000\n",
            "如何變成可以做到你想做的事等等\n",
            "\n",
            "222\n",
            "00:10:13,000 --> 00:10:15,000\n",
            "這些都是很重要的\n",
            "\n",
            "223\n",
            "00:10:15,000 --> 00:10:18,000\n",
            "那我們通常稱這些東西\n",
            "\n",
            "224\n",
            "00:10:18,000 --> 00:10:21,000\n",
            "是所謂的soft skills\n",
            "\n",
            "225\n",
            "00:10:21,000 --> 00:10:24,000\n",
            "也就是軟實力\n",
            "\n",
            "226\n",
            "00:10:24,000 --> 00:10:30,000\n",
            "所謂軟實力就是硬實力以外的軟實力\n",
            "\n",
            "227\n",
            "00:10:30,000 --> 00:10:34,000\n",
            "硬實力是說你電子學的功力\n",
            "\n",
            "228\n",
            "00:10:34,000 --> 00:10:36,000\n",
            "數學的功力\n",
            "\n",
            "229\n",
            "00:10:36,000 --> 00:10:40,000\n",
            "這個城市能力這種是硬實力\n",
            "\n",
            "230\n",
            "00:10:40,000 --> 00:10:45,000\n",
            "軟實力我們主要就是講各種人際之間的\n",
            "\n",
            "231\n",
            "00:10:45,000 --> 00:10:49,000\n",
            "在人跟人之間的各種能力\n",
            "\n",
            "232\n",
            "00:10:49,000 --> 00:10:54,000\n",
            "包括溝通能力協調能力交朋友的能力\n",
            "\n",
            "233\n",
            "00:10:54,000 --> 00:10:59,000\n",
            "說服人的能力團隊精神領導能力等等\n",
            "\n",
            "234\n",
            "00:10:59,000 --> 00:11:02,000\n",
            "那些就是所謂的soft skills\n",
            "\n",
            "235\n",
            "00:11:02,000 --> 00:11:04,000\n",
            "重要不重要重要\n",
            "\n",
            "236\n",
            "00:11:04,000 --> 00:11:07,000\n",
            "你看到任何一個成功的電機工程師\n",
            "\n",
            "237\n",
            "00:11:07,000 --> 00:11:09,000\n",
            "他都有一堆這種\n",
            "\n",
            "238\n",
            "00:11:09,000 --> 00:11:13,000\n",
            "這個才是他成功的一個非常重要的關鍵\n",
            "\n",
            "239\n",
            "00:11:13,000 --> 00:11:15,000\n",
            "這種東西怎麼來\n",
            "\n",
            "240\n",
            "00:11:15,000 --> 00:11:18,000\n",
            "我們剛才講的各種課業外的\n",
            "\n",
            "241\n",
            "00:11:18,000 --> 00:11:20,000\n",
            "各種學習增長的機會\n",
            "\n",
            "242\n",
            "00:11:20,000 --> 00:11:26,000\n",
            "都可以幫助一個人塑造他的soft skills\n",
            "\n",
            "243\n",
            "00:11:26,000 --> 00:11:30,000\n",
            "是有少數人的這些soft skills是天生的\n",
            "\n",
            "244\n",
            "00:11:30,000 --> 00:11:31,000\n",
            "他天生就厲害\n",
            "\n",
            "245\n",
            "00:11:31,000 --> 00:11:32,000\n",
            "有沒有\n",
            "\n",
            "246\n",
            "00:11:32,000 --> 00:11:33,000\n",
            "有\n",
            "\n",
            "247\n",
            "00:11:33,000 --> 00:11:35,000\n",
            "但這種人畢竟沒那麼多\n",
            "\n",
            "248\n",
            "00:11:35,000 --> 00:11:37,000\n",
            "對很多人而言\n",
            "\n",
            "249\n",
            "00:11:37,000 --> 00:11:42,000\n",
            "他的soft skills是自己努力慢慢培養起來的\n",
            "\n",
            "250\n",
            "00:11:42,000 --> 00:11:45,000\n",
            "我剛才一開始前面講的那一段\n",
            "\n",
            "251\n",
            "00:11:45,000 --> 00:11:49,000\n",
            "我說我在進台大電機系以前\n",
            "\n",
            "252\n",
            "00:11:49,000 --> 00:11:51,000\n",
            "我幾乎不會交朋友\n",
            "\n",
            "253\n",
            "00:11:51,000 --> 00:11:53,000\n",
            "我不太會說話\n",
            "\n",
            "254\n",
            "00:11:53,000 --> 00:11:57,000\n",
            "我在讀大學的四年裡面改變我自己\n",
            "\n",
            "255\n",
            "00:11:57,000 --> 00:12:02,000\n",
            "讓我變成有很多這方面的能力的人\n",
            "\n",
            "256\n",
            "00:12:02,000 --> 00:12:06,000\n",
            "其實最重要就是我的很多soft skills\n",
            "\n",
            "257\n",
            "00:12:06,000 --> 00:12:07,000\n",
            "都是我自己培養\n",
            "\n",
            "258\n",
            "00:12:07,000 --> 00:12:10,000\n",
            "在讀台大電機系的四年裡面\n",
            "\n",
            "259\n",
            "00:12:10,000 --> 00:12:14,000\n",
            "獲得的非常多這方面的收穫的\n",
            "\n",
            "260\n",
            "00:12:14,000 --> 00:12:18,000\n",
            "那是為什麼我每次都要強調\n",
            "\n",
            "261\n",
            "00:12:18,000 --> 00:12:21,000\n",
            "這個東西有多麼重要\n",
            "\n",
            "262\n",
            "00:12:21,000 --> 00:12:27,000\n",
            "我之前曾經在幾年前的這個\n",
            "\n",
            "263\n",
            "00:12:27,000 --> 00:12:31,000\n",
            "信號與人生裡面有說到這一件事\n",
            "\n",
            "264\n",
            "00:12:31,000 --> 00:12:33,000\n",
            "我現在不要重複\n",
            "\n",
            "265\n",
            "00:12:33,000 --> 00:12:36,000\n",
            "但是我簡單的summarize\n",
            "\n",
            "266\n",
            "00:12:36,000 --> 00:12:42,000\n",
            "我說我們電機系的電機工程師的\n",
            "\n",
            "267\n",
            "00:12:42,000 --> 00:12:45,000\n",
            "一生career的發展\n",
            "\n",
            "268\n",
            "00:12:45,000 --> 00:12:48,000\n",
            "黃金實在是在什麼時候\n",
            "\n",
            "269\n",
            "00:12:48,000 --> 00:12:52,000\n",
            "我認為是在35歲到55歲\n",
            "\n",
            "270\n",
            "00:12:52,000 --> 00:12:56,000\n",
            "這20年是我們的黃金時代\n",
            "\n",
            "271\n",
            "00:12:56,000 --> 00:12:58,000\n",
            "在這以前當然更好\n",
            "\n",
            "272\n",
            "00:12:58,000 --> 00:13:01,000\n",
            "只是說可能各方面尚未具備\n",
            "\n",
            "273\n",
            "00:13:01,000 --> 00:13:03,000\n",
            "還沒有完全訓練的好\n",
            "\n",
            "274\n",
            "00:13:03,000 --> 00:13:05,000\n",
            "在這以後是最好的\n",
            "\n",
            "275\n",
            "00:13:05,000 --> 00:13:10,000\n",
            "這以後年紀大了難免有一些要打個折扣等等\n",
            "\n",
            "276\n",
            "00:13:10,000 --> 00:13:13,000\n",
            "就這裡面我們看到\n",
            "\n",
            "277\n",
            "00:13:13,000 --> 00:13:15,000\n",
            "我們的電機系的畢業的同學\n",
            "\n",
            "278\n",
            "00:13:15,000 --> 00:13:18,000\n",
            "過去有幾千人畢業我都看到\n",
            "\n",
            "279\n",
            "00:13:18,000 --> 00:13:22,000\n",
            "我覺得有的人的發展是像這樣\n",
            "\n",
            "280\n",
            "00:13:22,000 --> 00:13:24,000\n",
            "有一定的斜率\n",
            "\n",
            "281\n",
            "00:13:24,000 --> 00:13:28,000\n",
            "但到某一個階段它會慢慢saturate\n",
            "\n",
            "282\n",
            "00:13:28,000 --> 00:13:31,000\n",
            "有的人也許開始向上比較晚\n",
            "\n",
            "283\n",
            "00:13:31,000 --> 00:13:33,000\n",
            "但它斜率比較高\n",
            "\n",
            "284\n",
            "00:13:33,000 --> 00:13:38,000\n",
            "它最後會saturate在比較高的地方\n",
            "\n",
            "285\n",
            "00:13:38,000 --> 00:13:41,000\n",
            "也有的人也許開始的比較快\n",
            "\n",
            "286\n",
            "00:13:41,000 --> 00:13:44,000\n",
            "但是後來會overshoot之後\n",
            "\n",
            "287\n",
            "00:13:44,000 --> 00:13:46,000\n",
            "會開始收斂在比較低的地方等等\n",
            "\n",
            "288\n",
            "00:13:46,000 --> 00:13:48,000\n",
            "每一個人都不一樣\n",
            "\n",
            "289\n",
            "00:13:48,000 --> 00:13:50,000\n",
            "但是當然也有一種人\n",
            "\n",
            "290\n",
            "00:13:50,000 --> 00:13:54,000\n",
            "你會看到他一直向上走\n",
            "\n",
            "291\n",
            "00:13:54,000 --> 00:13:58,000\n",
            "完全沒有saturate\n",
            "\n",
            "292\n",
            "00:13:58,000 --> 00:14:01,000\n",
            "這些人這些差別在哪裡\n",
            "\n",
            "293\n",
            "00:14:01,000 --> 00:14:04,000\n",
            "這些東西差別在哪裡\n",
            "\n",
            "294\n",
            "00:14:04,000 --> 00:14:06,000\n",
            "我以前已經說過這件事\n",
            "\n",
            "295\n",
            "00:14:06,000 --> 00:14:08,000\n",
            "我不要多重複\n",
            "\n",
            "296\n",
            "00:14:08,000 --> 00:14:11,000\n",
            "我說最主要因素有四個\n",
            "\n",
            "297\n",
            "00:14:11,000 --> 00:14:18,000\n",
            "實力、努力、大智\n",
            "\n",
            "298\n",
            "00:14:18,000 --> 00:14:23,000\n",
            "跟self skill這四件事情\n",
            "\n",
            "299\n",
            "00:14:23,000 --> 00:14:29,000\n",
            "我認為真正影響這個的\n",
            "\n",
            "300\n",
            "00:14:29,000 --> 00:14:32,000\n",
            "不是因為電子學考得好不好\n",
            "\n",
            "301\n",
            "00:14:32,000 --> 00:14:35,000\n",
            "不是因為信號與系統念得好不好\n",
            "\n",
            "302\n",
            "00:14:35,000 --> 00:14:37,000\n",
            "也就是我剛才講\n",
            "\n",
            "303\n",
            "00:14:37,000 --> 00:14:39,000\n",
            "你把每一門必修課\n",
            "\n",
            "304\n",
            "00:14:39,000 --> 00:14:41,000\n",
            "當成是單一跑道\n",
            "\n",
            "305\n",
            "00:14:41,000 --> 00:14:43,000\n",
            "跑到第一名並不表示怎樣\n",
            "\n",
            "306\n",
            "00:14:43,000 --> 00:14:45,000\n",
            "我們最後不看那個的\n",
            "\n",
            "307\n",
            "00:14:45,000 --> 00:14:47,000\n",
            "最後看的是這個\n",
            "\n",
            "308\n",
            "00:14:47,000 --> 00:14:49,000\n",
            "這個是怎麼樣影響\n",
            "\n",
            "309\n",
            "00:14:49,000 --> 00:14:51,000\n",
            "我認為是這四件事\n",
            "\n",
            "310\n",
            "00:14:51,000 --> 00:14:55,000\n",
            "就是實力、努力、大智跟self skills\n",
            "\n",
            "311\n",
            "00:14:55,000 --> 00:14:57,000\n",
            "這四件事裡面\n",
            "\n",
            "312\n",
            "00:14:57,000 --> 00:14:59,000\n",
            "我們現在可以summarize\n",
            "\n",
            "313\n",
            "00:14:59,000 --> 00:15:00,000\n",
            "我剛才講的\n",
            "\n",
            "314\n",
            "00:15:00,000 --> 00:15:02,000\n",
            "什麼是實力\n",
            "\n",
            "315\n",
            "00:15:02,000 --> 00:15:05,000\n",
            "實力就是所有的這些\n",
            "\n",
            "316\n",
            "00:15:05,000 --> 00:15:07,000\n",
            "我們電機工程的專業領域裡面\n",
            "\n",
            "317\n",
            "00:15:07,000 --> 00:15:09,000\n",
            "各種東西的實力\n",
            "\n",
            "318\n",
            "00:15:09,000 --> 00:15:11,000\n",
            "實力怎麼厲害法\n",
            "\n",
            "319\n",
            "00:15:11,000 --> 00:15:13,000\n",
            "就是我剛才講的\n",
            "\n",
            "320\n",
            "00:15:13,000 --> 00:15:16,000\n",
            "你如果都是在做全面的學習的話\n",
            "\n",
            "321\n",
            "00:15:16,000 --> 00:15:18,000\n",
            "你就會學到各種該學到的\n",
            "\n",
            "322\n",
            "00:15:18,000 --> 00:15:20,000\n",
            "最後你的實力就是很強的\n",
            "\n",
            "323\n",
            "00:15:20,000 --> 00:15:25,000\n",
            "所以實力最主要就是不要overfitting\n",
            "\n",
            "324\n",
            "00:15:25,000 --> 00:15:27,000\n",
            "要盡量都做\n",
            "\n",
            "325\n",
            "00:15:27,000 --> 00:15:31,000\n",
            "學到該學的全面的學習\n",
            "\n",
            "326\n",
            "00:15:31,000 --> 00:15:33,000\n",
            "努力是沒有疑問\n",
            "\n",
            "327\n",
            "00:15:33,000 --> 00:15:34,000\n",
            "每一個人都了解\n",
            "\n",
            "328\n",
            "00:15:34,000 --> 00:15:36,000\n",
            "確實我們可以看到\n",
            "\n",
            "329\n",
            "00:15:36,000 --> 00:15:38,000\n",
            "一個人在未來的幾十年裡面\n",
            "\n",
            "330\n",
            "00:15:38,000 --> 00:15:40,000\n",
            "有的人他一直努力\n",
            "\n",
            "331\n",
            "00:15:40,000 --> 00:15:42,000\n",
            "有的人慢慢不太努力等等\n",
            "\n",
            "332\n",
            "00:15:42,000 --> 00:15:44,000\n",
            "這個是有明顯差別的\n",
            "\n",
            "333\n",
            "00:15:44,000 --> 00:15:46,000\n",
            "那self skills我剛才已經講了\n",
            "\n",
            "334\n",
            "00:15:46,000 --> 00:15:50,000\n",
            "就是很多我們平常沒有算成績\n",
            "\n",
            "335\n",
            "00:15:50,000 --> 00:15:52,000\n",
            "覺得大家不重視的事情\n",
            "\n",
            "336\n",
            "00:15:52,000 --> 00:15:54,000\n",
            "其實它常常是很重要的\n",
            "\n",
            "337\n",
            "00:15:54,000 --> 00:15:56,000\n",
            "你如果好好的\n",
            "\n",
            "338\n",
            "00:15:56,000 --> 00:16:00,000\n",
            "多在各種課業外的事情上\n",
            "\n",
            "339\n",
            "00:16:00,000 --> 00:16:02,000\n",
            "增長進步的話\n",
            "\n",
            "340\n",
            "00:16:02,000 --> 00:16:04,000\n",
            "你這些東西會很強\n",
            "\n",
            "341\n",
            "00:16:04,000 --> 00:16:06,000\n",
            "你會很厲害的\n",
            "\n",
            "342\n",
            "00:16:06,000 --> 00:16:08,000\n",
            "當然對少數人而言\n",
            "\n",
            "343\n",
            "00:16:08,000 --> 00:16:09,000\n",
            "他天生就有\n",
            "\n",
            "344\n",
            "00:16:09,000 --> 00:16:10,000\n",
            "他可能不需要\n",
            "\n",
            "345\n",
            "00:16:10,000 --> 00:16:12,000\n",
            "這是每一個人不一樣的\n",
            "\n",
            "346\n",
            "00:16:12,000 --> 00:16:14,000\n",
            "那這三個我都提過了\n",
            "\n",
            "347\n",
            "00:16:14,000 --> 00:16:16,000\n",
            "那麼大致我還沒有提\n",
            "\n",
            "348\n",
            "00:16:16,000 --> 00:16:18,000\n",
            "其實大致沒有什麼要特別說的\n",
            "\n",
            "349\n",
            "00:16:18,000 --> 00:16:21,000\n",
            "那應該就是我剛才前面有講過\n",
            "\n",
            "350\n",
            "00:16:21,000 --> 00:16:26,000\n",
            "就是每一個人可以有你自己的長程目標\n",
            "\n",
            "351\n",
            "00:16:26,000 --> 00:16:29,000\n",
            "那有的人本來就有了\n",
            "\n",
            "352\n",
            "00:16:29,000 --> 00:16:32,000\n",
            "有的人也許我平常沒有想過\n",
            "\n",
            "353\n",
            "00:16:32,000 --> 00:16:37,000\n",
            "那你可以在適當時機開始想\n",
            "\n",
            "354\n",
            "00:16:37,000 --> 00:16:40,000\n",
            "我有沒有想要做什麼事情\n",
            "\n",
            "355\n",
            "00:16:40,000 --> 00:16:43,000\n",
            "哪些事情可能是我的長程目標\n",
            "\n",
            "356\n",
            "00:16:43,000 --> 00:16:48,000\n",
            "我希望最後讓我花個5年10年\n",
            "\n",
            "357\n",
            "00:16:48,000 --> 00:16:50,000\n",
            "15年或者更長\n",
            "\n",
            "358\n",
            "00:16:50,000 --> 00:16:52,000\n",
            "我把我的很多的努力\n",
            "\n",
            "359\n",
            "00:16:52,000 --> 00:16:55,000\n",
            "都來把某一些事情做得非常漂亮\n",
            "\n",
            "360\n",
            "00:16:55,000 --> 00:16:57,000\n",
            "那是我很想做的事\n",
            "\n",
            "361\n",
            "00:16:57,000 --> 00:17:00,000\n",
            "那就是長程目標\n",
            "\n",
            "362\n",
            "00:17:00,000 --> 00:17:04,000\n",
            "如果我覺得做那些事情會讓我非常的\n",
            "\n",
            "363\n",
            "00:17:04,000 --> 00:17:05,000\n",
            "覺得有意義\n",
            "\n",
            "364\n",
            "00:17:05,000 --> 00:17:07,000\n",
            "願意花功夫下去做的\n",
            "\n",
            "365\n",
            "00:17:07,000 --> 00:17:09,000\n",
            "那就是我的長程目標\n",
            "\n",
            "366\n",
            "00:17:09,000 --> 00:17:12,000\n",
            "那有的人如果可以想出這個來的話\n",
            "\n",
            "367\n",
            "00:17:12,000 --> 00:17:15,000\n",
            "那就是他的大致\n",
            "\n",
            "368\n",
            "00:17:15,000 --> 00:17:17,000\n",
            "那越是有這種大致的人\n",
            "\n",
            "369\n",
            "00:17:17,000 --> 00:17:20,000\n",
            "也比較容易向上衝\n",
            "\n",
            "370\n",
            "00:17:20,000 --> 00:17:25,000\n",
            "那我感覺起來真正影響的就是這四件事\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "''' Open the SRT file and read its content.\n",
        "The format of SRT is:\n",
        "\n",
        "[Index]\n",
        "[Begin time] (hour:minute:second) --> [End time] (hour:minute:second)\n",
        "[Transcription]\n",
        "\n",
        "'''\n",
        "\n",
        "with open(output_subtitle_path, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7JcN-kUDE_g"
      },
      "source": [
        "# Part3 - Preprocess the results of automatic speech recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P2R40faVDShf"
      },
      "outputs": [],
      "source": [
        "def extract_and_save_text(srt_filename, output_filename):\n",
        "\n",
        "    '''\n",
        "    (1) Objective:\n",
        "        - This function extracts the text from an SRT file and saves it to a new text file.\n",
        "        - It also converts the Simplified Chinese to Traditional Chinese.\n",
        "\n",
        "    (2) Arguments:\n",
        "\n",
        "        - srt_filename: The path to the SRT file.\n",
        "\n",
        "        - output_filename: The name of the output text file.\n",
        "\n",
        "    (3) Example:\n",
        "        - If your SRT file is named 'subtitle.srt' and you want to save the extracted text to a file named 'output.txt', you can use the function like this:\n",
        "            extract_and_save_text('subtitle.srt', 'output.txt')\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Open the SRT file and read its content.\n",
        "    with open(srt_filename, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Use regular expression to remove the timecode.\n",
        "    pure_text = re.sub(r'\\d+\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}\\n', '', content)\n",
        "\n",
        "    # Remove the empty lines.\n",
        "    pure_text = re.sub(r'\\n\\n+', '\\n', pure_text)\n",
        "\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    pure_text_conversion = cc.convert(pure_text)\n",
        "\n",
        "    # Write the extracted text to a new file.\n",
        "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(pure_text_conversion)\n",
        "\n",
        "    print(f'Extracted text has been saved to {output_filename}.\\n\\n')\n",
        "\n",
        "    return pure_text_conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tWDl1vuADd0e"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_length):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - This function is used to split a long string into smaller strings of a specified length.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - text: str, the long string to be split.\n",
        "        - max_length: int, the maximum length of each smaller string.\n",
        "\n",
        "    (3) Returns:\n",
        "        - split_text: list, a list of smaller strings.\n",
        "\n",
        "    (3) Example:\n",
        "        - If you want to split a string named \"long_string\" into smaller strings of length 100, you can use the function like this:\n",
        "            chunk_text(long_string, 100)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return textwrap.wrap(text, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aO01S41pOsSP"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired parameters and the path of input file. '''\n",
        "\n",
        "# # The length of the text chunks.\n",
        "chunk_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O-PpbkoS-5bI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted text has been saved to ./output-信號與人生.txt.\n",
            "\n",
            "\n",
            "Review the results of splitting the long text into several short texts.\n",
            "\n",
            "\n",
            "========== The 1-st segment of the split (505 words) ==========\n",
            "\n",
            "\n",
            "每次說這個學問是做出來的 什麼意思? 要做才會獲得學問 你如果每天光是坐在那裡聽 學問很可能是左耳進右耳出的 你光是坐在那兒讀\n",
            "\n",
            "學問可能從眼睛進入腦海之後就忘掉了 如何能夠學問在腦海裡面 真的變成你自己學問 就是要做 可能有很多同學有這個經驗 你如果去修某一門課 或者做某一個實驗\n",
            "\n",
            "在期末就是要教一個final project 那個final project就是要你把 學到的很多東西 最後整合在你的final project裡面\n",
            "\n",
            "最後做出來的時候 就是把它們都整合了 當你學期結束 真的把final project做完的時候 你會忽然發現 我真的學到很多東西 那就是做出來的學問\n",
            "\n",
            "也許可以舉另外一個例子 就是你如果學了某一些很複雜的演算法 或者什麼 好像覺得那些不見得在你的腦海裡 可是後來老師出了個習題 那個習題教你寫一個很大的程式\n",
            "\n",
            "要把所有東西都包進去 當你把這個程式寫完的時候你會發現 你忽然把演算法裡所有東西都弄通了 那就是學問是做出來的 所以我們永遠要記得 盡量多動手多做\n",
            "\n",
            "在動手跟做的過程之中 學問纔可以變成是自己的 同樣的情形就是說 很多時候這樣動手或者做的表現或者成績 沒有一個成績單上的數字\n",
            "\n",
            "\n",
            "========== The 2-nd segment of the split (506 words) ==========\n",
            "\n",
            "\n",
            "使得很多人覺得那不重要 很多人甚至覺得這門課要做final project 我就不修了太累了 或者說那門課需要怎麼樣怎麼樣太累 我就不要做了\n",
            "\n",
            "而不知道其實那個纔是讓你做的機會 然後可以學到最多 也就是說雖然很可能那麼辛苦的做很多事 沒有讓你獲得什麼具體成績 對你的overfitting可能沒有幫助\n",
            "\n",
            "可是對你的全面學習是很有幫助 是該學的 那不要漏掉這些事 那這是我所說的 那這個課業內可以做的這些事 那剛才我們講到思考的時候 我覺得我漏掉一點\n",
            "\n",
            "你如果修我的信號課你可能會發現 我上課沒講到一個數學式子的時候 我通常都不推他的 我是在解釋那個數學式子在說什麼話 同樣的呢 沒講到一個什麼什麼事情的時候\n",
            "\n",
            "我通常就在解釋他在說什麼話 也就是說 我在講的就是我讀到特本那裡的時候 我心裡怎麼想的 也就是我在告訴同學如何這個讀書的時候 如何一面讀一面練習思考\n",
            "\n",
            "那這個纔是最重要的一件事 如何培養自己思考的能力 跟培養思考的習慣 我覺得最好的辦法就是讀書的時候 凡是讀到一個數學式子都去想一想 那個數學式子到底在說什麼\n",
            "\n",
            "凡是讀到特本上講什麼就去想一想 那個到底在說什麼 你要真的瞭解他在說什麼的時候 你就用了很多思考的功夫\n",
            "\n",
            "\n",
            "========== The 3-rd segment of the split (504 words) ==========\n",
            "\n",
            "\n",
            "你就在練習自己思考的能力了 好 以上說的是課業內的部分 那當然除了課業內之外呢 還有一大堆是不在課業內的 那就是課業外的 課業外也有很多式的 那我們可以舉例來說\n",
            "\n",
            "課業外有什麼可以學習的 那我通常把學習定義成為 什麼是學習 學習就是一種增長 一種進步 然後獲得快樂 這就是學習 所以即使是課業外的任何事情\n",
            "\n",
            "只要你覺得是有增長的 是有進步的 讓你覺得快樂的 那應該就是值得學習的地方 那我們可以舉很多例子 譬如說很多同學喜歡打球 打球是不是學習 當然是\n",
            "\n",
            "在打球中間有沒有增長 當然是 在打球中間有沒有增長 當然有增長 打球不只是對健康有增長 而且可能對於譬如說手腦協調 譬如說團隊精神 譬如說個人之間的互動\n",
            "\n",
            "什麼可能都有幫助 所以打球當然是有增長的 那當然是很好的學習的機會 有人喜歡爬山 爬山是不是好的學習機會 當然是 這個我以前兩年前就講過很多 爬山可以學到很多的\n",
            "\n",
            "那爬山當然是一種學習 有人說我不喜歡爬山 我去旅行好不好 旅行當然好 旅行可以增長見識 可以擴增事業 可以增加很多很多 當然是有進步的 所以當然是很好的學習\n",
            "\n",
            "你凡是獲得快樂都是很好的事 那這些都值得下功夫去 把它看成是學習 都值得下功夫去做的\n",
            "\n",
            "\n",
            "========== The 4-th segment of the split (506 words) ==========\n",
            "\n",
            "\n",
            "我們再講另外一系列 譬如說 有人說談戀愛是不是學習 談戀愛除了你在談戀愛上 會有收穫以外 本身也是有收穫的 因為讓你體驗到人跟人之間 的各種感覺\n",
            "\n",
            "人跟人之間的各種期待等等 有沒有幫助 當然有幫助 對每一個人都是很好的學習 所以談戀愛當然是一件很好的事 有人會說那要靠緣分 沒有緣分沒有辦法 對不對 對\n",
            "\n",
            "但是你不是一定要談戀愛嗎 你可以交朋友 交朋友是不是學習 當然是 交朋友也一樣 讓我們學到很多人際的互動 學到很多人跟人之間的溝通 人跟人之間的期待\n",
            "\n",
            "人跟人之間的感覺 這都是交朋友之後學到的 對我們電機系的同學而言 你四周有一大羣好同學 都是很好的交朋友的對象 你下一番功夫交朋友好不好 好 當然是有幫助的\n",
            "\n",
            "另外當然我們可以舉很多 我們最現成的例子 譬如說我們的戲學會辦各種活動 那些活動有沒有幫助 當然有 我們舉例來講電業 你如果去參加某一個舞跳個舞\n",
            "\n",
            "或者參加某個劇演個劇 有沒有幫助 當然有幫助 你在這中間一定發現有所增長 有所進步 那是為什麼有那麼多同學要去參加 就是因為發現那個確實是有增長有進步 有的人說\n",
            "\n",
            "我不去跳那個舞 或者演那個劇 我做幕後的 譬如說是幕後的什麼規劃 或者說是什麼光舞的什麼軟體組\n",
            "\n",
            "\n",
            "========== The 5-th segment of the split (501 words) ==========\n",
            "\n",
            "\n",
            "還是什麼服裝道具組 一樣 那個都是可以有獲得很多的增長 很多進步的 當然都是很有用的 都是很好的學習 那當然也包括電業以外的戲學會 其他的各種活動都一樣\n",
            "\n",
            "也包括電機系以外的其他的校內 或者校外的各種活動幾乎都一樣 都可以讓人有所增長有所進步 都是很好的學習的機會 都是很好的學習 同樣的問題是這些東西都沒有考試\n",
            "\n",
            "沒有成績 不能顯示在成績單上 因此對有一些同學會認為那個浪費時間 我不需要花時間去做那個 因為不影響我的overfitting的目標 裡面沒有這個嘛\n",
            "\n",
            "具體成績沒有這些嘛 那不要這樣想 因為那些都非常的重要 都對你發展非常的重要 那我們說電機工程 今天的電機工程 很少什麼事情自己一個人可以做成功的\n",
            "\n",
            "你必須跟很多人一起 纔可能做成功一個非常重要的 有意義的工作 那當你跟一羣人在一起做的時候 你必須學會如何進入一個團隊 從邊緣開始慢慢進入核心\n",
            "\n",
            "從底層開始慢慢變成leader 然後如何可以推動你想做的事 如何變成可以做到你想做的事等等 這些都是很重要的 那我們通常稱這些東西 是所謂的soft\n",
            "\n",
            "skills 也就是軟實力 所謂軟實力就是硬實力以外的軟實力 硬實力是說你電子學的功力 數學的功力\n",
            "\n",
            "\n",
            "========== The 6-th segment of the split (504 words) ==========\n",
            "\n",
            "\n",
            "這個城市能力這種是硬實力 軟實力我們主要就是講各種人際之間的 在人跟人之間的各種能力 包括溝通能力協調能力交朋友的能力 說服人的能力團隊精神領導能力等等\n",
            "\n",
            "那些就是所謂的soft skills 重要不重要重要 你看到任何一個成功的電機工程師 他都有一堆這種 這個纔是他成功的一個非常重要的關鍵 這種東西怎麼來\n",
            "\n",
            "我們剛才講的各種課業外的 各種學習增長的機會 都可以幫助一個人塑造他的soft skills 是有少數人的這些soft skills是天生的 他天生就厲害\n",
            "\n",
            "有沒有 有 但這種人畢竟沒那麼多 對很多人而言 他的soft skills是自己努力慢慢培養起來的 我剛才一開始前面講的那一段 我說我在進臺大電機系以前\n",
            "\n",
            "我幾乎不會交朋友 我不太會說話 我在讀大學的四年裡面改變我自己 讓我變成有很多這方面的能力的人 其實最重要就是我的很多soft skills 都是我自己培養\n",
            "\n",
            "在讀臺大電機系的四年裡面 獲得的非常多這方面的收穫的 那是為什麼我每次都要強調 這個東西有多麼重要 我之前曾經在幾年前的這個 信號與人生裡面有說到這一件事\n",
            "\n",
            "我現在不要重複 但是我簡單的summarize 我說我們電機系的電機工程師的\n",
            "\n",
            "\n",
            "========== The 7-th segment of the split (502 words) ==========\n",
            "\n",
            "\n",
            "一生career的發展 黃金實在是在什麼時候 我認為是在35歲到55歲 這20年是我們的黃金時代 在這以前當然更好 只是說可能各方面尚未具備 還沒有完全訓練的好\n",
            "\n",
            "在這以後是最好的 這以後年紀大了難免有一些要打個折扣等等 就這裡面我們看到 我們的電機系的畢業的同學 過去有幾千人畢業我都看到 我覺得有的人的發展是像這樣\n",
            "\n",
            "有一定的斜率 但到某一個階段它會慢慢saturate 有的人也許開始向上比較晚 但它斜率比較高 它最後會saturate在比較高的地方 也有的人也許開始的比較快\n",
            "\n",
            "但是後來會overshoot之後 會開始收斂在比較低的地方等等 每一個人都不一樣 但是當然也有一種人 你會看到他一直向上走 完全沒有saturate\n",
            "\n",
            "這些人這些差別在哪裡 這些東西差別在哪裡 我以前已經說過這件事 我不要多重複 我說最主要因素有四個 實力、努力、大智 跟self skill這四件事情\n",
            "\n",
            "我認為真正影響這個的 不是因為電子學考得好不好 不是因為信號與系統念得好不好 也就是我剛才講 你把每一門必修課 當成是單一跑道 跑到第一名並不表示怎樣\n",
            "\n",
            "我們最後不看那個的 最後看的是這個 這個是怎麼樣影響 我認為是這四件事\n",
            "\n",
            "\n",
            "========== The 8-th segment of the split (512 words) ==========\n",
            "\n",
            "\n",
            "就是實力、努力、大智跟self skills 這四件事裡面 我們現在可以summarize 我剛才講的 什麼是實力 實力就是所有的這些\n",
            "\n",
            "我們電機工程的專業領域裡面 各種東西的實力 實力怎麼厲害法 就是我剛才講的 你如果都是在做全面的學習的話 你就會學到各種該學到的 最後你的實力就是很強的\n",
            "\n",
            "所以實力最主要就是不要overfitting 要盡量都做 學到該學的全面的學習 努力是沒有疑問 每一個人都瞭解 確實我們可以看到 一個人在未來的幾十年裡面\n",
            "\n",
            "有的人他一直努力 有的人慢慢不太努力等等 這個是有明顯差別的 那self skills我剛才已經講了 就是很多我們平常沒有算成績 覺得大家不重視的事情\n",
            "\n",
            "其實它常常是很重要的 你如果好好的 多在各種課業外的事情上 增長進步的話 你這些東西會很強 你會很厲害的 當然對少數人而言 他天生就有 他可能不需要\n",
            "\n",
            "這是每一個人不一樣的 那這三個我都提過了 那麼大致我還沒有提 其實大致沒有什麼要特別說的 那應該就是我剛才前面有講過 就是每一個人可以有你自己的長程目標\n",
            "\n",
            "那有的人本來就有了 有的人也許我平常沒有想過 那你可以在適當時機開始想 我有沒有想要做什麼事情 哪些事情可能是我的長程目標\n",
            "\n",
            "\n",
            "========== The 9-th segment of the split (169 words) ==========\n",
            "\n",
            "\n",
            "我希望最後讓我花個5年10年 15年或者更長 我把我的很多的努力 都來把某一些事情做得非常漂亮 那是我很想做的事 那就是長程目標\n",
            "\n",
            "如果我覺得做那些事情會讓我非常的 覺得有意義 願意花功夫下去做的 那就是我的長程目標 那有的人如果可以想出這個來的話 那就是他的大致 那越是有這種大致的人\n",
            "\n",
            "也比較容易向上衝 那我感覺起來真正影響的就是這四件事\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extracts the text from an SRT file and saves it to a new text file\n",
        "pure_text = extract_and_save_text(srt_filename=output_subtitle_path, output_filename=output_raw_text_path)\n",
        "\n",
        "# Split a long document into smaller chunks of a specified length\n",
        "chunks = chunk_text(text=pure_text, max_length=512)\n",
        "\n",
        "# You can see the number of words and contents in each paragraph.\n",
        "print(\"Review the results of splitting the long text into several short texts.\\n\")\n",
        "for index, chunk in enumerate(chunks):\n",
        "    if index == 0:\n",
        "        print(f\"\\n========== The {index + 1}-st segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    elif index == 1:\n",
        "        print(f\"\\n========== The {index + 1}-nd segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    elif index == 2:\n",
        "        print(f\"\\n========== The {index + 1}-rd segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "    else:\n",
        "        print(f\"\\n========== The {index + 1}-th segment of the split ({len(chunk)} words) ==========\\n\\n\")\n",
        "        for text in textwrap.wrap(chunk, 80):\n",
        "            print(f\"{text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuvx30fkW4kU"
      },
      "source": [
        "# Part4 - Summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mr9Kz634zT2"
      },
      "source": [
        "## **You only need to choose one of the following parts.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCPYjOAyXWaE"
      },
      "source": [
        "## **If you want to use ChatGPT, begin with this part.**\n",
        "##### (1) You can refer to https://shorturl.at/X0NDY (Page 44) for obtaining ChatGPT API key.\n",
        "##### (2) You can refer to https://platform.openai.com/docs/models/overview for more details about models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCara20SW8AN"
      },
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"gpt-3.5-turbo\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the OpenAI Chat API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - client: OpenAI Chat API client.\n",
        "        - summarization_prompt: The summarization prompt including the text which need to be summarized.\n",
        "        - model_name: The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", system_prompt is \"GHI\", model_name is \"gpt-3.5-turbo\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(client=client, text=\"ABC\", summarization_prompt=\"DEF\", system_prompt=\"GHI\", model_name=\"gpt-3.5-turbo\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The user prompt is a concatenation of the summarization_prompt and text.\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the OpenAI Chat API to summarize the text.\n",
        "            chat_completion = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": user_prompt,\n",
        "                    }\n",
        "                ],\n",
        "                    model=model_name,\n",
        "                    temperature=temperature,\n",
        "                    top_p=top_p,\n",
        "                    max_tokens=max_tokens\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p3VZeUfBYcih"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of ChatGPT { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your OpenAI API key ===== '''\n",
        "\n",
        "# Your OpenAI API key.\n",
        "# @markdown **openai_api_key**: Your OpenAI API key.\n",
        "openai_api_key = \"YOUR_OPENAI_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"gpt-3.5-turbo\". You can refer to \"https://platform.openai.com/docs/models/overview\" for more details.\n",
        "model_name = \"gpt-3.5-turbo\" # @param {type: \"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysnuD_eIeWjY"
      },
      "outputs": [],
      "source": [
        "# Construct openai client.\n",
        "client = OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwDxai-YY4Hl"
      },
      "source": [
        "The code block below takes about **30** seconds to run when using the **gpt-3.5-turbo** model, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvCBfcW7Qd-e"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9QC8lG_QRZL"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUma_oXAQtmg"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQtk04XATy1j"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUTFgIGl1IRp"
      },
      "source": [
        "The code block below takes about **80** seconds to run when using the (1) **gpt-3.5-turbo** model, (2) length of chunks is 512 and (3) maximum number of tokens is 250, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv-Ko3UZYjpg"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6QXhSK_eACs"
      },
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itdqp3H5T2T7"
      },
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0ghZaKfVEyN"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka0JcOvYVIWu"
      },
      "source": [
        "The code block below takes about **10** seconds to run when using the (1) **gpt-3.5-turbo** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zHPMRDWeCuq"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "# Run final summarization.\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "        print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi7eERB8eGdK"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-chatgpt-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRzf_0cTV6TS"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k79C13kWW_Ye"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of ChatGPT Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEtfC9WeZkNH"
      },
      "source": [
        "The code block below takes about **200** seconds to run when using the (1) **gpt-3.5-turbo** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the OpenAI API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rugUlJ6HeZPF"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aFmk1ATAOdd"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(first_paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(first_paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaXWOLIOa4dM"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-chatgpt-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCM4kPuBXh7R"
      },
      "source": [
        "## **If you want to use Gemini, begin with this part.**\n",
        "##### (1) You can refer to https://shorturl.at/X0NDY (Page 35) for obtaining Gemini API key.\n",
        "##### (2) You can refer to https://ai.google.dev/models/gemini for more details about which models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "anuFrhHNkMgY"
      },
      "outputs": [],
      "source": [
        "def summarization(summarization_prompt, model_name=\"gemini-pro\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the OpenAI Chat API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - summarization_prompt: The summarization prompt.\n",
        "        - model_name: The model name, default is \"gemini-pro\". You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", model_name is \"gemini-pro\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(text=\"ABC\", summarization_prompt=\"DEF\", model_name=\"gemini-pro\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # The user prompt is a concatenation of the summarization_prompt and text.\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    # Load the generative model.\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    # Set the generation configuration.\n",
        "    generation_config = genai.GenerationConfig(temperature=temperature, top_p=top_p, max_output_tokens=max_tokens)\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the OpenAI Chat API to summarize the text.\n",
        "            response = model.generate_content(contents=user_prompt, generation_config=generation_config)\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cellView": "form",
        "id": "plyvWCvXllz3"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Gemini { run: \"auto\" }\n",
        "''' In this block, you can modify your desired parameters and set your api key. '''\n",
        "\n",
        "# Your google api key.\n",
        "# @markdown **google_api_key**: Your google api key.\n",
        "google_api_key = \"AIzaSyDsbB1SoWrYEDVM2QV3yeFe-SIVvJUInEQ\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name. You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "# @markdown **model_name**: The model name. You can refer to \"https://ai.google.dev/models/gemini\" for more details.\n",
        "model_name = \"gemini-pro\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 0.0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Eed0NjqJtGfM"
      },
      "outputs": [],
      "source": [
        "# Set Google API key.\n",
        "genai.configure(api_key=google_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csw7hxrHsJym"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGHFGCyasp9h"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_TsX6dBgs1iw"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkQdOOixuJU9"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olrUkpe615-E"
      },
      "source": [
        "The code block below takes about **40** seconds to run when using the (1) **gemini-pro** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5KOp_d6XloCn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n",
            "The API call fails, wait for 1 second and try again.\n"
          ]
        }
      ],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amp-gOibmpvK"
      },
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y08WjQxiuZ0k"
      },
      "source": [
        "#####Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4GEhPEIudBe"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdTjmyvfZwar"
      },
      "source": [
        "The code block below takes about **20** seconds to run when using the (1) **gemini-pro** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x5fn6I-msGC"
      },
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "# Run final summarization.\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "final_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "        print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPR2Am6omt0U"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-gemini-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "print(f\"Final summary has been saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZVeHCKvcMy"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1goEansHvc8C"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5AunhcUwDWh"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJMgqTC_2WD0"
      },
      "source": [
        "The code block below takes about **45** seconds to run when using the (1) **gemini-pro** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Google API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDWW_K4OwG1N"
      },
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(first_paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(first_paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5jw0HwWwW83"
      },
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-gemini-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **If you want to use Claude, begin with this part.**\n",
        "##### (1) You can refer to https://reurl.cc/yLy06D for obtaining Claude API key.\n",
        "##### (2) You can refer to https://docs.anthropic.com/claude/docs/models-overview for more details about which models you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"claude-3-sonnet-20240229\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    \"\"\"\n",
        "    (1) Objective:\n",
        "        - Use the Claude API to summarize a given text.\n",
        "\n",
        "    (2) Arguments:\n",
        "        - client: Claude API client.\n",
        "        - text: The text to be summarized.\n",
        "        - summarization_prompt: The summarization prompt.\n",
        "        - model_name: The model name, default is \"claude-3-sonnet-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "        - temperature: Controls randomness in the response. Lower values make responses more deterministic, default is 0.0.\n",
        "        - top_p: Controls diversity via nucleus sampling. Higher values lead to more diverse responses, default is 1.0.\n",
        "        - max_tokens: The maximum number of tokens to generate in the completion, default is 512.\n",
        "\n",
        "    (3) Return:\n",
        "        - The summarized text.\n",
        "\n",
        "    (4) Example:\n",
        "        - If the text is \"ABC\" and the summarization prompt is \"DEF\", system_prompt is \"GHI\", model_name is \"claude-3-sonnet-20240229\",\n",
        "          temperature is 0.0, top_p is 1.0, and max_tokens is 512, then you can call the function like this:\n",
        "\n",
        "              summarization(client=client, text=\"ABC\", summarization_prompt=\"DEF\", system_prompt=\"GHI\", model_name=\"claude-3-sonnet-20240229\", temperature=0.0, top_p=1.0, max_tokens=512)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the Claude API to summarize the text.\n",
        "            message = client.messages.create(\n",
        "                model=model_name,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(\"The API call fails, wait for 1 second and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return message.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Claude { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your Claude API key ===== '''\n",
        "\n",
        "# Your Claude API key.\n",
        "# @markdown **claude_api_key**: Your Claude api key.\n",
        "claude_api_key = \"YOUR_CLAUDE_API_KEY\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "model_name = \"claude-3-opus-20240229\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct Claude client.\n",
        "client = anthropic.Anthropic(api_key=claude_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code block below takes about **120** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code block below takes about **25** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "\n",
        "# Run final summarization.\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "    print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-claude-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code block below takes about **150** seconds to run when using the (1) **claude-3-opus-20240229** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(response, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-claude-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **If you want to use Deepseek, begin with this part.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "w-51sj3lXieC"
      },
      "outputs": [],
      "source": [
        "def summarization(client, summarization_prompt, model_name=\"deepseek-chat\", temperature=0.0, top_p=1.0, max_tokens=512):\n",
        "    user_prompt = summarization_prompt\n",
        "\n",
        "    while True:\n",
        "\n",
        "        try:\n",
        "            # Use the Claude API to summarize the text.\n",
        "            response = client.chat.completions.create(\n",
        "                model=model_name,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}\n",
        "                ],\n",
        "                stream=False\n",
        "            )\n",
        "\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            # If the API call fails, wait for 1 second and try again.\n",
        "            print(f\"Error: {e}, wait for 1s and try again.\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "cellView": "form",
        "id": "1mfcYh1Rwkrh"
      },
      "outputs": [],
      "source": [
        "# @title Parameter Setting of Claude { run: \"auto\" }\n",
        "''' ===== In this block, you can modify your desired parameters and set your Deepseek API key ===== '''\n",
        "\n",
        "# Your Claude API key.\n",
        "# @markdown **claude_api_key**: Your Claude api key.\n",
        "deepseek_api_key = \"sk-588fa9bef7544d0b8860cf3b4c9bc4d8\" # @param {type:\"string\"}\n",
        "\n",
        "# The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "# @markdown **model_name**: The model name, default is \"claude-3-opus-20240229\". You can refer to \"https://docs.anthropic.com/claude/docs/models-overview#model-comparison\" for more details.\n",
        "model_name = \"deepseek-chat\" # @param {type:\"string\"}\n",
        "\n",
        "# Controls randomness in the response. Lower values make responses more deterministic.\n",
        "# @markdown **temperature**: Controls randomness in the response. Lower values make responses more deterministic.\n",
        "temperature = 1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "# @markdown **top_p**: Controls diversity via nucleus sampling. Higher values lead to more diverse responses.\n",
        "top_p = 1.0 # @param {type:\"slider\", min:0, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "X-ivbHl6wrIj"
      },
      "outputs": [],
      "source": [
        "# Construct Claude client.\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDZJpX7xagQ"
      },
      "source": [
        "### We offer the following two methods for summarization.\n",
        "Reference: https://reurl.cc/VzagLA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmYimdXxctB"
      },
      "source": [
        "#### **If you want to use the method of Multi-Stage Summarization, begin with this part.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "s4OBcswSxfUp"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Multi-Stage Summarization: Paragraph { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# The maximum number of tokens to generate in the completion.\n",
        "# @markdown **max_tokens**: The maximum number of tokens to generate in the completion.\n",
        "max_tokens = 350 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown #### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"用 300 個字內寫出這段文字的摘要，其中包括要點和所有重要細節：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WClFqWBsyUWB"
      },
      "source": [
        "##### Step1: Split the long text into multiple smaller pieces and obtain summaries for each smaller text piece separately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avQs_YFg2mK7"
      },
      "source": [
        "The code block below takes about **120** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 350, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aJlEpq4EqIAN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------Summary of Segment 1----------------------------\n",
            "\n",
            "這段文字強調「學問是做出來的」，意指知識的獲得與內化需要透過實際行動與實踐，而非被動聽講或閱讀。作者指出，單純聽課或閱讀容易讓知識流於表面，無法真正吸收。透過動\n",
            "\n",
            "手實踐，例如完成期末專案或解決複雜習題，才能將所學整合並內化為自己的知識。這種「做中學」的過程讓學習者真正理解並掌握知識，而非僅停留在記憶層面。因此，作者鼓勵多\n",
            "\n",
            "動手實踐，讓學問成為自己的，而非依賴成績單上的數字來衡量學習成效。\n",
            "\n",
            "Length of summary for segment 1: 193\n",
            "Time taken to generate summary for segment 1: 20.56 sec.\n",
            "\n",
            "----------------------------Summary of Segment 2----------------------------\n",
            "\n",
            "這段文字強調了學習過程中思考的重要性，並指出許多人因為怕累而放棄修課或做專案，卻忽略了這些挑戰正是學習的最佳機會。雖然這些努力可能不會直接提升成績，但對全面學習\n",
            "\n",
            "和培養思考能力至關重要。作者以教授信號課為例，說明自己在教學時不直接推導數學式子，而是解釋其背後的意義，鼓勵學生在讀書時主動思考，理解內容的真正含義。這種思考習\n",
            "\n",
            "慣的培養，才是學習中最重要的事。\n",
            "\n",
            "Length of summary for segment 2: 176\n",
            "Time taken to generate summary for segment 2: 19.96 sec.\n",
            "\n",
            "----------------------------Summary of Segment 3----------------------------\n",
            "\n",
            "這段文字強調學習不僅限於課業內，課業外的活動同樣能帶來增長、進步和快樂，這些都是學習的一部分。例如，打球不僅增進健康，還能提升手腦協調、團隊精神和人際互動；爬山\n",
            "\n",
            "和旅行則能增長見識、擴展視野，並帶來快樂。只要活動能讓人感到進步和快樂，就值得投入時間和精力，視為學習的機會。\n",
            "\n",
            "Length of summary for segment 3: 135\n",
            "Time taken to generate summary for segment 3: 11.47 sec.\n",
            "\n",
            "----------------------------Summary of Segment 4----------------------------\n",
            "\n",
            "這段文字強調了人際互動與參與活動對個人成長的重要性。談戀愛不僅是情感上的收穫，還能讓人體驗人際關係中的感覺與期待，是一種學習。即使沒有戀愛機會，交朋友同樣能學習\n",
            "\n",
            "溝通、互動與理解他人，對電機系的同學而言，身邊的同學都是很好的交友對象。此外，參與活動如戲學會的舞蹈或戲劇表演，無論是台前或幕後工作，都能帶來成長與進步，因此許\n",
            "\n",
            "多同學積極參與這些活動，從中獲得經驗與提升。\n",
            "\n",
            "Length of summary for segment 4: 182\n",
            "Time taken to generate summary for segment 4: 12.69 sec.\n",
            "\n",
            "----------------------------Summary of Segment 5----------------------------\n",
            "\n",
            "這段文字強調了參與課外活動和團隊合作的重要性，尤其是對電機工程學生的發展。雖然這些活動沒有考試或成績，無法顯示在成績單上，但它們能培養「軟實力」，如團隊合作、領\n",
            "\n",
            "導能力和溝通技巧，這些都是未來職場成功的關鍵。電機工程領域很少能單打獨鬥，必須與他人合作才能完成重要工作。因此，學生應珍惜這些學習機會，從團隊邊緣逐步進入核心，\n",
            "\n",
            "甚至成為領導者，推動目標實現。這些軟實力與專業知識（硬實力）同樣重要，共同構成個人發展的基礎。\n",
            "\n",
            "Length of summary for segment 5: 207\n",
            "Time taken to generate summary for segment 5: 14.43 sec.\n",
            "\n",
            "----------------------------Summary of Segment 6----------------------------\n",
            "\n",
            "這段文字強調了「軟實力」（soft skills）的重要性，尤其是在電機工程師等專業領域中。軟實力包括溝通、協調、交朋友、說服、團隊精神和領導能力等，這些能力在\n",
            "\n",
            "人際互動中至關重要。雖然少數人天生具備這些能力，但大多數人是通過後天努力培養的。作者以自身經驗為例，說明在臺大電機系的四年中，他從不善交際到培養出許多軟實力，這\n",
            "\n",
            "些能力成為他成功的關鍵。因此，作者強調軟實力的重要性，並鼓勵學生透過課外學習和實踐來提升這些能力。\n",
            "\n",
            "Length of summary for segment 6: 209\n",
            "Time taken to generate summary for segment 6: 14.07 sec.\n",
            "\n",
            "----------------------------Summary of Segment 7----------------------------\n",
            "\n",
            "這段文字討論了職業生涯的黃金時期，認為35歲到55歲是關鍵的20年。在此之前，個人可能尚未完全準備好；在此之後，年紀增長可能影響表現。作者觀察到電機系畢業生的職\n",
            "\n",
            "業發展軌跡各異：有些人穩定成長後趨於平穩，有些人起步晚但成長快，有些人則先快後慢。影響職業發展的關鍵因素包括實力、努力、大智（智慧）和自我技能，而非單一學科的成\n",
            "\n",
            "績。最終，職業成就取決於這些綜合因素，而非單一領域的表現。\n",
            "\n",
            "Length of summary for segment 7: 189\n",
            "Time taken to generate summary for segment 7: 15.15 sec.\n",
            "\n",
            "----------------------------Summary of Segment 8----------------------------\n",
            "\n",
            "這段文字摘要如下：   實力是電機工程專業領域的全面學習，避免過度專精（overfitting），才能累積強大實力。努力是長期堅持的關鍵，影響未來成就。Self\n",
            "\n",
            "skills指課業外能力的培養，雖不計成績卻很重要，能提升個人競爭力。大智則是設定長程目標，有些人已有明確方向，有些人則需適時思考未來規劃。這四點（實力、努力、\n",
            "\n",
            "self skills、大智）共同塑造個人成長與成功。\n",
            "\n",
            "Length of summary for segment 8: 188\n",
            "Time taken to generate summary for segment 8: 14.57 sec.\n",
            "\n",
            "----------------------------Summary of Segment 9----------------------------\n",
            "\n",
            "這段文字強調了長程目標的重要性。作者希望花費5年、10年、15年甚至更長時間，將某些事情做到非常出色，這是他認為有意義且願意投入努力的目標。擁有這樣的長程目標能\n",
            "\n",
            "讓人更有動力向上衝刺。作者認為，真正影響一個人成長和成功的關鍵在於這四件事：明確的長程目標、對目標的熱情、願意投入時間與努力，以及目標對個人的意義。\n",
            "\n",
            "Length of summary for segment 9: 154\n",
            "Time taken to generate summary for segment 9: 12.25 sec.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    # Record the start time.\n",
        "    start = time.time()\n",
        "\n",
        "    # Construct summarization prompt.\n",
        "    summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "    \n",
        "    # We summarize each section that has been split up separately.\n",
        "    response = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "    # Calculate the execution time and round it to 2 decimal places.\n",
        "    cost_time = round(time.time() - start, 2)\n",
        "\n",
        "    # Print the summary and its length.\n",
        "    print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "    for text in textwrap.wrap(response, 80):\n",
        "        print(f\"{text}\\n\")\n",
        "    print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "    print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Record the result.\n",
        "    paragraph_summarizations.append(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3vzgd8rWqJP9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of segment 1: 這段文字強調「學問是做出來的」，意指知識的獲得與內化需要透過實際行動與實踐，而非被動聽講或閱讀。作者指出，單純聽課或閱讀容易讓知識流於表面，無法真正吸收。透過動手實踐，例如完成期末專案或解決複雜習題，才能將所學整合並內化為自己的知識。這種「做中學」的過程讓學習者真正理解並掌握知識，而非僅停留在記憶層面。因此，作者鼓勵多動手實踐，讓學問成為自己的，而非依賴成績單上的數字來衡量學習成效。\n",
            "\n",
            "Summary of segment 2: 這段文字強調了學習過程中思考的重要性，並指出許多人因為怕累而放棄修課或做專案，卻忽略了這些挑戰正是學習的最佳機會。雖然這些努力可能不會直接提升成績，但對全面學習和培養思考能力至關重要。作者以教授信號課為例，說明自己在教學時不直接推導數學式子，而是解釋其背後的意義，鼓勵學生在讀書時主動思考，理解內容的真正含義。這種思考習慣的培養，才是學習中最重要的事。\n",
            "\n",
            "Summary of segment 3: 這段文字強調學習不僅限於課業內，課業外的活動同樣能帶來增長、進步和快樂，這些都是學習的一部分。例如，打球不僅增進健康，還能提升手腦協調、團隊精神和人際互動；爬山和旅行則能增長見識、擴展視野，並帶來快樂。只要活動能讓人感到進步和快樂，就值得投入時間和精力，視為學習的機會。\n",
            "\n",
            "Summary of segment 4: 這段文字強調了人際互動與參與活動對個人成長的重要性。談戀愛不僅是情感上的收穫，還能讓人體驗人際關係中的感覺與期待，是一種學習。即使沒有戀愛機會，交朋友同樣能學習溝通、互動與理解他人，對電機系的同學而言，身邊的同學都是很好的交友對象。此外，參與活動如戲學會的舞蹈或戲劇表演，無論是台前或幕後工作，都能帶來成長與進步，因此許多同學積極參與這些活動，從中獲得經驗與提升。\n",
            "\n",
            "Summary of segment 5: 這段文字強調了參與課外活動和團隊合作的重要性，尤其是對電機工程學生的發展。雖然這些活動沒有考試或成績，無法顯示在成績單上，但它們能培養「軟實力」，如團隊合作、領導能力和溝通技巧，這些都是未來職場成功的關鍵。電機工程領域很少能單打獨鬥，必須與他人合作才能完成重要工作。因此，學生應珍惜這些學習機會，從團隊邊緣逐步進入核心，甚至成為領導者，推動目標實現。這些軟實力與專業知識（硬實力）同樣重要，共同構成個人發展的基礎。\n",
            "\n",
            "Summary of segment 6: 這段文字強調了「軟實力」（soft skills）的重要性，尤其是在電機工程師等專業領域中。軟實力包括溝通、協調、交朋友、說服、團隊精神和領導能力等，這些能力在人際互動中至關重要。雖然少數人天生具備這些能力，但大多數人是通過後天努力培養的。作者以自身經驗為例，說明在臺大電機系的四年中，他從不善交際到培養出許多軟實力，這些能力成為他成功的關鍵。因此，作者強調軟實力的重要性，並鼓勵學生透過課外學習和實踐來提升這些能力。\n",
            "\n",
            "Summary of segment 7: 這段文字討論了職業生涯的黃金時期，認為35歲到55歲是關鍵的20年。在此之前，個人可能尚未完全準備好；在此之後，年紀增長可能影響表現。作者觀察到電機系畢業生的職業發展軌跡各異：有些人穩定成長後趨於平穩，有些人起步晚但成長快，有些人則先快後慢。影響職業發展的關鍵因素包括實力、努力、大智（智慧）和自我技能，而非單一學科的成績。最終，職業成就取決於這些綜合因素，而非單一領域的表現。\n",
            "\n",
            "Summary of segment 8: 這段文字摘要如下：  \n",
            "實力是電機工程專業領域的全面學習，避免過度專精（overfitting），才能累積強大實力。努力是長期堅持的關鍵，影響未來成就。Self skills指課業外能力的培養，雖不計成績卻很重要，能提升個人競爭力。大智則是設定長程目標，有些人已有明確方向，有些人則需適時思考未來規劃。這四點（實力、努力、self skills、大智）共同塑造個人成長與成功。\n",
            "\n",
            "Summary of segment 9: 這段文字強調了長程目標的重要性。作者希望花費5年、10年、15年甚至更長時間，將某些事情做到非常出色，這是他認為有意義且願意投入努力的目標。擁有這樣的長程目標能讓人更有動力向上衝刺。作者認為，真正影響一個人成長和成功的關鍵在於這四件事：明確的長程目標、對目標的熱情、願意投入時間與努力，以及目標對個人的意義。\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# First, we collect all the summarizations obtained before and print them.\n",
        "\n",
        "collected_summarization = \"\"\n",
        "for index, paragraph_summarization in enumerate(paragraph_summarizations):\n",
        "    collected_summarization += f\"Summary of segment {index + 1}: {paragraph_summarization}\\n\\n\"\n",
        "\n",
        "print(collected_summarization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qDxoyLpymA5"
      },
      "source": [
        "##### Step2: After obtaining summaries for each smaller text piece separately, process these summaries to generate the final summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "E99bQfbgynXT"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Gemini Multi-Stage Summarization: Total { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens to ensure that the final summary does not exceed 550 tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "summarization_prompt_template = \"在 500 字以內寫出以下文字的簡潔摘要：<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-WvuFl4XV2J"
      },
      "source": [
        "The code block below takes about **25** seconds to run when using the (1) **claude-3-opus-20240229** model, (2) length of chunks is 512 and (3) maximum number of tokens is 550, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3qX4pa5QqKrc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------Final Summary----------------------------\n",
            "\n",
            "**摘要：**   這一系列文字強調了學習與成長的多面向，包括實踐、思考、課外活動、人際互動和軟實力的培養。知識的內化需要透過實際行動（如專案或習題）而非被動吸\n",
            "收，而思考則是學習的核心。課外活動如運動、旅行或參與社團，不僅帶來快樂，也促進個人成長。人際互動與團隊合作能培養溝通、領導等軟實力，這些能力與專業知識同等重要。\n",
            "職業生涯的黃金時期（35-55歲）取決於實力、努力、自我技能和長程目標的結合。最終，成功來自於明確的目標、持續的努力以及對個人意義的追求。\n",
            "\n",
            "Length of final summary: 229\n",
            "Time taken to generate the final summary: 16.93 sec.\n"
          ]
        }
      ],
      "source": [
        "# Finally, we compile a final summary from the summaries of each section.\n",
        "\n",
        "# Record the start time.\n",
        "start = time.time()\n",
        "\n",
        "summarization_prompt = summarization_prompt_template.replace(\"<text>\", collected_summarization)\n",
        "\n",
        "# Run final summarization.\n",
        "final_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "# Calculate the execution time and round it to 2 decimal places.\n",
        "cost_time = round(time.time() - start, 2)\n",
        "\n",
        "# Print the summary and its length.\n",
        "print(f\"----------------------------Final Summary----------------------------\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 80):\n",
        "    print(f\"{text}\")\n",
        "print(f\"\\nLength of final summary: {len(final_summarization)}\")\n",
        "print(f\"Time taken to generate the final summary: {cost_time} sec.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ldTq9WYlqL2U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final summary has been saved to ./final-summary-信號與人生-deepseek-multi-stage.txt\n",
            "\n",
            "===== Below is the final summary (229 words) =====\n",
            "\n",
            "**摘要：**   這一系列文字強調了學習與成長的多面向，包括實踐、思考、課外活動、人際互動和軟實力的培養。知識的內化需要透過實\n",
            "際行動（如專案或習題）而非被動吸收，而思考則是學習的核心。課外活動如運動、旅行或參與社團，不僅帶來快樂，也促進個人成長。人際互動\n",
            "與團隊合作能培養溝通、領導等軟實力，這些能力與專業知識同等重要。職業生涯的黃金時期（35-55歲）取決於實力、努力、自我技能和長\n",
            "程目標的結合。最終，成功來自於明確的目標、持續的努力以及對個人意義的追求。\n"
          ]
        }
      ],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-deepseek-multi-stage.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    final_summarization = cc.convert(final_summarization)\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(final_summarization)\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(final_summarization)} words) =====\\n\")\n",
        "for text in textwrap.wrap(final_summarization, 64):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBI-Ba8QzLU8"
      },
      "source": [
        "#### **If you want to use the method of Refinement, begin with this part.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-t2muOwUzNd0"
      },
      "outputs": [],
      "source": [
        "# @title Prompt Setting of Claude Refinement { run: \"auto\" }\n",
        "''' You can modify the summarization prompt and maximum number of tokens. '''\n",
        "''' However, DO NOT modify the part of <text>.'''\n",
        "\n",
        "# We set the maximum number of tokens.\n",
        "# @markdown **max_tokens**: We set the maximum number of tokens.\n",
        "max_tokens = 550 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Changing **summarization_prompt_template** and **summarization_prompt_refine_template**\n",
        "# @markdown You can modify the summarization prompt and maximum number of tokens. However, **DO NOT** modify the part of `<text>`.\n",
        "\n",
        "# Initial prompt.\n",
        "# @markdown **summarization_prompt_template**: Initial prompt.\n",
        "summarization_prompt_template = \"請在 300 字以內，提供以下文字的簡潔摘要:<text>\" # @param {type:\"string\"}\n",
        "\n",
        "# Refinement prompt.\n",
        "# @markdown **summarization_prompt_refinement_template**: Refinement prompt.\n",
        "summarization_prompt_refinement_template = \"請在 500 字以內，結合原先的摘要和新的內容，提供簡潔的摘要:<text>\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JxW1AR2zZDL"
      },
      "source": [
        "Pipeline of the method of Refinement.\n",
        "\n",
        "Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "\n",
        "Step2: For each following document, the previous output is fed in along with the new document.\n",
        "\n",
        "Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "\n",
        "Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkm60apd2tQd"
      },
      "source": [
        "The code block below takes about **150** seconds to run when using the (1) **claude-3-opus-20240229** model and (2) maximum number of tokens is 500, but the actual time may vary depending on the condition of Colab and the status of the Claude API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Feco4LK4zazs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------Summary of Segment 1----------------------------\n",
            "\n",
            "這段文字強調了長程目標的重要性。作者希望花費5年、10年、15年甚至更長時間，將某些事情做到非常出色，這是他認為有意義且願意投入努力的目標。擁有這樣的長程目標能\n",
            "\n",
            "讓人更有動力向上衝刺。作者認為，真正影響一個人成長和成功的關鍵在於這四件事：明確的長程目標、對目標的熱情、願意投入時間與努力，以及目標對個人的意義。\n",
            "\n",
            "Length of summary for segment 1: 154\n",
            "Time taken to generate summary for segment 1: 12.53 sec.\n",
            "\n",
            "----------------------------Summary of the First 2 Segments----------------------------\n",
            "\n",
            "這段文字強調「學問是做出來的」，指出知識的獲得不能僅靠被動的聽講或閱讀，而是需要透過實際操作與實踐才能真正內化。例如，修課時完成期末專案（final proje\n",
            "\n",
            "ct）或解決複雜的程式習題，能讓人將所學整合並深刻理解。然而，許多人因怕累或覺得不重要而避開這些實踐機會，卻忽略了這些正是學習的關鍵。雖然這些努力可能不會直接反\n",
            "\n",
            "映在成績上，但對全面學習和思考能力的培養至關重要。作者進一步以自身教學為例，強調在學習過程中應注重理解而非單純記憶，例如在閱讀數學式子時，應思考其背後的意義，而\n",
            "\n",
            "非機械式推導。這種主動思考的習慣，才是真正將知識轉化為學問的核心方法。因此，學習者應珍惜實踐機會，並培養思考能力，才能真正掌握並應用所學。\n",
            "\n",
            "Length of summary for the first 2 segments: 309\n",
            "Time taken to generate summary for the first 2 segments: 17.6 sec.\n",
            "\n",
            "----------------------------Summary of the First 3 Segments----------------------------\n",
            "\n",
            "這段文字強調「學問是做出來的」，指出知識的獲得不能僅靠被動的聽講或閱讀，而是需要透過實際操作與實踐才能真正內化。例如，修課時完成期末專案（final proje\n",
            "\n",
            "ct）或解決複雜的程式習題，能讓人將所學整合並深刻理解。然而，許多人因怕累或覺得不重要而避開這些實踐機會，卻忽略了這些正是學習的關鍵。雖然這些努力可能不會直接反\n",
            "\n",
            "映在成績上，但對全面學習和思考能力的培養至關重要。作者進一步以自身教學為例，強調在學習過程中應注重理解而非單純記憶，例如在閱讀數學式子時，應思考其背後的意義，而\n",
            "\n",
            "非機械式推導。這種主動思考的習慣，才是真正將知識轉化為學問的核心方法。因此，學習者應珍惜實踐機會，並培養思考能力，才能真正掌握並應用所學。  除了課業內的學習，\n",
            "\n",
            "課業外也有許多值得學習的機會。作者將學習定義為一種增長、進步並帶來快樂的過程。因此，任何能帶來增長、進步並讓人感到快樂的活動，都可以視為學習。例如，打球不僅能增\n",
            "\n",
            "進健康，還能提升手腦協調、團隊精神和人際互動能力；爬山則能讓人學習到許多生活智慧；旅行則能增長見識、擴展視野。這些活動雖然不在傳統課業範圍內，但同樣能帶來成長與\n",
            "\n",
            "快樂，值得投入時間與精力。因此，學習不應局限於課堂，生活中的各種體驗都是值得珍惜的學習機會。\n",
            "\n",
            "Length of summary for the first 3 segments: 526\n",
            "Time taken to generate summary for the first 3 segments: 32.62 sec.\n",
            "\n",
            "----------------------------Summary of the First 4 Segments----------------------------\n",
            "\n",
            "這段文字進一步擴展了學習的範疇，強調學習不僅限於課業，生活中的各種活動和體驗都是重要的學習機會。作者以談戀愛和交朋友為例，指出這些人際互動能讓人體驗到人與人之間\n",
            "\n",
            "的情感、期待與溝通，這些都是寶貴的學習經驗。即使沒有談戀愛的機會，交朋友同樣能帶來成長，尤其是與身邊的同學互動，能增進人際關係與溝通能力。此外，參與課外活動，如\n",
            "\n",
            "跳舞、演劇或幕後工作，也能讓人有所增長與進步。這些活動不僅帶來快樂，還能培養團隊合作、規劃能力等實用技能。因此，學習應被視為一種廣泛的成長過程，涵蓋生活中的各種\n",
            "\n",
            "體驗與實踐，而不應局限於傳統的課堂學習。\n",
            "\n",
            "Length of summary for the first 4 segments: 260\n",
            "Time taken to generate summary for the first 4 segments: 21.76 sec.\n",
            "\n",
            "----------------------------Summary of the First 5 Segments----------------------------\n",
            "\n",
            "這段文字進一步強調學習的廣泛性，指出學習不僅限於課業或特定領域，生活中的各種活動和體驗都是重要的學習機會。作者以談戀愛、交朋友和參與課外活動為例，說明這些經歷能\n",
            "\n",
            "讓人體驗情感、增進溝通能力，並培養團隊合作與規劃能力。即使這些活動沒有考試或成績單上的記錄，它們依然對個人成長至關重要。例如，參與服裝道具組或校內外活動，都能讓\n",
            "\n",
            "人有所增長，這些經驗被稱為「軟實力」，包括團隊合作、領導能力等，這些能力在現代社會中，尤其是電機工程等領域，是不可或缺的。作者提醒，不要因為這些活動沒有具體成績\n",
            "\n",
            "而忽視其價值，因為它們對個人發展和未來成功有著深遠的影響。因此，學習應被視為一種廣泛的成長過程，涵蓋生活中的各種體驗與實踐，而不應局限於傳統的課堂學習或硬實力的\n",
            "\n",
            "培養。\n",
            "\n",
            "Length of summary for the first 5 segments: 323\n",
            "Time taken to generate summary for the first 5 segments: 22.02 sec.\n",
            "\n",
            "----------------------------Summary of the First 6 Segments----------------------------\n",
            "\n",
            "這段文字強調學習的廣泛性，指出學習不僅限於課業或特定領域，生活中的各種活動和體驗都是重要的學習機會。作者以談戀愛、交朋友和參與課外活動為例，說明這些經歷能讓人體\n",
            "\n",
            "驗情感、增進溝通能力，並培養團隊合作與規劃能力。即使這些活動沒有考試或成績單上的記錄，它們依然對個人成長至關重要。例如，參與服裝道具組或校內外活動，都能讓人有所\n",
            "\n",
            "增長，這些經驗被稱為「軟實力」，包括團隊合作、領導能力等，這些能力在現代社會中，尤其是電機工程等領域，是不可或缺的。  作者進一步指出，軟實力（soft ski\n",
            "\n",
            "lls）如溝通能力、協調能力、說服力和領導力等，是成功電機工程師的關鍵。雖然有些人天生具備這些能力，但大多數人是通過努力培養的。作者以自身經歷為例，分享在臺大電\n",
            "\n",
            "機系就讀期間，從不善交際到培養出豐富的軟實力，強調這些能力對個人發展的重要性。他總結道，電機工程師的成功不僅依賴專業知識（硬實力），更需要軟實力的支持，這是在課\n",
            "\n",
            "業外的各種學習機會中逐步培養的。因此，學習應被視為一種廣泛的成長過程，涵蓋生活中的各種體驗與實踐，而不應局限於傳統的課堂學習或硬實力的培養。\n",
            "\n",
            "Length of summary for the first 6 segments: 470\n",
            "Time taken to generate summary for the first 6 segments: 23.38 sec.\n",
            "\n",
            "----------------------------Summary of the First 7 Segments----------------------------\n",
            "\n",
            "這段文字強調學習的廣泛性，指出學習不僅限於課業或特定領域，生活中的各種活動和體驗都是重要的學習機會。作者以談戀愛、交朋友和參與課外活動為例，說明這些經歷能讓人體\n",
            "\n",
            "驗情感、增進溝通能力，並培養團隊合作與規劃能力。這些經驗被稱為「軟實力」，包括團隊合作、領導能力等，這些能力在現代社會中，尤其是電機工程等領域，是不可或缺的。作\n",
            "\n",
            "者進一步指出，軟實力（soft skills）如溝通能力、協調能力、說服力和領導力等，是成功電機工程師的關鍵。雖然有些人天生具備這些能力，但大多數人是通過努力培\n",
            "\n",
            "養的。作者以自身經歷為例，分享在臺大電機系就讀期間，從不善交際到培養出豐富的軟實力，強調這些能力對個人發展的重要性。他總結道，電機工程師的成功不僅依賴專業知識（\n",
            "\n",
            "硬實力），更需要軟實力的支持，這是在課業外的各種學習機會中逐步培養的。  此外，作者探討了一生職業發展的黃金時期，認為35歲到55歲是關鍵的20年。在這之前，個\n",
            "\n",
            "人可能尚未完全準備好；在這之後，年紀增長可能影響表現。作者觀察到電機系畢業生的職業發展軌跡各不相同，有些人早期發展迅速但後期趨緩，有些人則持續向上。他強調，影響\n",
            "\n",
            "職業發展的關鍵因素並非學科成績，而是實力、努力、大智和軟實力（self skill）。這些因素決定了個人能否在職業生涯中持續成長，而非僅僅依賴專業知識的積累。因\n",
            "\n",
            "此，學習應被視為一種廣泛的成長過程，涵蓋生活中的各種體驗與實踐，而不應局限於傳統的課堂學習或硬實力的培養。\n",
            "\n",
            "Length of summary for the first 7 segments: 613\n",
            "Time taken to generate summary for the first 7 segments: 25.36 sec.\n",
            "\n",
            "----------------------------Summary of the First 8 Segments----------------------------\n",
            "\n",
            "這段文字總結了影響職業發展的四大關鍵因素：實力、努力、大智和軟實力（self skills）。實力指的是電機工程專業領域的全面學習能力，強調避免過度專精（ove\n",
            "\n",
            "rfitting），應廣泛涉獵以累積扎實的專業基礎。努力則是持續不懈的投入，長期來看，努力程度的不同會顯著影響個人成就。軟實力包括溝通、協調、領導等非專業能力，\n",
            "\n",
            "這些能力雖未列入課業成績，卻對職業發展至關重要，且可通過課外活動與生活經驗培養。至於大智，作者並未深入探討，僅提到個人應思考並設定長程目標，這有助於引導未來的努\n",
            "\n",
            "力方向。總結而言，成功不僅依賴專業知識，更需要全面的學習、持續的努力、清晰的目標以及軟實力的培養，這些因素共同塑造了個人在職業生涯中的成長與成就。\n",
            "\n",
            "Length of summary for the first 8 segments: 313\n",
            "Time taken to generate summary for the first 8 segments: 19.81 sec.\n",
            "\n",
            "----------------------------Summary of the First 9 Segments----------------------------\n",
            "\n",
            "這段文字總結了影響職業發展的四大關鍵因素：實力、努力、大智和軟實力（self skills）。實力指的是電機工程專業領域的全面學習能力，強調避免過度專精（ove\n",
            "\n",
            "rfitting），應廣泛涉獵以累積扎實的專業基礎。努力則是持續不懈的投入，長期來看，努力程度的不同會顯著影響個人成就。軟實力包括溝通、協調、領導等非專業能力，\n",
            "\n",
            "這些能力雖未列入課業成績，卻對職業發展至關重要，且可通過課外活動與生活經驗培養。至於大智，作者進一步闡述，認為個人應思考並設定長程目標，這些目標應是讓自己感到有\n",
            "\n",
            "意義且願意長期投入的事情。擁有清晰長程目標的人，更容易在職業生涯中取得突破與成就。總結而言，成功不僅依賴專業知識，更需要全面的學習、持續的努力、清晰的目標以及軟\n",
            "\n",
            "實力的培養，這些因素共同塑造了個人在職業生涯中的成長與成就。\n",
            "\n",
            "Length of summary for the first 9 segments: 350\n",
            "Time taken to generate summary for the first 9 segments: 17.35 sec.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "paragraph_summarizations = []\n",
        "\n",
        "# First, we summarize each section that has been split up separately.\n",
        "for index, chunk in enumerate(chunks):\n",
        "\n",
        "    if index == 0:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Construct summarization prompt.\n",
        "        summarization_prompt = summarization_prompt_template.replace(\"<text>\", chunk)\n",
        "\n",
        "        # Step1: It starts by running a prompt on a small portion of the data, generating initial output.\n",
        "        first_paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(first_paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # Print the summary and its length.\n",
        "        print(f\"----------------------------Summary of Segment {index + 1}----------------------------\\n\")\n",
        "        for text in textwrap.wrap(response, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for segment {index + 1}: {len(response)}\")\n",
        "        print(f\"Time taken to generate summary for segment {index + 1}: {cost_time} sec.\\n\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # Record the start time.\n",
        "        start = time.time()\n",
        "\n",
        "        # Step2: For each following document, the previous output is fed in along with the new document.\n",
        "        chunk_text = f\"\"\"前 {index} 段的摘要: {paragraph_summarizations[-1]}\\n第 {index + 1} 段的內容: {chunk}\"\"\"\n",
        "\n",
        "        # Construct refinement prompt for summarization.\n",
        "        summarization_prompt = summarization_prompt_refinement_template.replace(\"<text>\", chunk_text)\n",
        "\n",
        "        # Step3: The LLM is instructed to refine the output based on the new document's information.\n",
        "        paragraph_summarization = summarization(client=client, summarization_prompt=summarization_prompt, model_name=model_name, temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
        "\n",
        "        # Record the result.\n",
        "        paragraph_summarizations.append(paragraph_summarization)\n",
        "\n",
        "        # Calculate the execution time and round it to 2 decimal places.\n",
        "        cost_time = round(time.time() - start, 2)\n",
        "\n",
        "        # print results.\n",
        "        print(f\"----------------------------Summary of the First {index + 1} Segments----------------------------\\n\")\n",
        "        for text in textwrap.wrap(paragraph_summarization, 80):\n",
        "            print(f\"{text}\\n\")\n",
        "        print(f\"Length of summary for the first {index + 1} segments: {len(paragraph_summarization)}\")\n",
        "        print(f\"Time taken to generate summary for the first {index + 1} segments: {cost_time} sec.\\n\")\n",
        "\n",
        "    # Step4: This process continues iteratively until all documents have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "a4BGRgvFz0eW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final summary has been saved to ./final-summary-信號與人生-deepseek-refinement.txt\n",
            "\n",
            "===== Below is the final summary (350 words) =====\n",
            "\n",
            "這段文字總結了影響職業發展的四大關鍵因素：實力、努力、大智和軟實力（self skills）。實力指的是電機工程專業領域的全面學習能力，強調避免過度專精（ove\n",
            "rfitting），應廣泛涉獵以累積扎實的專業基礎。努力則是持續不懈的投入，長期來看，努力程度的不同會顯著影響個人成就。軟實力包括溝通、協調、領導等非專業能力，\n",
            "這些能力雖未列入課業成績，卻對職業發展至關重要，且可通過課外活動與生活經驗培養。至於大智，作者進一步闡述，認為個人應思考並設定長程目標，這些目標應是讓自己感到有\n",
            "意義且願意長期投入的事情。擁有清晰長程目標的人，更容易在職業生涯中取得突破與成就。總結而言，成功不僅依賴專業知識，更需要全面的學習、持續的努力、清晰的目標以及軟\n",
            "實力的培養，這些因素共同塑造了個人在職業生涯中的成長與成就。\n"
          ]
        }
      ],
      "source": [
        "''' In this block, you can modify your desired output path of final summary. '''\n",
        "\n",
        "output_path = f\"./final-summary-{suffix}-deepseek-refinement.txt\"\n",
        "\n",
        "# If you need to convert Simplified Chinese to Traditional Chinese, please set this option to True; otherwise, set it to False.\n",
        "convert_to_tradition_chinese = False\n",
        "\n",
        "if convert_to_tradition_chinese == True:\n",
        "    # Creating an instance of OpenCC for Simplified to Traditional Chinese conversion.\n",
        "    cc = OpenCC('s2t')\n",
        "    paragraph_summarizations[-1] = cc.convert(paragraph_summarizations[-1])\n",
        "\n",
        "# Output your final summary\n",
        "with open(output_path, \"w\") as fp:\n",
        "    fp.write(paragraph_summarizations[-1])\n",
        "\n",
        "# Show the result.\n",
        "print(f\"Final summary has been saved to {output_path}\")\n",
        "print(f\"\\n===== Below is the final summary ({len(paragraph_summarizations[-1])} words) =====\\n\")\n",
        "for text in textwrap.wrap(paragraph_summarizations[-1], 80):\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtQoyYjfdQLC"
      },
      "source": [
        "# Part5 - Check the correctness of the submission file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gclg3cORdiVR"
      },
      "outputs": [],
      "source": [
        "# Check the correctness of the submission file.\n",
        "import json\n",
        "import re\n",
        "\n",
        "your_submission_path = \"YOUR_SUBMISSION_PATH\"\n",
        "\n",
        "def check_format(your_submission_path):\n",
        "\n",
        "    final_score = 0\n",
        "\n",
        "    # check the extension of the file.\n",
        "    if not your_submission_path.endswith(\".json\"):\n",
        "        print(\"Please save your submission file in JSON format.\")\n",
        "        return False, final_score\n",
        "    else:\n",
        "        try:\n",
        "            with open(your_submission_path, \"r\") as fp:\n",
        "                your_submission = json.load(fp)\n",
        "\n",
        "            evaluation_result = your_submission[\"history\"][0][\"messages\"][1][\"content\"]\n",
        "\n",
        "            if \"總分：\" not in evaluation_result:\n",
        "                # Correct format: 總分: <你的分數>\n",
        "                print(\"Please make sure that the correct format of final score is included in the evaluation result.\")\n",
        "                print(\"The correct format is 總分: <你的分數>. For example, 總分: 97\")\n",
        "                return False, final_score\n",
        "\n",
        "            evaluation_result = evaluation_result.strip()\n",
        "            score_pattern = r\"總分：\\d+\"\n",
        "            score = re.findall(score_pattern, evaluation_result)\n",
        "\n",
        "            if score:\n",
        "                final_score = score[-1].replace(\"總分：\", \"\")\n",
        "                if \"/100\" in final_score:\n",
        "                    final_score = final_score.replace(\"/100\", \"\")\n",
        "            else:\n",
        "                print(\"Please make sure that the final score is included in the evaluation result.\")\n",
        "                return False, final_score\n",
        "\n",
        "        except:\n",
        "            print(\"Open the file failed. Please check the file path or save your submission file in correct JSON format\")\n",
        "            return False, final_score\n",
        "\n",
        "    return True, final_score\n",
        "\n",
        "format_correctness, final_score = check_format(your_submission_path)\n",
        "if format_correctness== True:\n",
        "    print(\"The format of your submission file is correct.\")\n",
        "    print(f\"Your final score is {final_score}.\")\n",
        "else:\n",
        "    print(\"The format of your submission file is wrong.\")\n",
        "    print(\"Please check the format of your submission file.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z9QC8lG_QRZL",
        "hRzf_0cTV6TS",
        "GEmYimdXxctB",
        "eBI-Ba8QzLU8"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
