{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnSzY2OlRI79"
      },
      "source": [
        "# GenAI HW6: LLM Values Alignment\n",
        "## Objectives\n",
        "- Learn how to align a model's behavior using labelled preference data.\n",
        "\n",
        "If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to ntu-gen-ai-2024-spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcXimb2YRI8A"
      },
      "source": [
        "## Install and import necessary libraries  (~2 min)\n",
        "### Ignore the warning if the blockes finish successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KKggQtnTRI8B",
        "outputId": "11b02b19-13e7-4f6c-c7c7-a584fc8e9450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes==0.43.1 datasets==2.19.0 peft==0.10.0 trl==0.8.6 accelerate==0.29.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7Nxtm9AqRI8C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "import gdown\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, GenerationConfig\n",
        "from tqdm.auto import tqdm\n",
        "from trl import DPOTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhR0gJZBRI8D"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-jHgIOYc9sA5",
        "outputId": "ebaf667d-1397-45a2-e860-e5774b132061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GenAI_hw6_dataset'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (4/4), 4.06 KiB | 1.35 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Baiiiiiiiiii/GenAI_hw6_dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1OmziTQ9RI8D"
      },
      "outputs": [],
      "source": [
        "# Open and load the json dataset\n",
        "with open(\"/content/GenAI_hw6_dataset/labelled_data.json\", 'r') as jsonfile:\n",
        "    full_data = json.load(jsonfile)\n",
        "\n",
        "with open(\"/content/GenAI_hw6_dataset/test_prompt.json\", 'r') as jsonfile:\n",
        "    test_data = json.load(jsonfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJgIZhNiRI8E"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2VqCkEXRI8E"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'MediaTek-Research/Breeze-7B-Instruct-v0_1',\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type='nf4'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6fTJw9BRI8F"
      },
      "source": [
        "## Get response from the original model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "P1nkgCcCRI8F"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('MediaTek-Research/Breeze-7B-Instruct-v0_1')\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def data_formulate(data):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": '回覆請少於20字'},\n",
        "        {\"role\": \"user\", \"content\": data['prompt']},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "original_model_response = []\n",
        "for data in tqdm(test_data):\n",
        "    id = data['id']\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    inputs = tokenizer(data_formulate(data), return_tensors=\"pt\").to('cuda')\n",
        "    generation_config=GenerationConfig(\n",
        "            do_sample=False,\n",
        "            max_new_tokens = 200,\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "    )\n",
        "    output = model.generate(**inputs, generation_config=generation_config)\n",
        "    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0].split('[/INST] ')[1]\n",
        "    original_model_response.append(output)\n",
        "    print('Response from original model:\\n'+output+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6bav5VuRI8C"
      },
      "source": [
        "## Set parameters\n",
        "### You only need to modify this block. Please don’t alter any other parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rbc-x1aKRI8D"
      },
      "outputs": [],
      "source": [
        "num_epoch = 1\n",
        "data_size = 50\n",
        "support_ratio = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSA2fmpEK1nW"
      },
      "source": [
        "## Prepare training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cjz12h8KsnV"
      },
      "outputs": [],
      "source": [
        "# Select part of the data for training\n",
        "training_data = full_data[:data_size]\n",
        "\n",
        "# Define the size of the support dataset\n",
        "support_data_size = int(data_size * support_ratio)\n",
        "\n",
        "# Prepare the data for the training dataset\n",
        "prompt_list = [data_formulate(data) for data in training_data]\n",
        "chosen_list = [data['support'] for data in training_data[:support_data_size]] + [data['oppose'] for data in training_data[support_data_size:]]\n",
        "rejected_list = [data['oppose'] for data in training_data[:support_data_size]] + [data['support'] for data in training_data[support_data_size:]]\n",
        "position_list = ['support' for _ in range(support_data_size)] + ['oppose' for _ in range(data_size - support_data_size)]\n",
        "\n",
        "# Create the training dataset\n",
        "train_dataset = Dataset.from_dict({'prompt': prompt_list, 'position': position_list, 'chosen': chosen_list, 'rejected': rejected_list})\n",
        "pd.DataFrame(train_dataset).rename(columns={\"chosen\": \"preferred\", \"rejected\": \"non-preferred\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIBhiTYQRI8F"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPd6nrnBRI8F"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./',\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=num_epoch,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=False,\n",
        "    learning_rate=2e-4,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    warmup_ratio = 0.1,\n",
        "    report_to = 'none'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdsDp5rkRI8G"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi5le-IQRI8G"
      },
      "outputs": [],
      "source": [
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzUnqb40RI8H"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I22TanExRI8H"
      },
      "source": [
        "## Get response from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuPGVAquRI8H"
      },
      "outputs": [],
      "source": [
        "trained_model_response = []\n",
        "for data in tqdm(test_data):\n",
        "    id = data['id']\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    inputs = tokenizer(data_formulate(data), return_tensors=\"pt\").to('cuda')\n",
        "    generation_config=GenerationConfig(\n",
        "            do_sample=False,\n",
        "            max_new_tokens = 200,\n",
        "            pad_token_id = tokenizer.pad_token_id\n",
        "    )\n",
        "    output = model.generate(**inputs, generation_config=generation_config)\n",
        "    output = tokenizer.batch_decode(output, skip_special_tokens=True)[0].split('[/INST] ')[1]\n",
        "    trained_model_response.append(output)\n",
        "    print('Response from trained model:\\n'+output+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-PtYZMRI8I"
      },
      "source": [
        "## Please observe the output of this block to complete your report, and don't forget to take a screenshot of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i0wI6rLRI8I"
      },
      "outputs": [],
      "source": [
        "model_response = []\n",
        "print(f'num_epoch: {num_epoch}\\ndata_size: {data_size}\\nsupport_ratio: {support_ratio}')\n",
        "print()\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    ref_output = original_model_response[id-1]\n",
        "    output = trained_model_response[id-1]\n",
        "    print(f'Question {id}:\\n'+data['prompt'])\n",
        "    print('Response from original model:\\n'+ref_output)\n",
        "    print('Response from trained model:\\n'+output)\n",
        "    print()\n",
        "    model_response.append({'id':data['id'], 'prompt':data['prompt'], 'response_from_original_model':ref_output, 'response_from_trained_model':output})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0FHblUvRI8I"
      },
      "source": [
        "## Get the output file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owGIuqdnRI8I"
      },
      "outputs": [],
      "source": [
        "with open(f\"epoch-{num_epoch}_size-{data_size}_ratio-{support_ratio}.json\", \"w\", encoding='UTF-8') as outfile:\n",
        "    json.dump(model_response, outfile, indent=4, ensure_ascii=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4412564,
          "sourceId": 7580251,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30636,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}