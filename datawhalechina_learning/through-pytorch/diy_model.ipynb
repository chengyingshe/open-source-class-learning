{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, mid_dim=None):\n",
    "        super().__init__()\n",
    "        if not mid_dim:\n",
    "            mid_dim = out_dim\n",
    "        self.double_conv = nn.Sequential(\n",
    "            # Conv -> BN -> Act\n",
    "            nn.Conv2d(in_dim, mid_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_dim, out_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_dim, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bilinear=False):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal conv to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # unlearnable (not change the number of channel)\n",
    "            self.conv = DoubleConv(in_dim, out_dim, in_dim // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_dim, in_dim // 2, kernel_size=2, stride=2)  # change the number of channel to half\n",
    "            self.conv = DoubleConv(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)  # [B, C, H, W]\n",
    "        diff_x = x2.shape[3] - x1.shape[3]\n",
    "        diff_y = x2.shape[2] - x1.shape[2]\n",
    "        # 这部分与原论文存在差异，原论文中将左边的输入进行裁剪，最后导致output segmentation map比输入图片的尺寸小，为了保持最后输出与输入图片的尺寸一致，这里采用padding的方式\n",
    "        x1 = F.pad(x1, [diff_x // 2,  # left\n",
    "                        diff_x - diff_x // 2,  # right\n",
    "                        diff_y // 2,  # top\n",
    "                        diff_y - diff_y // 2  # bottom\n",
    "                        ])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # 使用1x1卷积，只改变输出的通道数\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.out = OutConv(64, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UNet                                     [1, 1, 572, 572]          --\n",
       "├─DoubleConv: 1-1                        [1, 64, 572, 572]         --\n",
       "│    └─Sequential: 2-1                   [1, 64, 572, 572]         --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 572, 572]         1,728\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 572, 572]         128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 572, 572]         --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 572, 572]         36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 572, 572]         128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 572, 572]         --\n",
       "├─Down: 1-2                              [1, 128, 286, 286]        --\n",
       "│    └─MaxPool2d: 2-2                    [1, 64, 286, 286]         --\n",
       "│    └─DoubleConv: 2-3                   [1, 128, 286, 286]        --\n",
       "│    │    └─Sequential: 3-7              [1, 128, 286, 286]        221,696\n",
       "├─Down: 1-3                              [1, 256, 143, 143]        --\n",
       "│    └─MaxPool2d: 2-4                    [1, 128, 143, 143]        --\n",
       "│    └─DoubleConv: 2-5                   [1, 256, 143, 143]        --\n",
       "│    │    └─Sequential: 3-8              [1, 256, 143, 143]        885,760\n",
       "├─Down: 1-4                              [1, 512, 71, 71]          --\n",
       "│    └─MaxPool2d: 2-6                    [1, 256, 71, 71]          --\n",
       "│    └─DoubleConv: 2-7                   [1, 512, 71, 71]          --\n",
       "│    │    └─Sequential: 3-9              [1, 512, 71, 71]          3,540,992\n",
       "├─Down: 1-5                              [1, 1024, 35, 35]         --\n",
       "│    └─MaxPool2d: 2-8                    [1, 512, 35, 35]          --\n",
       "│    └─DoubleConv: 2-9                   [1, 1024, 35, 35]         --\n",
       "│    │    └─Sequential: 3-10             [1, 1024, 35, 35]         14,159,872\n",
       "├─Up: 1-6                                [1, 512, 71, 71]          --\n",
       "│    └─ConvTranspose2d: 2-10             [1, 512, 70, 70]          2,097,664\n",
       "│    └─DoubleConv: 2-11                  [1, 512, 71, 71]          --\n",
       "│    │    └─Sequential: 3-11             [1, 512, 71, 71]          7,079,936\n",
       "├─Up: 1-7                                [1, 256, 143, 143]        --\n",
       "│    └─ConvTranspose2d: 2-12             [1, 256, 142, 142]        524,544\n",
       "│    └─DoubleConv: 2-13                  [1, 256, 143, 143]        --\n",
       "│    │    └─Sequential: 3-12             [1, 256, 143, 143]        1,770,496\n",
       "├─Up: 1-8                                [1, 128, 286, 286]        --\n",
       "│    └─ConvTranspose2d: 2-14             [1, 128, 286, 286]        131,200\n",
       "│    └─DoubleConv: 2-15                  [1, 128, 286, 286]        --\n",
       "│    │    └─Sequential: 3-13             [1, 128, 286, 286]        442,880\n",
       "├─Up: 1-9                                [1, 64, 572, 572]         --\n",
       "│    └─ConvTranspose2d: 2-16             [1, 64, 572, 572]         32,832\n",
       "│    └─DoubleConv: 2-17                  [1, 64, 572, 572]         --\n",
       "│    │    └─Sequential: 3-14             [1, 64, 572, 572]         110,848\n",
       "├─OutConv: 1-10                          [1, 1, 572, 572]          --\n",
       "│    └─Conv2d: 2-18                      [1, 1, 572, 572]          65\n",
       "==========================================================================================\n",
       "Total params: 31,037,633\n",
       "Trainable params: 31,037,633\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 270.57\n",
       "==========================================================================================\n",
       "Input size (MB): 3.93\n",
       "Forward/backward pass size (MB): 2865.84\n",
       "Params size (MB): 124.15\n",
       "Estimated Total Size (MB): 2993.92\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNet(3, 1)\n",
    "summary(unet, (1, 3, 572, 572))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
