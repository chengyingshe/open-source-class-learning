{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = {\n",
    "    'num_classes': 10,\n",
    "    'batch_size': 512,\n",
    "    'n_epochs': 10,\n",
    "    'lr': 1e-3,\n",
    "    'seed': 42,\n",
    "    'val_ratio': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "def accuracy(pred, y):\n",
    "    return sum(torch.argmax(pred, dim=1) == y) / len(y)\n",
    "\n",
    "def train_step(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    loss_record = []\n",
    "    acc_record = []\n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_one_hot = torch.tensor(F.one_hot(y, num_classes=config['num_classes']), dtype=torch.float32)\n",
    "        X, y, y_one_hot = X.to(device), y.to(device), y_one_hot.to(device)\n",
    "        pred = model(X)        \n",
    "        acc = accuracy(pred, y)\n",
    "        acc_record.append(acc)\n",
    "        loss = criterion(pred, y_one_hot)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_record.append(loss.detach().item())\n",
    "    mean_train_loss = sum(loss_record) / len(loss_record)\n",
    "    mean_train_acc = sum(acc_record) / len(acc_record)\n",
    "    return mean_train_loss, mean_train_acc\n",
    "\n",
    "def val_step(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_record = []\n",
    "    acc_record = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader:\n",
    "            y_one_hot = torch.tensor(F.one_hot(y, num_classes=config['num_classes']), dtype=torch.float32)\n",
    "            X, y, y_one_hot = X.to(device), y.to(device), y_one_hot.to(device)\n",
    "            pred = model(X)\n",
    "            acc = accuracy(pred, y)\n",
    "            acc_record.append(acc)\n",
    "            loss = criterion(pred, y_one_hot)\n",
    "            loss_record.append(loss.detach().item())\n",
    "    mean_eval_loss = sum(loss_record) / len(loss_record)\n",
    "    mean_eval_acc = sum(acc_record) / len(acc_record)\n",
    "    return mean_eval_loss, mean_eval_acc\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, n_epochs, device):\n",
    "    for epoch in range(n_epochs):\n",
    "        # train\n",
    "        mean_train_loss, mean_train_acc = train_step(model, train_loader, criterion, optimizer, device)\n",
    "        # validation\n",
    "        mean_eval_loss, mean_eval_acc = val_step(model, valid_loader, criterion, device)\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}]\")\n",
    "        print(f\"Train: loss={mean_train_loss:.4f}, acc={mean_train_acc:.4f}\")\n",
    "        print(f\"Val: loss={mean_eval_loss:.4f}, acc={mean_eval_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练集和测试集\n",
    "def get_dataloader(batch_size=128, val_ratio=0.2, seed=42, num_workers=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    dataset = CIFAR10(root='./datasets', train=True, download=False, transform=transform)\n",
    "    val_size = int(val_ratio * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(seed))\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "train_dataloader, val_dataloader = get_dataloader(batch_size=config['batch_size'], val_ratio=config['val_ratio'], seed=config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([256, 3, 32, 32])\n",
      "Shape of y:  torch.Size([256]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleConv                               [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 1024, 1, 1]           --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 32, 32]           896\n",
       "│    └─ReLU: 2-2                         [1, 32, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 32, 16, 16]           --\n",
       "│    └─Conv2d: 2-4                       [1, 64, 16, 16]           18,496\n",
       "│    └─ReLU: 2-5                         [1, 64, 16, 16]           --\n",
       "│    └─MaxPool2d: 2-6                    [1, 64, 8, 8]             --\n",
       "│    └─Conv2d: 2-7                       [1, 256, 8, 8]            147,712\n",
       "│    └─ReLU: 2-8                         [1, 256, 8, 8]            --\n",
       "│    └─MaxPool2d: 2-9                    [1, 256, 4, 4]            --\n",
       "│    └─Conv2d: 2-10                      [1, 512, 4, 4]            1,180,160\n",
       "│    └─ReLU: 2-11                        [1, 512, 4, 4]            --\n",
       "│    └─MaxPool2d: 2-12                   [1, 512, 2, 2]            --\n",
       "│    └─Conv2d: 2-13                      [1, 1024, 2, 2]           4,719,616\n",
       "│    └─ReLU: 2-14                        [1, 1024, 2, 2]           --\n",
       "│    └─MaxPool2d: 2-15                   [1, 1024, 1, 1]           --\n",
       "├─Linear: 1-2                            [1, 10]                   10,250\n",
       "├─Softmax: 1-3                           [1, 10]                   --\n",
       "==========================================================================================\n",
       "Total params: 6,077,130\n",
       "Trainable params: 6,077,130\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 52.88\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.62\n",
       "Params size (MB): 24.31\n",
       "Estimated Total Size (MB): 24.94\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes=10, conv_layers=[32, 64, 256, 512, 1024]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(conv_layers)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, conv_layers[i], 3, stride=1, padding=1))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(conv_layers[i-1], conv_layers[i], 3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(conv_layers[-1], n_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = self.fc(x.flatten(1))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleConv(in_channels=3, n_classes=config['num_classes'])\n",
    "summary(model, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13668\\1287388975.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_one_hot = torch.tensor(F.one_hot(y, num_classes=config['num_classes']), dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13668\\1287388975.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_one_hot = torch.tensor(F.one_hot(y, num_classes=config['num_classes']), dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train: loss=2.1397, acc=0.3075 | Val: loss=2.0699, acc=0.3859\n",
      "Epoch [2/10]\n",
      "Train: loss=1.9914, acc=0.4657 | Val: loss=1.9872, acc=0.4693\n",
      "Epoch [3/10]\n",
      "Train: loss=1.9151, acc=0.5427 | Val: loss=1.8703, acc=0.5891\n",
      "Epoch [4/10]\n",
      "Train: loss=1.8529, acc=0.6066 | Val: loss=1.8413, acc=0.6147\n",
      "Epoch [5/10]\n",
      "Train: loss=1.8125, acc=0.6475 | Val: loss=1.8223, acc=0.6361\n",
      "Epoch [6/10]\n",
      "Train: loss=1.7771, acc=0.6842 | Val: loss=1.7931, acc=0.6659\n",
      "Epoch [7/10]\n",
      "Train: loss=1.7480, acc=0.7122 | Val: loss=1.7765, acc=0.6812\n",
      "Epoch [8/10]\n",
      "Train: loss=1.7196, acc=0.7410 | Val: loss=1.7536, acc=0.7047\n",
      "Epoch [9/10]\n",
      "Train: loss=1.7065, acc=0.7538 | Val: loss=1.7667, acc=0.6936\n",
      "Epoch [10/10]\n",
      "Train: loss=1.6834, acc=0.7778 | Val: loss=1.7443, acc=0.7171\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "train_model(model, train_dataloader, val_dataloader, criterion, optimizer, 10, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
